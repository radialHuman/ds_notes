{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43752a9f",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "### **[Preface](#preface)**\n",
    "## [Chapter 1](#chapter_1): Maths\n",
    "### **1.1 Stats**\n",
    "    [] 1.1.1 Mean, Median and Mode\n",
    "    [] 1.1.2 Correlation\n",
    "### **1.2 Probability**\n",
    "    [] 1.2.1 Conditional Probability\n",
    "    [] 1.2.2 P-value\n",
    "    [] 1.2.3 Likelihood\n",
    "### **1.3 Distributions**\n",
    "    [] 1.3.1 Normal\n",
    "    [] 1.3.2  \n",
    "    [] 1.3.3 Tests  \n",
    "### **1.4 Linear Algebra (Matrix)**\n",
    "    [] 1.4.1 Matrix Multiplication\n",
    "    [] 1.4.2 Inverse\n",
    "### [**1.5 Diffrentiation**](#1.5)\n",
    "    [X] 1.5.1 Gradient Descent\n",
    "    [] 1.5.2 Optimizers based on Gradient Descent\n",
    "    [] 1.5.3 Chain Rule\n",
    "### [**1.6 Useful Functions**](#1.6)\n",
    "    [X] 1.6.1 Activation Functions\n",
    "    [\\] 1.6.2 Normal Functions\n",
    "### **1.7 Stationarity**\n",
    "\n",
    "## [Chapter 2](#chapter_2) : Exploring data\n",
    "### **2.1 **\n",
    "### **2.2 Missing Value**\n",
    "### **2.3 Plots**\n",
    "### **2.4 Relations**\n",
    "### **2.5 Preprocessing**\n",
    "    [] 2.5.1 Language data\n",
    "    [] 2.5.2 One-Hot Encoding\n",
    "    [] 2.5.3 Outlier treatment : Isolation tree\n",
    "\n",
    "\n",
    "## [Chapter 3](#chapter_3) : Regression\n",
    "### [**3.1 Why?**](#3.1)\n",
    "### [**3.2 Linear Regression**](#3.2)\n",
    "    [X] 3.2.1 Assumptions\n",
    "    [X] 3.2.2 Working\n",
    "    [] 3.2.3 Pros and Cons\n",
    "### [**3.3 Polynomial Regression**](#3.3)\n",
    "    [] 3.3.1 Working\n",
    "    [] 3.3.2 GAM\n",
    "### [**3.4 Regularization**](#3.4)\n",
    "    [X] 3.4.1 Lasso\n",
    "    [X] 3.4.2 Ridge\n",
    "    [X] 3.4.3 Elastic\n",
    "### [**3.5 Validation**](#3.5)\n",
    "### [**3.6 Metirics**](#3.6)\n",
    "    [X]] 3.6.1 R2\n",
    "    [X] 3.6.2 Adjusted R2\n",
    "\n",
    "## [Chapter 4](#chapter_4) : Classification\n",
    "### [**4.1 Why?**](#4.1)\n",
    "### [**4.2 Logistic Regression**](#4.2)\n",
    "    [\\] 4.2.1 Working\n",
    "    [] 4.2.2 Pros and cons\n",
    "### [**4.3 Decision Tree**](#4.3)\n",
    "    [\\] 4.3.1 What?\n",
    "    [] 4.3.2 Classification\n",
    "    [] 4.3.3 Regression\n",
    "    [] 4.3.4 Pros and Cons\n",
    "### [**4.4 Random Forest**](#4.4)\n",
    "    [] 4.4.1 What?\n",
    "    [] 4.4.2 Working\n",
    "    [\\] 4.4.3 Types\n",
    "    [] 4.4.4 Pros and Cons\n",
    "### **4.5 Support Vector Machine**\n",
    "    [] 4.5.1 What?\n",
    "    [] 4.5.2 Working\n",
    "    [] 4.5.3 Pros and cons\n",
    "### **4.6 K-nearest Neighbour**\n",
    "    [] 4.6.1 What?\n",
    "    [] 4.6.2 Working\n",
    "    [] 4.6.3 Pros and cons\n",
    "### **4.7 Naive Bayes**\n",
    "    [] 4.7.1 What?\n",
    "    [] 4.7.4 Working\n",
    "    [] 4.7.5 Pros and cons\n",
    "### **4.8 Validation**\n",
    "    [] 4.8.1 K-Fold\n",
    "### **4.9 Imbalanced Data**\n",
    "    [] 4.9.1 SMOTE\n",
    "    [] 4.9.2 Scikit learn's imbalanced \" https://imbalanced-learn.org/stable/\n",
    "### **4.10 Metrics**\n",
    "    [] 4.10.1 Confusion Matrix\n",
    "\n",
    "## [Chapter 5](#chapter_5) : Unsupervised\n",
    "### **5.1 K-Means**\n",
    "    [] 5.1.1 Working\n",
    "    [] 5.1.2 Pros and cons\n",
    "### **5.2 K-Medoid**\n",
    "    [] 5.2.1 Working\n",
    "    [] 5.2.2 Pros and cons\n",
    "### **5.3 Hierarchical**\n",
    "    [] 5.3.1 Working\n",
    "    [] 5.3.2 Pros and cons\n",
    "### **5.4 Assosiative**\n",
    "    [] 5.4.1 Working\n",
    "    [] 5.4.2 Pros and cons\n",
    "### **5.5 TSNE**\n",
    "    [] 5.5.1 Working\n",
    "    [] 5.5.2 Pros and cons\n",
    "### **5.6 LDA**\n",
    "    [] 5.6.1 Working\n",
    "    [] 5.6.2 Pros and cons\n",
    "### **5.7 PCA**\n",
    "    [X] 5.7.1 Working\n",
    "    [X] 5.7.2 Pros and cons\n",
    "\n",
    "## [Chapter 6](#chapter_6) : Time Series\n",
    "### **6.1 Filters**\n",
    "### **6.2 ARIMA**\n",
    "    [] 6.2.1 AR\n",
    "    [] 6.2.2 MA\n",
    "    [] 6.2.3 ARIMAX\n",
    "    [] 6.2.4 SARIMA\n",
    "### **6.3 VAR**\n",
    "### **6.4 Validation**\n",
    "### **6.5 Metrics**\n",
    "\n",
    "## [Chapter 7](#chapter_7) : Neural Networks\n",
    "### **7.1 Motivation**\n",
    "### **7.2 CNN**\n",
    "    [] 7.2.1 Working\n",
    "    [] 7.2.2 Pros and cons\n",
    "### **7.3 RNN**\n",
    "    [] 7.3.1 Working\n",
    "    [] 7.3.2 Pros and cons\n",
    "### **7.4 LSTM**\n",
    "    [] 7.4.1 Working\n",
    "    [] 7.4.2 Types\n",
    "    [] 7.4.3 Pros and cons\n",
    "### **7.5 Auto Encoder**\n",
    "    [] 7.5.1 Working\n",
    "    [] 7.5.2 Pros and cons\n",
    "### **7.6 Generative Adversarial Network**\n",
    "    [] 7.6.1 Working\n",
    "    [] 7.6.2 Pros and cons\n",
    "### **7.7 Transformers**\n",
    "    [] 7.7.1 Working\n",
    "    [] 7.7.2 Pros and cons\n",
    "### **7.8 Transfer learning**\n",
    "    [] 7.8.1 Working\n",
    "    [] 7.8.2 Pros and cons\n",
    "### [**7.9 Knowledge Distillation**](#7.9)\n",
    "    [] 7.9.1 Working\n",
    "    [] 7.9.2 Pros and cons\n",
    "\n",
    "\n",
    "## [Chapter 8](#chapter_8) : Language Processing\n",
    "### **8.1 TF-IDF**\n",
    "    [] 8.1.1 Working\n",
    "    [] 8.1.2 Pros and cons\n",
    "### **8.2 Word Embedding**\n",
    "    [] 8.2.1 Working\n",
    "    [] 8.2.2 Pros and cons\n",
    "### **8.3 Summarization**\n",
    "    [] 8.3.1 Working\n",
    "    [] 8.3.2 Pros and cons\n",
    "### **8.4 Sentiment Analysis**\n",
    "    [] 8.4.1 Working\n",
    "    [] 8.4.2 Pros and cons\n",
    "### **8.5 BERT**\n",
    "    [] 8.5.1 Working\n",
    "    [] 8.5.2 Pros and cons\n",
    "\n",
    "## [Chapter 9](#chapter_9) : Recommender System\n",
    "### **9.1 **\n",
    "\n",
    "## [Chapter 10](#chapter_10) : Python\n",
    "### **10.1 Pandas**\n",
    "    [] 10.1.1 \n",
    "    [] 10.1.2 \n",
    "    [] 10.1.3 \n",
    "    [] 10.1.4 \n",
    "    [] 10.1.5 \n",
    "    [] 10.1.6 \n",
    "    [] 10.1.7 \n",
    "    [] 10.1.8 \n",
    "    [] 10.1.9 \n",
    "### **10.2 Numpy**\n",
    "    [] 10.2.1 \n",
    "    [] 10.2.2 \n",
    "    [] 10.2.3 \n",
    "    [] 10.2.4 \n",
    "    [] 10.2.5 \n",
    "    [] 10.2.6 \n",
    "    [] 10.2.7 \n",
    "    [] 10.2.8 \n",
    "    [] 10.2.9 \n",
    "### **10.3 Scikit-learn**\n",
    "    [] 10.3.1 \n",
    "    [] 10.3.2 \n",
    "    [] 10.3.3 \n",
    "    [] 10.3.4 \n",
    "    [] 10.3.5 \n",
    "    [] 10.3.6 \n",
    "    [] 10.3.7 \n",
    "    [] 10.3.8 \n",
    "    [] 10.3.9 \n",
    "### **10.4 statsmodel**\n",
    "### **10.5 TensorFlow**\n",
    "    [] 10.5.1 \n",
    "    [] 10.5.2 \n",
    "    [] 10.5.3 \n",
    "    [] 10.5.4 \n",
    "    [] 10.5.5 \n",
    "    [] 10.5.6 \n",
    "    [] 10.5.7 \n",
    "    [] 10.5.8 \n",
    "    [] 10.5.9 \n",
    "### [**10.6 Pytorch**](#10.6)\n",
    "    [\\] 10.6.1 About\n",
    "    [\\] 10.6.2 Functions\n",
    "### **10.7 NLTK**\n",
    "### **10.8 Scpacy**\n",
    "### **10.9 Gensim**\n",
    "### [**10.10 PyCaret**](#10.10)\n",
    "    [\\] 10.10.1 About\n",
    "    [\\] 10.10.2 Example\n",
    "### [**10.11 AutoML**](#10.11)\n",
    "    [\\] 10.11.1 About\n",
    "    [\\] 10.11.2 Types\n",
    "    [\\] 10.11.3 Auto-sklearn\n",
    "### **10.12 EDA**\n",
    "    [] 10.12.1 Sweetviz\n",
    "    [] 10.12.2 D-tale\n",
    "    [] 10.12.3 DataPrep\n",
    "    [] 10.12.4 Mito\n",
    "### **10.13 Numba, numexpr**\n",
    "### **10.14 Skippa**\n",
    "### **10.15 CUDA**\n",
    "### **10.16 hvplot** - https://youtu.be/-GcA0SEJaJM\n",
    "### **10.16 Ray** - https://youtu.be/-uZN_m_pgqw\n",
    "### **10.17 Dask**\n",
    "### **10.18 Rapids**\n",
    "### **10.19 SHAP**\n",
    "\n",
    "\n",
    "\n",
    "## [Chapter 11](#chapter_11) : Maintaining Models\n",
    "### **11.1 MLFlow**\n",
    "    [] 11.1.1 Working\n",
    "    [] 11.1.2 Parts\n",
    "    [] 11.1.3 Pros and cons\n",
    "### **11.2 KubeFlow**\n",
    "    [] 11.2.1 Working\n",
    "    [] 11.2.2 Parts\n",
    "    [] 11.2.3 Pros and cons \n",
    "### [**11.3 SageMaker**](#11.3)\n",
    "    [] 11.3.1 Working\n",
    "    [] 11.3.2 Parts\n",
    "    [] 11.3.3 Pros and cons\n",
    "### **11.4 Metaflow**\n",
    "    [] 11.4.1 Working\n",
    "    [] 11.4.2 Parts\n",
    "    [] 11.4.3 Pros and cons\n",
    "### **11.5 TFx**\n",
    "    [] 11.5.1 Working\n",
    "    [] 11.5.2 Parts\n",
    "    [] 11.5.3 Pros and cons\n",
    "### **11.6 MLComet**\n",
    "    [] 11.6.1 Working\n",
    "    [] 11.6.2 Parts\n",
    "    [] 11.6.3 Pros and cons\n",
    "### **11.7 CookieCutter**\n",
    "    [] 11.7.1 Working\n",
    "    [] 11.7.2 Parts\n",
    "    [] 11.7.3 Pros and cons\n",
    "### **11.8 Kendro**\n",
    "    [] 11.8.1 Working\n",
    "    [] 11.8.2 Parts\n",
    "    [] 11.8.3 Pros and cons\n",
    "### **11.9 Kale**\n",
    "    [] 11.9.1 Working\n",
    "    [] 11.9.2 Parts\n",
    "    [] 11.9.3 Pros and cons\n",
    "\n",
    "## [Chapter 12](#chapter_12) : Git stuff\n",
    "### **12.1 Commands**\n",
    "### **12.2**\n",
    "### **12.5 DVC**\n",
    "\n",
    "## [Chapter 13](#chapter_13) : Cloud Stuff\n",
    "### **13.1 AWS**\n",
    "### **13.2 GCP**\n",
    "### **13.3 Azure**\n",
    "### **13.4**\n",
    "### **13.5**\n",
    "\n",
    "## [Chapter 14](#chapter_14) : Containers\n",
    "### **14.1 Docker**\n",
    "### **14.2 Kubernetes**\n",
    "### **14.3**\n",
    "### **14.4**\n",
    "### **14.5**\n",
    "\n",
    "## [Chapter 15](#chapter_15) : Data Storage\n",
    "### **15.1 Spark**\n",
    "    [] 15.1.1 Working\n",
    "    [] 15.1.2 Parts\n",
    "    [] 15.1.3 Pros and cons\n",
    "### **15.2 Hadoop**\n",
    "    [] 15.2.1 Working\n",
    "    [] 15.2.2 Parts\n",
    "    [] 15.2.3 Pros and cons\n",
    "### **15.3 SQL**\n",
    "### **15.4 NoSQL**\n",
    "\n",
    "## [Chapter 16](#chapter_16) : Misc\n",
    "### **16.1 API**\n",
    "### **16.2 Flask**\n",
    "### **16.3 Django**\n",
    "### **16.4 Streamlit**\n",
    "\n",
    " ---\n",
    "### Resources\n",
    "### Videos\n",
    "### Books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2fd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a111bf3",
   "metadata": {},
   "source": [
    "<a id='preface'></a>\n",
    "## Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f2b9e",
   "metadata": {},
   "source": [
    "A refresher for all thats related to contemporary (2021) data science. \n",
    "\n",
    "#### **DISCLAIMER** : This is all that I know and understand. If you find anything incorrect or error feel free to correct me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569b2ec",
   "metadata": {},
   "source": [
    "<a id='chapter_1'></a>\n",
    "## Chapter 1: Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d4c01",
   "metadata": {},
   "source": [
    "### 1.1 Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110b5bf",
   "metadata": {},
   "source": [
    "#### 1.1.1 Mean, Median, Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33830f44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9329544c",
   "metadata": {},
   "source": [
    "#### 1.1.2 Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a537e7",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://www.youtube.com/watch?v=9dr8rJ9fE7o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a255f80",
   "metadata": {},
   "source": [
    "**Pearson Correlation**\n",
    "> * Since covariance doesn provide the magnitude on how two variables act againste ach other, correlation is used not just for the direction (+/-) but also the magnitude\n",
    "> * Range is [-1,1] : -1 being negative slope, and + otherwise, 0 means no apparent correlation\n",
    "> * Pearson Correlation = Covariance/(std. deviation of variable 1 * std. deviation of variable 2)\n",
    "> * Pearson correlation will not tell cause and effect relation\n",
    "> * Correlation has no unit while covariance does and does not have a range\n",
    "> * Correlation can be visible in a scatter chart beween two linearly related variables\n",
    "> * As a general rule if the abs(correlation)>= 2/sqrt(# of points) then relation ship exists\n",
    "> * CON: falls prey ot Outliers \n",
    "\n",
    "**Spearman Correlation**\n",
    "> * To prevent outlier affecting the correlation, it sorts the data and then ranks both the variables, if there are ties it uses mean\n",
    "> * These ranks are used to find correlation between the variables\n",
    "> * PRO : Handles both continuous and ordinal\n",
    "> * PRO : Outliers are ranked so looses its affect to mislead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10135af",
   "metadata": {},
   "source": [
    "### 1.2 Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b52fe",
   "metadata": {},
   "source": [
    "#### 1.2.1 Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd1db1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a5cfa3",
   "metadata": {},
   "source": [
    "####  1.2.2 P-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f32070",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b32f1a",
   "metadata": {},
   "source": [
    "####  1.2.3 Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b7e40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f841c62c",
   "metadata": {},
   "source": [
    "<a id='1.5'></a>\n",
    "### 1.5 Diffrentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb1cd4",
   "metadata": {},
   "source": [
    "####  1.5.1 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370cb07",
   "metadata": {},
   "source": [
    "> * After predicting/classifying, using a model based on input, to check how far off is it from the least amount of error possible, and moving towards that\n",
    "> * Moving by changing model paramters, checking if the minima is reached by checking the rate of change from previous set of parameters\n",
    "> * Usually used to find the lowest point in the graph of a function created by mapping loss function against weights\n",
    "> * But different starting points may lead to different minima which might not be the lowest of all (global minima) as the descent is said to move in the direction where the slope reduces\n",
    "> * To find the gradient/slope at a point it needs to be partially differentiated w.r.t. all the weights\n",
    "> * Then after finding which way is down the hill, the point is moved controlled by hyperparameter learning rate/step size\n",
    "> * Bigger steps might not converge and might move around skipping the mininma\n",
    "> * Smaller steps will take more time to reach the minima\n",
    "> * Modification to these steps and the way they move are in the form of adam, rmsprop etc\n",
    "\n",
    "[SOURCE](https://towardsdatascience.com/stochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5)\n",
    "\n",
    "**BATCH**\n",
    "> * Takes the entire data set in one go (epoch) and builds the model\n",
    "> * PROS:\n",
    "    * Simple, efficient, stable error gradient\n",
    "    * stable convergence : calulates weights at every epoch and takes the average\n",
    "> * CONS : \n",
    "    * Slow as updates occur once after the epoch\n",
    "    * Might get stuck in local minima or saddle as its smooth\n",
    "\n",
    "\n",
    "**STOCHATIC**\n",
    "> * Updates model one data input/point at a time (picked at random)\n",
    "> * Since the data to trian on is small for updating the model, the errors might be all over the space\n",
    "> * Due to multiple updates, its computationally expensive\n",
    "> * PROS: \n",
    "    * Faster, SGD can be used for larger datasets.\n",
    "    * It converges faster when the dataset is large as it causes updates to the parameters more frequently.\n",
    "> * CONS :\n",
    "    * Computationally expensive as it updates at every data point\n",
    "    * Due to jumpy nature, might not stay in global minima\n",
    "    * learning rate is constant and not adaptive\n",
    "\n",
    "\n",
    "**MINI-BATCH**\n",
    "> * Best of both the worlds: Takes random samples of data (50-256 data points/rows)\n",
    "> * PROS : Efficient , stable, faster\n",
    "> * CONS : new hyperparamter for the bathc size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98c697",
   "metadata": {},
   "source": [
    "####  1.5.2 Optimizers based on gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36dda8",
   "metadata": {},
   "source": [
    "[SOURCE](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)\n",
    "\n",
    "[SOURCE](https://ruder.io/optimizing-gradient-descent/)\n",
    "\n",
    "**MOMENTUM**\n",
    "> * To accelarate SGD in right direction and reduce its noisiness by adding a fraction of the past update usually 0.9\n",
    "> * PROS:\n",
    "    * Makes SGD faster\n",
    "    * Possible to escape local minima due to the \"push\"\n",
    "> * CONS:\n",
    "    * New paramter is introduced to decide the fraction value\n",
    "\n",
    "**ADAGRAD**\n",
    "> * Adapts the learning rate on the basis of history of gradients. \n",
    "> * lr will be inversely proportional to gradient size\n",
    "> * PROS:\n",
    "    * Performas well in sparse data\n",
    "    * For frequently occuring features, it has low lr and the opposite otherwise\n",
    "> * CONS:\n",
    "    *\n",
    "\n",
    "**ADAM**\n",
    "> *\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *\n",
    "    \n",
    "**ADADELTA**\n",
    "> * Extension of adagrad\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *\n",
    "\n",
    "**RMSPROP**\n",
    "> * Variation of RMSPROP\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *\n",
    "   \n",
    "\n",
    "**ADAMAX**\n",
    "> *\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *\n",
    "    \n",
    "**NADAM**\n",
    "> * Extention of ADAM\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *\n",
    "\n",
    "**AMSGRAD**\n",
    "> * \n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0189f0f",
   "metadata": {},
   "source": [
    "#### 1.5.3 Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea9ce2",
   "metadata": {},
   "source": [
    "![](https://www.oreilly.com/library/view/hands-on-convolutional-neural/9781789130331/assets/85cb6c18-9cbc-44ff-9db6-c7d90d3c7cf0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b40d6",
   "metadata": {},
   "source": [
    "<a id='1.6'></a>\n",
    "### 1.6 Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d4db4",
   "metadata": {},
   "source": [
    "####  1.6.1 Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae86cd",
   "metadata": {},
   "source": [
    "> * Used mostyl in Neural networks within neurons as a function which takes in input with weights and bias and applys non-linear function on it to classify a non-linear data\n",
    "> * output of a neuron = F(wx+b)\n",
    "\n",
    "**SIGMOID FUNCTION**\n",
    "![](https://ai-master.gitbooks.io/logistic-regression/content/assets/sigmoid_function.png)\n",
    "> * f(x) = 1/(1+e^-(x))\n",
    "> * Looks like elonged S with base on negative X and top parallel to positive X\n",
    "> * value ranges from 0-1\n",
    "> * Differentiation of sigmoid function ranges from 0-0.25 (for backpropagation)\n",
    "> * CONS : Affected by vanishing gradient problem\n",
    "> * CONS : Since the output of the function is not 0 centered, its time consuming to converge\n",
    "\n",
    "**TANH FUNCTION**\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/8/87/Hyperbolic_Tangent.svg)\n",
    "> * f(x) = (e^x - e^-x)/(e^x + e^-x)\n",
    "> * Looks like elonged S with base parallel to negative X and top parallel to positive X\n",
    "> * Values : [-1, 1]\n",
    "> * differentiated values : [0, 1]\n",
    "> * CONS : Affected by vanishing gradient problem\n",
    "\n",
    "**RELU FUNCTION**\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/ReLU_and_GELU.svg/1024px-ReLU_and_GELU.svg.png)\n",
    "rectified linear unit\n",
    "> * f(x) = max(0,x)\n",
    "> * Looks like ramp with 0 in negative\n",
    "> * Values : [0,x]\n",
    "> * differentiated values : [0,1]\n",
    "> * Cant be differentiated at 0 so sub gradient is taken\n",
    "> * Faster as it linear function unlike exponents in tanh and sigmoid\n",
    "> * CONS : Since negative is cast to 0, in case of negative weight, new weight and old weight will not change : dying relu/dead activation function\n",
    "\n",
    "**Leaky RELU FUNCTION**\n",
    "![](https://www.researchgate.net/profile/Aditi-Shenoy-3/publication/334389306/figure/fig8/AS:779352161677313@1562823443351/Illustration-of-output-of-ELU-vs-ReLU-vs-Leaky-ReLU-function-with-varying-input-values.ppm)\n",
    "> * f(x) = max(0.01x,x)\n",
    "> * special case of PRelu (parametric relu where 0.01 can vary)\n",
    "> * Looks like ramp with 0.01x in negative\n",
    "> * Values : [0.01x,x]\n",
    "> * differentiated values : [0.01x,1]\n",
    "> * The small value can be hyperparameter\n",
    "> * Solves dying relu issue\n",
    "> * CONS : Vanishing gradient can happen if multiple neagative weights can lead to very very small change\n",
    "\n",
    "**ELU FUNCTION** \n",
    "![]()\n",
    "exponential linear units\n",
    "> * f(x) = x if x is +ve\n",
    "> * f(x) = a(e^x -1) if x is 0/-ve\n",
    "> * Looks like ramp with small expotential to 0 for negative\n",
    "> * The small value for negative/0 can be hyperparameter\n",
    "> * Solves dying relu issue as diffentiable at 0\n",
    "> * CONS : A bit more complicated as exponential is involved\n",
    "\n",
    "**SWISH RELU FUNCTION** \n",
    "![]()\n",
    "Self gated\n",
    "> * f(x) = x * sigmoid(x)\n",
    "> * Graph is similar to elu but with left extreme starting from near 0 \n",
    "> * The small value for negative/0 can be hyperparameter\n",
    "> * Used when layers are 40 or more \n",
    "> * CONS : Computationally expensive\n",
    "\n",
    "**SOFTPLUS FUNCTION**\n",
    "![]()\n",
    "> * f(x) = ln(1+e^x)\n",
    "> * Graph is similar to relu but lifetd and smooth with value for 0\n",
    "> * CONS : Computationally expensive\n",
    "\n",
    "**SOFTMAX FUNCTION**\n",
    "![](https://www.gstatic.com/education/formulas2/397133473/en/softmax_function.svg)\n",
    "used in the final layer of the classification network\n",
    "> * Used when the output is of category with multiple classes\n",
    "> * Since the output will be a number for each class, exponent of that is divided by the sum of exponent of all\n",
    "> * To use the maximum probability as the class category output\n",
    "> * Its normalization of exponentials. Exponent makes +ve real numbers and normalization helps with finding our probability distribution\n",
    "> * The problem is, it tries to diminish the significance of other classes if not highest. This can lead to loss of understanding which classes are closer\n",
    "> * this can be done by smoothing the softmax function, where a hyperparameter temprature is used\n",
    "![](https://i0.wp.com/neptune.ai/wp-content/uploads/Gumbel-Softmax.png?resize=626%2C547&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac3c1d",
   "metadata": {},
   "source": [
    "####  1.6.2 Normal Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35947663",
   "metadata": {},
   "source": [
    "**LOG FUNCTION**\n",
    "![Image of Log function](http://wiki.engageeducation.org.au/wp-content/uploads/2015/10/funclog1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4609525",
   "metadata": {},
   "source": [
    "**ODDS FUNCTION**\n",
    "> * probability of something occuring/ probability of not occuring\n",
    "> * Log(Odds function) is used in logistic regression = regression line equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db6ae2c",
   "metadata": {},
   "source": [
    "**LOSS FUNCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf98d71",
   "metadata": {},
   "source": [
    "> * Cross-Entropy loss function (classification)\n",
    "![](https://miro.medium.com/proxy/1*ftQegucUqLc7RQ-UNrs-Wg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10053cfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8849c430",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b7b51ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42878677",
   "metadata": {},
   "source": [
    "<a id='chapter_2'></a>\n",
    "## Chapter 2 : Exploring data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107d0b2d",
   "metadata": {},
   "source": [
    "<a id='chapter_3'></a>\n",
    "## Chapter 3 : Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b298a57",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d84aa",
   "metadata": {},
   "source": [
    "* To get an equation of a line/plane/multi-dimensional plane with all the features learning their weights (coefficients) and a bias term to predict the target variable\n",
    "* Simple to explain as weights tell how important the feature is while predicting the target\n",
    "* Explains a unit change in a feature and its affect on target while keeping all the other weights and bias same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ecfd1",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "### 3.2 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b9ebb",
   "metadata": {},
   "source": [
    "* When the relation between the features and the target is linear i.e. while plotting each feature against the target if there exists a straight line that can explain most of the points or points fall close to the straight line\n",
    "> Example: Target = Bias + w1\\*Feature1 + w2\\*Feature2 +  w3\\*Feature3 + ... +  wN\\*FeatureN + error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a537768",
   "metadata": {},
   "source": [
    "#### 3.2.1 Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126814bf",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://www.youtube.com/playlist?list=PLTNMv857s9WUI1Nz4SssXDKAELESXz-bi)\n",
    "* Is these assumptions are not meet, the values of wieghts and bias derived will not be reliable on unknown inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee36b1",
   "metadata": {},
   "source": [
    "1. There is a linear relation between the features and the target : plot scatter plot and check for each feature against target\n",
    "\n",
    "> **What does it mean?**\n",
    "> * The function/equation in this case, should be of specific type with weights multiplied with features or transforme version of features and summed up\n",
    "\n",
    "\n",
    "> **What if its not?**\n",
    "> * The relation is not linear and the line/equation formed will not be the best fit line\n",
    "\n",
    "> **How to find out?**\n",
    "> * Residual Plot (when the line drawn from the relation between feature and target is parallel along the x-axis)\n",
    "> * In this plot, the points should show any pattern like a curve and must be random\n",
    "> * Likelihood ratio test : TODO\n",
    "\n",
    "> **How to fix this?**\n",
    "> * By adding/transforming the feature with functions like log or polynomials and making the equation such that the line fits well and there is no pattern in the residual plot (trail and error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659a8da",
   "metadata": {},
   "source": [
    "2. The correlation between features is as low as possible (No multicolleniarity) : \n",
    "\n",
    "> **What does it mean?**\n",
    "> * Correlation between the fetures is high\n",
    "\n",
    "> **What if its not?**\n",
    "> * Multiple effects on target by feature. \n",
    "> * While interpreting, the other features will not be constant as they are related\n",
    "> * Equation is unreliable completely\n",
    "\n",
    "> **How to find out?**\n",
    "> *  Correlation among features \n",
    "> * Variance inflation factor (VIF) : Finds how is the model's variance, if one of the related variables is removed. If the VIF of a feature is more, the info provided by that feature already exists in the other features\n",
    "\n",
    "> **How to fix this?**\n",
    "> * Remove a feature which has high correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57618777",
   "metadata": {},
   "source": [
    "3. The variance of the residuals should not vary as the target is increased in residual plot (No Heteroscedasticity) : \n",
    "\n",
    "> **What does it mean?**\n",
    "> * If a feature has a wide range of numbers then this can happen as the feature might not be a good predictor\n",
    "> * The variance of points when plotted X vs Y, is not constant\n",
    "\n",
    "> **What if its not?**\n",
    "> * Std. erros of the equation will not be reliable\n",
    "\n",
    "> **How to find out?**\n",
    "> * In residual plot of the particular feature, the poitns would be funnel shaped as the variance is not constant\n",
    "> * Goldfledt-Quant test : TODO\n",
    "> * Breusch-Pagan test : TODO\n",
    "\n",
    "> **How to fix this?**\n",
    "> * Log transformation on the feture might help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34faffef",
   "metadata": {},
   "source": [
    "4. Each error should be independent of each other (No Autocorrelation/Serial Corrleation)\n",
    "\n",
    "> **What does it mean?**\n",
    "> * Usually occurs in time series data.\n",
    "> * If there is any order/time relation to feature, rare\n",
    "> * Successive value of feature depends on its past values, might even show seasonality or trend\n",
    "\n",
    "> **What if its not?**\n",
    "> * Std errors cant be relied on ergo hypothesis testing on the feature's coefficient will be difficult\n",
    "\n",
    "> **How to find out?**\n",
    "> * Residual plot might show wave like patterns\n",
    "> * Durbin-Watson test\n",
    "> * Breusch-Godfrey test\n",
    "\n",
    "> **How to fix this?**\n",
    "> * Look for features that are affecting the target (Missing features)\n",
    "> * Target can be modified by finding weight difference like AR(1) or Cochrane-Orchutt method \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc5fba",
   "metadata": {},
   "source": [
    "5. The spread of the errors at a given point, should be normally distributed\n",
    "\n",
    "> **What does it mean?**\n",
    "> * At any given point in the residual plot, the errors occuring, must fit in nicely with a normal distributed bell curve\n",
    "\n",
    "\n",
    "> **What if its not?**\n",
    "> * Not that big of an issue when data points are more\n",
    "> * For small size of data, std error are not reliable\n",
    "\n",
    "> **How to find out?**\n",
    "> * Histogram of residuals : must fit a bell curve\n",
    "> * Q-Q Plot : Quantiles from feature vs quantiles of normally distributed data, must fit the line to be used\n",
    "> * Shapiro-Wilk test\n",
    "> * Komolgorov-Smirnov test\n",
    "> * Anderson-Darling test\n",
    "\n",
    "> **How to fix this?**\n",
    "> * Get more data points\n",
    "> * Transform the feature by log etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89dc7d",
   "metadata": {},
   "source": [
    "6. Exogeneity\n",
    "\n",
    "> **What does it mean?**\n",
    "> * When an equation doesn have all the required and missing feature affects other features and the target, there exists a ommited variable bias\n",
    ">* This would cause error term to indicate something is missing\n",
    "\n",
    "> **What if its not?**\n",
    "> * Causation cant be known completely\n",
    "\n",
    "> **How to find out?**\n",
    "> * Domain knowledge/logic : to find whats missing \n",
    "\n",
    "> **How to fix this?**\n",
    "> * Instrumental variables : TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01026a17",
   "metadata": {},
   "source": [
    "#### 3.2.2 Working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceedd5a4",
   "metadata": {},
   "source": [
    "*Terminology*\n",
    "1. **SSE** : Squared distance between actual and predicted value\n",
    "1. **SSR** : squared distance between predicted value and mean of feature (unexplained error)\n",
    "1. **SST** : Squared distance between target and mean of feature (SSR+SSE) (explained error)\n",
    "1. **R2** : 1-(SSR/SST)\n",
    "1. **Adjusted R2** : 1-((1-R2)(# of datapoints-1)/(# of data points-1-# of features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d074f94",
   "metadata": {},
   "source": [
    "1. Gradient Descent technique\n",
    "\n",
    "> * Like many other ML algorithms, this uses a cost function to get learn the values of weights and bias\n",
    "> * The slope on which the global minima to be found is created using error vs all possible combinations of the coefficients\n",
    "\n",
    "> * The cost function can vary as per requirement ex: MSE, MAE/MAD, MAPE, MASE, Hubier\n",
    "> *MSE* : Mean square error (predicted - actual)^2. (-) Outlier handelling is bad, as the error is sqaured\n",
    "> *MAE* : mean absolute error |predicted - actual|. (-) Computationally expensive, cant be differentiated but good with outliers\n",
    "> *Hubier* : If the error is small acts like MSE else MAE, (-) Needs extra hyperparameter tuning \n",
    "> *MAPE* : TODO\n",
    "> *MASE* : TODO\n",
    "\n",
    "> 1. A line is drawn, and these predicted values are compared with the actual values. Since difference between these values (error) can be nullified either a square (MSE) or an absolute (MAE) or combination of both (Hubier) is used to find the loss value\n",
    "> 2. The idea is to find a combination of weights (coefficients of features) that gives the least error \n",
    "> 3. This is done using finding global minima in the landscape where error is plotted against all the coefficients\n",
    "> 4. Gradient descent (using partial derivative w.r.t. each feature) finds the slope of a particular combination and moves one step in a direction, if the slope is negative then it keeps moving till it reaches a point where the slope becomes positive or error doesn seem to change in any direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3262098",
   "metadata": {},
   "source": [
    "2. Oridinary least square method (y = mx+c)\n",
    "\n",
    "> * To find slope m  = sum((each feature value - mean of the feature)\\*(each target - mean of target))/sum((each feature value - mean of the feature)^2)\n",
    "> * To find intercept c = mean of target - m*(mean of feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba037b2",
   "metadata": {},
   "source": [
    "3. Using Linear Algebra\n",
    "\n",
    "[**SOURCE**](https://www.youtube.com/watch?v=RDQNc5xYQYM)\n",
    "[**SOURCE**](https://www.youtube.com/watch?v=S8UsRS2YvsM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf0547",
   "metadata": {},
   "source": [
    "> * When all the data points needs to be expressed, it not enough to show y = b0 + w1x1 + w2x2+ ... + E\n",
    "> * It needs to be shown as matrix exquation\n",
    "> * Via which weights are found using : B = Inverse(Xt*X) * (Xt*Y), where X is a design matrix with a column of 1 (for intercept) and a column of Xs\n",
    "> * Xt*X gives 2x2 matrix  = [[N, mean(x)],[mean(x),mean(x^2]]\n",
    "> * Xt*Y gives a 2x1 amtrix = [[Sum(Y)],[Sum(XY)]]\n",
    "> * Multiplying them gives the learned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8603d",
   "metadata": {},
   "source": [
    "4. Maximum likelihood Estimator\n",
    "\n",
    "[**SOURCE**](https://www.youtube.com/watch?v=UdADuHJUX6Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be56ff",
   "metadata": {},
   "source": [
    "> * Assumption here is the error terms are noramlly identically independent distributed = N(0, sigma^2)\n",
    "> * Unlike in OLS, where distance is used to optimize, in MLE, all the points of feature are assumed to be from a normally > distributed curve with varying mean (that value of y)\n",
    "> * The MLE formula here will be maximized to get weights instead of reducing the loss function in OLS/GD\n",
    "> * When the features are Normally distributed, the cost function here is negative of loss function in OLS. So the weights are the same.\n",
    "> * Log is taken while increasing the likelihood, hence log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e0598",
   "metadata": {},
   "source": [
    "### 3.3 Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c5a80",
   "metadata": {},
   "source": [
    "NOTE : If an equation has x and x^2 as 2 features, though the are derivable, they are not linearly correlated so thier existence will  not lead to multicolliarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63089dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import corrcoef\n",
    "# import matplotlib.pyplot as plt\n",
    "# x = [1,2,3,4,5,6,7,8]\n",
    "# x2 = [i*i for i in x]\n",
    "# x3 = [i*i*i for i in x]\n",
    "# print(f\"Square {corrcoef(x,x2)[0][1]}\")\n",
    "# plt.plot(x,x2)\n",
    "# plt.show()\n",
    "# print(f\"Cube {corrcoef(x,x3)[0][1]}\")\n",
    "# plt.plot(x,x3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6eb976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ef13936",
   "metadata": {},
   "source": [
    "<a id='3.4'></a>\n",
    "### 3.4 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8804d1",
   "metadata": {},
   "source": [
    "* Adding a little bias to decrease variance by a lot\n",
    "* To avoid overfitting\n",
    "* To penalize model for having high test error\n",
    "* Variables must be standardized before applying regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde90e3",
   "metadata": {},
   "source": [
    "#### 3.4.1 Lasso (L1) Least Absolute Shrinkage and selection Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8608b",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://www.youtube.com/watch?v=NGf0voTMlcs)\n",
    "[**SOURCE**](https://www.youtube.com/watch?v=t_1ZSWGDkX4&list=PL2GWo47BFyUPWL5fBZSn6FFHRr1bSkX_J&index=16)\n",
    "\n",
    "> * Main difference here, w.r.t. Ridge is the sum of squared residual is added with Lambda * abs(slope) (Manhattan distance) instead of square of slope\n",
    "> * This causes line to have slope of extacly 0 for high values of lambda unlike Ridge which comes close\n",
    "> * Making a slope of feature 0 causes removal of that feature there by making it useful for feature reduction\n",
    "> * GD can be used to reduce the loss function\n",
    "> * While plotting Possible values for weights, a diamond shape is formed around the origin and the predicted weights countours using mean square error , if overlapped gives the values and can at times, meet at the edges and make predicted weight 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378d519",
   "metadata": {},
   "source": [
    "#### 3.4.2 Ridge (L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798615a6",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://www.youtube.com/watch?v=Q81RR3yKn30)\n",
    "> * Along with the loss/cost function, Lambda * slope^2 is added (Eucledian distance), where lambda is the hyperpararmeter (0-infinity)\n",
    "> * Slope square acts as a penalty if the line is steep\n",
    "> * This adds small bias and there by reducing variance of the equation\n",
    "> * Value of lambda is directly proportional to reducing affect of the feature on target\n",
    "> * Value of lambda is found by using random value and k-fold cross validation\n",
    "> * By addding the penalty, sensitivity of the line is decreased and unit change in feature doesn show more than required change in target\n",
    "> * In case of categorical feature, instead of slope, the distance between the means of categories is used\n",
    "> * In case of multiple features, lambda is multiplied with sum of square of slope of each numnerical feature and mean distance of categorical except the intercept value\n",
    "> * While plotting Possible values for weights, a circle is formed around the origin and the predicted weights countours using mean square error , if overlapped gives the values and can at times, meet at the edges and make values close to 0 but not exactly 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04ac24",
   "metadata": {},
   "source": [
    "#### 3.4.3 Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a10ef",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://youtu.be/iJE2fZcNPlA?list=PL2GWo47BFyUPWL5fBZSn6FFHRr1bSkX_J)\n",
    "> * Its Hubier loss of regularization. A combo of L1 and L2 above\n",
    "> * The penalty term is both the absolute and Square of slope making the equation : MSE + L1 + L2\n",
    "> * There are 2 lambdas involved in hyperparameter tuning\n",
    "> * This can be used as feature removal + reducing the magnitude of others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987d7c5",
   "metadata": {},
   "source": [
    "**DECIDING FACTOR**\n",
    "> * If all the feautres must be present go with Ridge\n",
    "> * If features can be removed then dont use Ridge\n",
    "> * If outliers are present use Lasso\n",
    "> * If gradient descent cant be used, go with Ridge\n",
    "> * For unique solution avoid Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b5a13",
   "metadata": {},
   "source": [
    "<a id='3.5'></a>\n",
    "### 3.5 Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88fde9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "889dfafc",
   "metadata": {},
   "source": [
    "<a id='3.6'></a>\n",
    "### 3.6 Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6070a8",
   "metadata": {},
   "source": [
    "#### 3.6.1 R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17845fd1",
   "metadata": {},
   "source": [
    "> * To measure how good the equation of a linear regression is by finding the proportion of variance in the target that can be explained by the features\n",
    "            R2 = 1-(SSR/SST)\n",
    "> * Bigger the R2 better the model but 1 would be overfitting\n",
    "> * Usually the range is between 0 to 1 unless the fit is actually worse than just fitting a horizontal line then R-square can be negative\n",
    "> * CON : Adding more features may increase the R2 but that is not the right way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1feb8",
   "metadata": {},
   "source": [
    "#### 3.6.2 Adjusted R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccab7d",
   "metadata": {},
   "source": [
    "> * Adding more feature will increase R2 so adjusted R2 is used to penalize the model for having unwanted features\n",
    "            Adjusted R2 = 1-((1-R2)(# of point -1)/(# of points - # of feautres -1))\n",
    "> * If R2 is 0, Adjusted R2 might even be neagtive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd27df2",
   "metadata": {},
   "source": [
    "<a id='chapter_4'></a>\n",
    "## Chapter 4 : Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20effa61",
   "metadata": {},
   "source": [
    "<a id='4.1'></a>\n",
    "### 4.1 Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bf910",
   "metadata": {},
   "source": [
    "> * When the target is categorical and not numerical/continuous\n",
    "> * Since linear rgerssion is sussptible to outliers, this can lead to unreliable model\n",
    "> * Linear regression can give value thats not applicable to category of target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308694e3",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>\n",
    "### 4.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e04f19",
   "metadata": {},
   "source": [
    "#### 4.2.1 Working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7e606",
   "metadata": {},
   "source": [
    "> * The value to be predicted is a probability of it being in a category\n",
    "> * Since probability is not linear, linear Regression cant be used\n",
    "> * A signmoid funciton is used in case of binary classification to get the probability\n",
    "> * Sigmoid function makes o/p 1 if value is more than 0.5 and 0 otherwise\n",
    "> * p(X) = sigmoid(w0+w1x1+...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ae459",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/875/1*6oBgYMy4wOls9zGC-frSQg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92eec7",
   "metadata": {},
   "source": [
    "> * COST FUNCTION : \n",
    "    if y = 1 ; -log(predicted probability) \n",
    "    if y = 0 ; -log(1-predicted probability) \n",
    "    ![](https://miro.medium.com/max/1192/1*wilGXrItaMAJmZNl6RJq9Q.png)\n",
    "> * If the output is actually 1 but predcited is 0, the cost fucntion penalizes heavily as -log(0) = infinity\n",
    "> * Same otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697373c",
   "metadata": {},
   "source": [
    "<a id='4.3'></a>\n",
    "### 4.3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e4179f",
   "metadata": {},
   "source": [
    "> * Based on impurity functions which measures how pure a given set it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e39f4",
   "metadata": {},
   "source": [
    "#### 4.3.1 What?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964b2a7d",
   "metadata": {},
   "source": [
    "> * Entropy :  0 is pure, while 1 is most impure\n",
    "![](https://miro.medium.com/max/1010/1*-ZZAH1q_DHbVxobuXKY2ng.png)\n",
    "> * Gini : 0 means all belong to one class and is pure. 0.5 is the most impure class. Faster then entropy as its not log based\n",
    "![](https://miro.medium.com/max/664/1*otdoiyIwxJI-UV0ukkyutw.png)\n",
    "> * Information gain :\n",
    "> * Node : Usually splits into 2 nodes/leaves. Signifies a feature with condition for splitting\n",
    "> * Leaf : end of the line with label associated with it to classify\n",
    "\n",
    "CART uses Gini; ID3 and C4.5 use Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad926b1a",
   "metadata": {},
   "source": [
    "#### 4.3.2 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3be0d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2045a17",
   "metadata": {},
   "source": [
    "#### 4.3.3 Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668e032",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5513a698",
   "metadata": {},
   "source": [
    "#### 4.3.4 Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c643e",
   "metadata": {},
   "source": [
    "> * Usually overfits as it memorizes rules of traning data\n",
    "> * Low depth trees leads to high bias (solved by boosting)\n",
    "> * High depth leads to high variance (solved by bagging)\n",
    "> * More data increases time exponentially to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ee9be",
   "metadata": {},
   "source": [
    "<a id='4.4'></a>\n",
    "### 4.4 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1cf0e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d215aca5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fc725d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8287969",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e093ceff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d57f5ca7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d29e14c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49b46c1e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cf4b1a3",
   "metadata": {},
   "source": [
    "#### 4.4.3 Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6efaf",
   "metadata": {},
   "source": [
    "[SOURCE](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7514b08",
   "metadata": {},
   "source": [
    "> * Bootstrapping : Using multiple random subsets of original traning datasets to create many models (decision trees) for generalizing. This can happen in parallel and they are all weak independent learners.\n",
    "> * Aggregating : After building multiple models, taking average or majority of the result of these outputs as final prediction/classification\n",
    "> * Bagging : boosting+aggregating\n",
    "> * Boosting : Similar to bagging create multiple models but sequentially with the aim of correcting incorrect outcomes (residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05c33e",
   "metadata": {},
   "source": [
    "[SOURCE](https://www.youtube.com/watch?v=D9A8zIiJSAo)\n",
    "\n",
    "**AdaBoost (Adaptive boosting)**\n",
    "> * Multiple models are created one by one, by assigns bigger weights to the observations which are incorrectly predicted in the previous model and the subsequent model works to predict these values correctly till the error function does not change, or the maximum limit of the number of estimators is reached.\n",
    "> * These learns are weak and are stumps (built based on 1-2 features, slected based on gini/entropy)\n",
    "> * The trees are assigned weights based on metric\n",
    "> * The observations are assigned weights proprtional to incorrectness\n",
    "> * The incorrect ones will be repeated in the next weak stummp learner\n",
    "\n",
    "**GradientBoost**\n",
    "> * Can be used for both reg and classification. The target variable here is the residual between actual and average for regression. Then the model is build to minimize error/loss. \n",
    "> * Not too large but not a stump either are weak learners\n",
    "> * In the end its prediction fo first tree and the weighted sum of residuals from others\n",
    "> * learning rate is a hyperparamtere here which has a trade off with # of trees\n",
    "\n",
    "**XGBoost**\n",
    "> * Extremem boost is faster than normal one and includes\n",
    "> * regularization\n",
    "> * parallization\n",
    "> * custom optimization objectives\n",
    "> * missing value handelling\n",
    "> * built in cross validation at each tree \n",
    "> * pruning\n",
    "\n",
    "**Light GradientBoost**\n",
    "> * Useful for very large datasets as it takes less time. Uses leaf growth than level growth\n",
    "\n",
    "**CatBoost**\n",
    "> * Useful for categorical datasets with  highly cardinality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6687aa48",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c8d45d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ea32bce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed3a4bd0",
   "metadata": {},
   "source": [
    "#### 4.4.4 Pros and cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e45f48",
   "metadata": {},
   "source": [
    "\n",
    "**Pros**\n",
    "> * Handels categorical data\n",
    "> * Avoids DT's overfitting\n",
    "> * Learns non linear patterns\n",
    "> * reduces preprocessing\n",
    "> * Can run multiple tress in parallel\n",
    "> * Better than DT on higher dimensional data\n",
    "\n",
    "**CONS**\n",
    "> * Relatively a black box as it doesn explain well\n",
    "> * needs hyperparamter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bebdc0",
   "metadata": {},
   "source": [
    "<a id='chapter_5'></a>\n",
    "## Chapter 5 : Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c32066a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2b097a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85be3f65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af1386f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c73c5607",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21ff8ab2",
   "metadata": {},
   "source": [
    "<a id='5.7'></a>\n",
    "### 5.7 : PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34f85e",
   "metadata": {},
   "source": [
    "#### 5.7.1 Working\n",
    "[SOURCE](https://www.youtube.com/watch?v=TJdH6rPA-TI)\n",
    "[SOURCE](https://medium.com/analytics-vidhya/understanding-principle-component-analysis-pca-step-by-step-e7a4bb4031d9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5f1ad",
   "metadata": {},
   "source": [
    "> * Though called a reduction technique, its a transforming technique. \n",
    "> * It transforms the data so that it looks more meaningful\n",
    "> * For this to work the data must be stabdardized i.e. mean 0 and sd 1\n",
    "> * Deciding between two features by plotting scatter chart and finding an aixs which helps to retain the information as much as possible while allowing to remove a feature \n",
    "> * The new direction of axis where the variance is the highest and the error along the new axis is the least\n",
    "> * The next Principle axis is at 90* with the previous line\n",
    "> * This is repeated for all the features and # of PC are == # of features\n",
    "> * But as the # of PC increases, since variance decreases, they become less useful\n",
    "> * This way the PC with most information can be retained and rest eliminated\n",
    "> * The PCs that add upto 99% of the variance accountability can be used to represent the data withotu losing much information\n",
    "> * The resulting projected data are linear combinations of the original data\n",
    "\n",
    "??? How/in what order and why are features selected to create PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf033a",
   "metadata": {},
   "source": [
    "> * 1: Standardize the dataset if not defined by same unit\n",
    "> * 2: Calculate the covariance matrix for the features in the dataset\n",
    "> * 3: Calculate the eigenvalues and eigenvectors for the covariance matrix\n",
    "> * 4: Sort eigenvalues and their corresponding eigenvectors\n",
    "> * 5: Pick k eigenvalues and form a matrix of eigenvectors\n",
    "> * 6: Transform the original matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee660689",
   "metadata": {},
   "source": [
    "#### 5.7.2 Pros and cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ae0ef",
   "metadata": {},
   "source": [
    "**PROS**\n",
    "> * Less features, fast computation\n",
    "\n",
    "**CONS**\n",
    "> * The PCs will not be easily interpretable\n",
    "> * Might reduce the accuracy if modelled\n",
    "> * Measurements from all of the original variables are used in the projection to the lower dimensional space\n",
    "> * Only linear relationships are considered, Doesn’t work well for non linearly correlated data.\n",
    "> * Always uses orthogonal PC\n",
    "> * Sometime lo wvariance features can add value but gets removed, while something with outlier might be included\n",
    "> * PCA or SVD-based methods, as well as univariate screening methods (t-test, correlation, etc.), do not take into account the potential multivariate nature of the data structure (e.g., higher order interaction between variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e23e9c",
   "metadata": {},
   "source": [
    "<a id='chapter_6'></a>\n",
    "## Chapter 6 : Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8203215",
   "metadata": {},
   "source": [
    "> * The mean and the variance must be constant over the time period\n",
    "> * If the mean is not constant then there can be a trend\n",
    "> * If variance is not, then it is highly volatile\n",
    "> * There can also be repeated pattern, visually looking like a cycle called seasonality\n",
    "> * If all the values are taken form the TS and plotted like in a bell curve, and then the probability is found for a certain point it would still not help because the occurance of a data point might be dependent on its previous n values i.e. autocorrelation, correlation with self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020e3a7",
   "metadata": {},
   "source": [
    "<a id='6.1'></a>\n",
    "### 6.1 : Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f27c407",
   "metadata": {},
   "source": [
    "> * To get signal (informative part) from data and leave out noise\n",
    "> * Used for preprocessing TS data to\n",
    "    - remove seasonality (detected by making box plots for each time interval)\n",
    "    - detrending it (by taking a linear regression line equation and subtracting it with original data)\n",
    "    - differencing it (if the change in data from time t ot t+n is same for any time period, then the difference b/w the points can be used as TS, if n=1 it is first difference )\n",
    "    - make it stationary \n",
    "    - remove outliers \n",
    "    - Smoothing out by moving average or exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226aee3e",
   "metadata": {},
   "source": [
    "[SOURCE](https://www.youtube.com/playlist?list=PLvcbYUQ5t0UHOLnBzl46_Q6QKtFgfMGc3)\n",
    "#### 6.1.1 Autocorrelation\n",
    "> * The TS might have memory, i.e. values at time t will be dependent on previous values unless it a random walk data\n",
    "> * Finding out how value at T is getting affected by value at t-n gives the correlation or ACF\n",
    "> * This can have two types of affect one is direct i.e. t-n value affecting t's value or  t-n affecting t via t-(n+1) t-(n+2) ... t-(n-n-1)\n",
    "> * Can be found by having x as original value and y as -n values and finding pearson correlation between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0005daf",
   "metadata": {},
   "source": [
    "#### 6.1.2 Partial Autocorrelation\n",
    "> * If only the direct affect is considered on t frmo t-n then its partial autocorrelation\n",
    "> * This ignores the indirect effect\n",
    "> * Can be found using regression equation giving weights to past values\n",
    "        t(n) = w1*t(n-1)+w2*t(n-2)+w3*t(n-3)+w4*t(n-4)+... + error \n",
    "> * where wn is the the direct effect of that time to the present value\n",
    "> * values of the direct effect (wn) if more than the threshold will be considered having significant effect in finding present value given the past of these values of weights\n",
    "> * This equation is AR (auto rgessive model) which has a decaying Auto correlation plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270d8c0",
   "metadata": {},
   "source": [
    "<a id='6.2'></a>\n",
    "### 6.2 : ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f8614",
   "metadata": {},
   "source": [
    "> * AR(n) : y(t) = w1*y(t-1)+w2*y(t-2)+w3*y(t-3)+...wn*y(t-n)+E\n",
    "> * MA(n) : y(t) = mean + w1*e(t-1)+w2*e(t-2)+w3*e(t-3)+...wn*e(t-n)\n",
    "> * If ACF wanes over time it indicates MA elements\n",
    "> * If ACF oscillates over time it indicates AR elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109eac1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0419add",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21248849",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fc23b93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6768449a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4ecfd6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d505435",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d5aa91c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf9a26b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7bd861b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aa41069",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b14545d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "574264c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b5f732f",
   "metadata": {},
   "source": [
    "<a id='chapter_7'></a>\n",
    "## Chapter 7 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ef99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787fd74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa82830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c0c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5cc3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb1b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f2e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b1b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d11b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8922aa4b",
   "metadata": {},
   "source": [
    "<a id='7.9'></a>\n",
    "## 7.9 Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef527d",
   "metadata": {},
   "source": [
    "### 7.9.1 Working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58991e53",
   "metadata": {},
   "source": [
    "[SOURCE](https://www.youtube.com/watch?v=k63qGsH1jLo&t=905s)\n",
    "\n",
    "[EXAMPLE](https://keras.io/examples/vision/knowledge_distillation/)\n",
    "\n",
    "> * Unlike Transfer learning where the weights are copied from a pre-trained very deep network and changing only the last few layers, here the pre trained network acts as a supervisor or a guide which allows a lesser network compare its output and provide additional loss \n",
    "> * The pre trained (teacher) is allowed to classify along with smaller network (in depth and # of nodes) then before the backpropagation, along with the actual loss, loss between both the outputs are compared and summed. The next training is based on this loss\n",
    "> * This is especially good when a small network needs to be deployed with high performance and fast training\n",
    "> * In this case while computing loss between teacher and student, softmax function is modified a bit with exponent getting fraction of value in denominator and numerator this fraction is a hyperparameter ??? how does this effect?\n",
    "> * This can be called a soft lable\n",
    "![](https://miro.medium.com/max/1838/0*VGV3vCW27E4UV5dM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871c6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fd984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "660e605a",
   "metadata": {},
   "source": [
    "### 7.9.2 PROS and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181d954",
   "metadata": {},
   "source": [
    "**PROS**\n",
    "> * Useful to create smaller but almost as good as the teacher models for delpying on small devices\n",
    "\n",
    "**CONS**\n",
    "> * The teacher must be first trained to guide the student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dce687",
   "metadata": {},
   "source": [
    "<a id='chapter_8'></a>\n",
    "## Chapter 8 : Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71122c2",
   "metadata": {},
   "source": [
    "<a id='chapter_9'></a>\n",
    "## Chapter 9 : Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb539049",
   "metadata": {},
   "source": [
    "<a id='chapter_10'></a>\n",
    "## Chapter 10 : Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4512742",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86a09f81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb60d7bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edfa7f8b",
   "metadata": {},
   "source": [
    "<a id='10.6'></a>\n",
    "### 10.6 PYTORCH\n",
    "[SOURCE](https://www.youtube.com/watch?v=c36lUUr864M&t=15s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59f645",
   "metadata": {},
   "source": [
    "#### 10.6.1 About\n",
    "> * FB's framework for DL\n",
    "> * CUDA ???\n",
    "> * any function with _ after it means inplace operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52161e16",
   "metadata": {},
   "source": [
    "[INSTALLATION](https://www.youtube.com/watch?v=eodnCUzSeTk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6375051",
   "metadata": {},
   "source": [
    "#### 10.6.2 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c28e4b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2009ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for GPU via cuda except device paramrter eneds to be added while creating tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "320b246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCALAR : tensor([0.])\n",
      "VECTOR : tensor([1.4013e-45, 0.0000e+00, 0.0000e+00])\n",
      "2D : tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "3D : tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# empty scalar\n",
    "scalar = th.empty(1)\n",
    "print(f\"SCALAR : {scalar}\")\n",
    "\n",
    "# empty vector\n",
    "vector = th.empty(3)\n",
    "print(f\"VECTOR : {vector}\")\n",
    "\n",
    "# empty multidimensional vector\n",
    "vector2 = th.empty(3,2) # 3 rows, 2 columns\n",
    "print(f\"2D : {vector2}\")\n",
    "\n",
    "vector23 = th.empty(3,2,3) # 3 rows, 2 columns, 3 in z axis\n",
    "print(f\"3D : {vector23}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb0dd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random vector : tensor([[0.6008, 0.7545, 0.5031],\n",
      "        [0.4845, 0.6914, 0.8870]])\n",
      "0 vector : tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "1 vector : tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "1 float vector : tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# random value\n",
    "rvector = th.rand(2,3)\n",
    "print(f\"random vector : {rvector}\")\n",
    "\n",
    "# zero tensor\n",
    "vector0 = th.zeros(2,3)\n",
    "print(f\"0 vector : {vector0}\")\n",
    "\n",
    "# one tensor\n",
    "vector1 = th.ones(2,3)\n",
    "print(f\"1 vector : {vector1}\")\n",
    "\n",
    "# data type change\n",
    "vector1f = th.ones(2,3, dtype=th.float16)\n",
    "print(f\"1 float vector : {vector1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "710c6d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# shape\n",
    "print(vector1f.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64f55cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [5, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list to tensor\n",
    "ltt = th.tensor([[1,2,3,4],[5,0,0,0]])\n",
    "ltt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5423c5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector addition : tensor([[2.3450, 1.6173, 1.0281],\n",
      "        [2.2884, 2.2244, 0.6923]])\n",
      "vector addition : tensor([[2.3450, 1.6173, 1.0281],\n",
      "        [2.2884, 2.2244, 0.6923]])\n",
      "vector addition : tensor([[2.3450, 1.6173, 1.0281],\n",
      "        [2.2884, 2.2244, 0.6923]])\n",
      "vector addition inplace : tensor([[2.3450, 1.6173, 1.0281],\n",
      "        [2.2884, 2.2244, 0.6923]])\n",
      "vector subtraction : tensor([[-0.8080,  0.6138,  0.1416],\n",
      "        [-0.3632,  0.9735,  1.1736]])\n",
      "vector subtraction : tensor([[-0.8080,  0.6138,  0.1416],\n",
      "        [-0.3632,  0.9735,  1.1736]])\n",
      "vector subtraction : tensor([[-0.8080,  0.6138,  0.1416],\n",
      "        [-0.3632,  0.9735,  1.1736]])\n",
      "vector subtraction inplace : tensor([[ 0.8080, -0.6138, -0.1416],\n",
      "        [ 0.3632, -0.9735, -1.1736]])\n"
     ]
    }
   ],
   "source": [
    "# operations\n",
    "v1 = th.rand(2,3)*2 # element wise multiplication\n",
    "v2 = th.rand(2,3)\n",
    "\n",
    "print(f\"vector addition : {v1.add(v2)}\")\n",
    "print(f\"vector addition : {v1+v2}\")\n",
    "print(f\"vector addition : {th.add(v1,v2)}\")\n",
    "\n",
    "# inplace addition (function_)\n",
    "v2.add_(v1)\n",
    "print(f\"vector addition inplace : {v2}\")\n",
    "\n",
    "v1 = th.rand(2,3)*2 # element wise multiplication\n",
    "v2 = th.rand(2,3)\n",
    "\n",
    "print(f\"vector subtraction : {v1.sub(v2)}\")\n",
    "print(f\"vector subtraction : {v1-v2}\")\n",
    "print(f\"vector subtraction : {th.sub(v1,v2)}\")\n",
    "\n",
    "# inplace addition (function_)\n",
    "v2.sub_(v1)\n",
    "print(f\"vector subtraction inplace : {v2}\")\n",
    "\n",
    "# th.mul : *\n",
    "# th.div : /\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e58f453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ROW : tensor([ 0.8080, -0.6138, -0.1416])\n",
      "Second COLUMN : tensor([-0.6138, -0.9735])\n",
      "Tensor at 1,1 : -0.9735285043716431\n",
      "Value at 1,1 : -0.9735285043716431\n"
     ]
    }
   ],
   "source": [
    "# slicing\n",
    "print(f\"First ROW : {v2[0,:]}\")\n",
    "print(f\"Second COLUMN : {v2[:,1]}\")\n",
    "print(f\"Tensor at 1,1 : {v2[1,1]}\")\n",
    "print(f\"Scalar at 1,1 : {v2[1,1].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fdb1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make it 1 row : tensor([[ 0.8080, -0.6138, -0.1416,  0.3632, -0.9735, -1.1736]])\n",
      "make it 1 column : tensor([[ 0.8080],\n",
      "        [-0.6138],\n",
      "        [-0.1416],\n",
      "        [ 0.3632],\n",
      "        [-0.9735],\n",
      "        [-1.1736]])\n",
      "make it of equavalent shape : tensor([[ 0.8080, -0.6138],\n",
      "        [-0.1416,  0.3632],\n",
      "        [-0.9735, -1.1736]])\n"
     ]
    }
   ],
   "source": [
    "# rehspaing, -1 stands for find out what value should be based on the other parameter\n",
    "print(f\"make it 1 row : {v2.reshape(1,-1)}\")\n",
    "print(f\"make it 1 column : {v2.reshape(-1,1)}\")\n",
    "print(f\"make it of equavalent shape : {v2.reshape(3,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72c0b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor conversion\n",
    "v1 = th.rand(5)\n",
    "np_v1 = v1.numpy()\n",
    "print(type(np_v1))\n",
    "# !!!! both v1 and np_v1 points to one location change in one will affecrt the other !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83584671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor([1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# numpy array conversion\n",
    "np_v2 = np.ones(4)\n",
    "v2 = th.from_numpy(np_v2) # default is float64\n",
    "print(type(v2), v2)\n",
    "# How to change its type while converting ???\n",
    "# !!!! GPU tensor cant be converted to normal cpu !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2fda727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient decsent activated tensor\n",
    "w  = th.tensor(3., requires_grad=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56eded7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since it is + : tensor([ 1.1932, -0.7377,  0.4194,  2.1041], grad_fn=<AddBackward0>)\n",
      "Since it is - : tensor([-2.8068, -4.7377, -3.5806, -1.8959], grad_fn=<SubBackward0>)\n",
      "Since it is * : tensor([ 1.3019, 14.9899,  4.9964,  0.0217], grad_fn=<MulBackward0>)\n",
      "Since it is / : tensor([-0.4034, -1.3688, -0.7903,  0.0521], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# autograd : to calculate gradients\n",
    "gv = th.randn(4, requires_grad=True)\n",
    "gv\n",
    "# if a function needs to be drived w.r.t. gv\n",
    "# any operation, since grad=True, will create a computational grapg (like a node with i/p and o/p)\n",
    "# the operation performed will be the same in backward propagation\n",
    "a = gv+2\n",
    "print(f\"Since it is + : {a}\")\n",
    "s = gv-2\n",
    "print(f\"Since it is - : {s}\")\n",
    "m = gv*gv*2\n",
    "print(f\"Since it is * : {m}\")\n",
    "d = gv/2\n",
    "print(f\"Since it is / : {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b240d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3275, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.8068, -2.7377, -1.5806,  0.1041])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to compute the backward : more info while working on neural neywork\n",
    "z = m.mean() # to get a scalar value\n",
    "print(z)\n",
    "z.backward() # dz/dgv\n",
    "gv.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ea04463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.3275)\n"
     ]
    }
   ],
   "source": [
    "# to prevent from tracking radient\n",
    "# 1.inplace \n",
    "w.requires_grad_(False)\n",
    "# 2.not inplace\n",
    "wn = w.detach()\n",
    "# 3.with with\n",
    "with th.no_grad():\n",
    "    y = z+2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7b9a31b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent accumulation of gradients, always use this after backward()\n",
    "# d.grad.zero_()\n",
    "# also used in optimizers, more on it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "30fa8b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back propagation\n",
    "# chain rule of differentiation is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "47999796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = th.tensor(1.0)\n",
    "y = th.tensor(2.0)\n",
    "weight = th.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass : to get loss\n",
    "y_predicted = weight*x\n",
    "loss = (y-y_predicted)**2\n",
    "print(loss)\n",
    "\n",
    "# backward prop\n",
    "print(loss.backward())\n",
    "weight.grad\n",
    "\n",
    "# update wieghts based on optimizer and start again with the forward pass till the losses are minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "da2ce6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before traning for value of 6 : 0.0\n",
      "Epoch : 1: w = 2.200 , loss = 44.000\n",
      "Epoch : 2: w = 1.980 , loss = 0.440\n",
      "Epoch : 3: w = 2.002 , loss = 0.004\n",
      "Epoch : 4: w = 2.000 , loss = 0.000\n",
      "Prediction after traning for value of 6 : 11.998799371719361\n"
     ]
    }
   ],
   "source": [
    "# gradient descent using Linear regression, manually\n",
    "# assuming the relation si y = w*x\n",
    "X = np.array([1,2,3,4,5], dtype=np.float32)\n",
    "y = np.array([2,4,6,8,10], dtype=np.float32)\n",
    "weight = 0.0\n",
    "\n",
    "# model prediction with intial weight\n",
    "def forward(x):\n",
    "    return weight * x # just the linear euqtion\n",
    "# cal loss (MSE)\n",
    "def loss(y, y_pred):\n",
    "    return ((y-y_pred)**2).mean() # how far off are the points from the line\n",
    "# gradient = differentiating loss w.r.t. weight\n",
    "def gradient(x,y,y_pred):\n",
    "    return np.dot(2*x, y_pred - y).mean() # differentiation of loss function\n",
    "\n",
    "print(f\"Prediction before traning for value of 6 : {forward(6)}\")\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "iterations = 10\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    # prediction using forward\n",
    "    y_pred = forward(X) # preidct first\n",
    "    l = loss(y,y_pred) # find how off are the values\n",
    "    dw = gradient(X,y,y_pred) # find gradient\n",
    "    # update weights\n",
    "    weight = weight - (learning_rate*dw) # learn and update weights\n",
    "    print(f\"Epoch : {epoch+1}: w = {weight:.3f} , loss = {l:.3f}\")\n",
    "    if(l<0.001): # if loss less than a threshold stop learning\n",
    "        break\n",
    "    \n",
    "print(f\"Prediction after traning for value of 6 : {forward(6)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c9590497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before traning for value of 6 : 0.0\n",
      "Epoch : 1: w = 0.440 , loss = 44.000\n",
      "Epoch : 2: w = 0.783 , loss = 26.770\n",
      "Epoch : 3: w = 1.051 , loss = 16.287\n",
      "Epoch : 4: w = 1.260 , loss = 9.909\n",
      "Epoch : 5: w = 1.423 , loss = 6.029\n",
      "Epoch : 6: w = 1.550 , loss = 3.668\n",
      "Epoch : 7: w = 1.649 , loss = 2.231\n",
      "Epoch : 8: w = 1.726 , loss = 1.358\n",
      "Epoch : 9: w = 1.786 , loss = 0.826\n",
      "Epoch : 10: w = 1.833 , loss = 0.503\n",
      "Epoch : 11: w = 1.870 , loss = 0.306\n",
      "Epoch : 12: w = 1.899 , loss = 0.186\n",
      "Epoch : 13: w = 1.921 , loss = 0.113\n",
      "Epoch : 14: w = 1.938 , loss = 0.069\n",
      "Epoch : 15: w = 1.952 , loss = 0.042\n",
      "Epoch : 16: w = 1.962 , loss = 0.025\n",
      "Epoch : 17: w = 1.971 , loss = 0.016\n",
      "Epoch : 18: w = 1.977 , loss = 0.009\n",
      "Epoch : 19: w = 1.982 , loss = 0.006\n",
      "Epoch : 20: w = 1.986 , loss = 0.003\n",
      "Epoch : 21: w = 1.989 , loss = 0.002\n",
      "Epoch : 22: w = 1.992 , loss = 0.001\n",
      "Epoch : 23: w = 1.993 , loss = 0.001\n",
      "Prediction after traning for value of 6 : 11.960430145263672\n"
     ]
    }
   ],
   "source": [
    "# gradient descent using Linear regression, torch\n",
    "# assuming the relation si y = w*x\n",
    "X = th.tensor([1,2,3,4,5], dtype=th.float32)\n",
    "y = th.tensor([2,4,6,8,10], dtype=th.float32)\n",
    "\n",
    "weight = th.tensor(0.0, dtype = th.float32, requires_grad=True) # to track gradeints\n",
    "\n",
    "# model prediction with intial weight\n",
    "def forward(x):\n",
    "    return weight * x # just the linear euqtion\n",
    "\n",
    "# cal loss (MSE)\n",
    "def loss(y, y_pred):\n",
    "    return ((y-y_pred)**2).mean() # how far off are the points from the line\n",
    "\n",
    "# gradient function goes away as it is now inbuint in torch\n",
    "\n",
    "print(f\"Prediction before traning for value of 6 : {forward(6)}\")\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "iterations = 100\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    # prediction using forward\n",
    "    y_pred = forward(X) # preidct first\n",
    "    l = loss(y,y_pred) # find how off are the values\n",
    "    l.backward() # to get gradient of l based on weihgt\n",
    "    # update weights\n",
    "    # to prevent this step form involving in computational graph\n",
    "    with th.no_grad():\n",
    "        # instead of assinign this can be done to avoid error\n",
    "        weight.copy_(weight - (learning_rate*weight.grad)) # learn and update weights\n",
    "    \n",
    "    weight.grad.zero_() # to avoid cumulative gradient at every epoch\n",
    "    print(f\"Epoch : {epoch+1}: w = {weight:.3f} , loss = {l:.3f}\")\n",
    "    if(l<0.001): # if loss less than a threshold stop learning\n",
    "        break\n",
    "    \n",
    "print(f\"Prediction after traning for value of 6 : {forward(6)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68900044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before traning for value of 6 : -4.778003215789795\n",
      "Epoch : 1: w = -0.336 , loss = 80.889\n",
      "Epoch : 2: w = 0.119 , loss = 47.309\n",
      "Epoch : 3: w = 0.467 , loss = 27.739\n",
      "Epoch : 4: w = 0.733 , loss = 16.334\n",
      "Epoch : 5: w = 0.936 , loss = 9.686\n",
      "Epoch : 6: w = 1.092 , loss = 5.811\n",
      "Epoch : 7: w = 1.211 , loss = 3.552\n",
      "Epoch : 8: w = 1.302 , loss = 2.234\n",
      "Epoch : 9: w = 1.372 , loss = 1.465\n",
      "Epoch : 10: w = 1.426 , loss = 1.016\n",
      "Epoch : 11: w = 1.467 , loss = 0.753\n",
      "Epoch : 12: w = 1.499 , loss = 0.599\n",
      "Epoch : 13: w = 1.524 , loss = 0.508\n",
      "Epoch : 14: w = 1.543 , loss = 0.453\n",
      "Epoch : 15: w = 1.558 , loss = 0.421\n",
      "Epoch : 16: w = 1.570 , loss = 0.401\n",
      "Epoch : 17: w = 1.579 , loss = 0.388\n",
      "Epoch : 18: w = 1.586 , loss = 0.380\n",
      "Epoch : 19: w = 1.592 , loss = 0.374\n",
      "Epoch : 20: w = 1.597 , loss = 0.369\n",
      "Epoch : 21: w = 1.601 , loss = 0.366\n",
      "Epoch : 22: w = 1.604 , loss = 0.362\n",
      "Epoch : 23: w = 1.607 , loss = 0.360\n",
      "Epoch : 24: w = 1.610 , loss = 0.357\n",
      "Epoch : 25: w = 1.612 , loss = 0.354\n",
      "Epoch : 26: w = 1.614 , loss = 0.352\n",
      "Epoch : 27: w = 1.616 , loss = 0.349\n",
      "Epoch : 28: w = 1.618 , loss = 0.347\n",
      "Epoch : 29: w = 1.619 , loss = 0.345\n",
      "Epoch : 30: w = 1.621 , loss = 0.342\n",
      "Epoch : 31: w = 1.622 , loss = 0.340\n",
      "Epoch : 32: w = 1.624 , loss = 0.338\n",
      "Epoch : 33: w = 1.625 , loss = 0.335\n",
      "Epoch : 34: w = 1.626 , loss = 0.333\n",
      "Epoch : 35: w = 1.628 , loss = 0.331\n",
      "Epoch : 36: w = 1.629 , loss = 0.329\n",
      "Epoch : 37: w = 1.630 , loss = 0.326\n",
      "Epoch : 38: w = 1.631 , loss = 0.324\n",
      "Epoch : 39: w = 1.633 , loss = 0.322\n",
      "Epoch : 40: w = 1.634 , loss = 0.320\n",
      "Epoch : 41: w = 1.635 , loss = 0.318\n",
      "Epoch : 42: w = 1.636 , loss = 0.316\n",
      "Epoch : 43: w = 1.638 , loss = 0.313\n",
      "Epoch : 44: w = 1.639 , loss = 0.311\n",
      "Epoch : 45: w = 1.640 , loss = 0.309\n",
      "Epoch : 46: w = 1.641 , loss = 0.307\n",
      "Epoch : 47: w = 1.643 , loss = 0.305\n",
      "Epoch : 48: w = 1.644 , loss = 0.303\n",
      "Epoch : 49: w = 1.645 , loss = 0.301\n",
      "Epoch : 50: w = 1.646 , loss = 0.299\n",
      "Epoch : 51: w = 1.647 , loss = 0.297\n",
      "Epoch : 52: w = 1.649 , loss = 0.295\n",
      "Epoch : 53: w = 1.650 , loss = 0.293\n",
      "Epoch : 54: w = 1.651 , loss = 0.291\n",
      "Epoch : 55: w = 1.652 , loss = 0.289\n",
      "Epoch : 56: w = 1.653 , loss = 0.287\n",
      "Epoch : 57: w = 1.655 , loss = 0.285\n",
      "Epoch : 58: w = 1.656 , loss = 0.283\n",
      "Epoch : 59: w = 1.657 , loss = 0.281\n",
      "Epoch : 60: w = 1.658 , loss = 0.279\n",
      "Epoch : 61: w = 1.659 , loss = 0.278\n",
      "Epoch : 62: w = 1.660 , loss = 0.276\n",
      "Epoch : 63: w = 1.661 , loss = 0.274\n",
      "Epoch : 64: w = 1.663 , loss = 0.272\n",
      "Epoch : 65: w = 1.664 , loss = 0.270\n",
      "Epoch : 66: w = 1.665 , loss = 0.268\n",
      "Epoch : 67: w = 1.666 , loss = 0.266\n",
      "Epoch : 68: w = 1.667 , loss = 0.265\n",
      "Epoch : 69: w = 1.668 , loss = 0.263\n",
      "Epoch : 70: w = 1.669 , loss = 0.261\n",
      "Epoch : 71: w = 1.671 , loss = 0.259\n",
      "Epoch : 72: w = 1.672 , loss = 0.258\n",
      "Epoch : 73: w = 1.673 , loss = 0.256\n",
      "Epoch : 74: w = 1.674 , loss = 0.254\n",
      "Epoch : 75: w = 1.675 , loss = 0.252\n",
      "Epoch : 76: w = 1.676 , loss = 0.251\n",
      "Epoch : 77: w = 1.677 , loss = 0.249\n",
      "Epoch : 78: w = 1.678 , loss = 0.247\n",
      "Epoch : 79: w = 1.679 , loss = 0.246\n",
      "Epoch : 80: w = 1.680 , loss = 0.244\n",
      "Epoch : 81: w = 1.681 , loss = 0.242\n",
      "Epoch : 82: w = 1.683 , loss = 0.241\n",
      "Epoch : 83: w = 1.684 , loss = 0.239\n",
      "Epoch : 84: w = 1.685 , loss = 0.237\n",
      "Epoch : 85: w = 1.686 , loss = 0.236\n",
      "Epoch : 86: w = 1.687 , loss = 0.234\n",
      "Epoch : 87: w = 1.688 , loss = 0.233\n",
      "Epoch : 88: w = 1.689 , loss = 0.231\n",
      "Epoch : 89: w = 1.690 , loss = 0.230\n",
      "Epoch : 90: w = 1.691 , loss = 0.228\n",
      "Epoch : 91: w = 1.692 , loss = 0.226\n",
      "Epoch : 92: w = 1.693 , loss = 0.225\n",
      "Epoch : 93: w = 1.694 , loss = 0.223\n",
      "Epoch : 94: w = 1.695 , loss = 0.222\n",
      "Epoch : 95: w = 1.696 , loss = 0.220\n",
      "Epoch : 96: w = 1.697 , loss = 0.219\n",
      "Epoch : 97: w = 1.698 , loss = 0.217\n",
      "Epoch : 98: w = 1.699 , loss = 0.216\n",
      "Epoch : 99: w = 1.700 , loss = 0.215\n",
      "Epoch : 100: w = 1.701 , loss = 0.213\n",
      "Prediction after traning for value of 6 : 11.286256790161133\n"
     ]
    }
   ],
   "source": [
    "# TRAINING PIPELINE\n",
    "'''\n",
    "1. Design model with input and output specifications\n",
    "2. Construct loass function and select optimizer\n",
    "3. Training loop : \n",
    "    1. Forward pass : predict\n",
    "    2. Backward pass : update weights using gradients\n",
    "'''\n",
    "import torch as th\n",
    "from torch import nn\n",
    "# gradient descent using Linear regression, torch\n",
    "# assuming the relation si y = w*x\n",
    "X = th.tensor([[1],[2],[3],[4],[5]], dtype=th.float32)\n",
    "y = th.tensor([[2],[4],[6],[8],[10]], dtype=th.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "test_tensor = th.tensor([6], dtype=th.float32)\n",
    "\n",
    "# forward and weight using torch\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# loss function goes way as its iunbuilt \n",
    "# gradient function goes away as it is now inbuint in torch\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "iterations = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = th.optim.SGD(model.parameters(), learning_rate)\n",
    "\n",
    "print(f\"Prediction before traning for value of 6 : {model(test_tensor).item()}\")\n",
    "\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    # prediction using forward\n",
    "    y_pred = model(X) # preidct first\n",
    "    l = loss(y,y_pred) # find how off are the values\n",
    "    l.backward() # to get gradient of l based on weihgt\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad() # to avoid cumulative gradient at every epoch \n",
    "    [w,b] = model.parameters()\n",
    "    print(f\"Epoch : {epoch+1}: w = {w[0][0].item():.3f} , loss = {l:.3f}\")\n",
    "    if(l<0.001): # if loss less than a threshold stop learning\n",
    "        break\n",
    "    \n",
    "print(f\"Prediction after traning for value of 6 : {model(test_tensor).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e28c706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss = 1171.4404296875\n",
      "10: loss = 927.19091796875\n",
      "20: loss = 758.3771362304688\n",
      "30: loss = 641.6405639648438\n",
      "40: loss = 560.87548828125\n",
      "50: loss = 504.9706115722656\n",
      "60: loss = 466.2557373046875\n",
      "70: loss = 439.4329833984375\n",
      "80: loss = 420.8414306640625\n",
      "90: loss = 407.9497985839844\n",
      "100: loss = 399.0069885253906\n",
      "110: loss = 392.80096435546875\n",
      "120: loss = 388.49267578125\n",
      "130: loss = 385.500732421875\n",
      "140: loss = 383.4221496582031\n",
      "150: loss = 381.97772216796875\n",
      "160: loss = 380.9736328125\n",
      "170: loss = 380.27545166015625\n",
      "180: loss = 379.7898254394531\n",
      "190: loss = 379.451904296875\n",
      "200: loss = 379.2167663574219\n",
      "210: loss = 379.0531311035156\n",
      "220: loss = 378.9391174316406\n",
      "230: loss = 378.8597412109375\n",
      "240: loss = 378.8044738769531\n",
      "250: loss = 378.76593017578125\n",
      "260: loss = 378.7391357421875\n",
      "270: loss = 378.7203674316406\n",
      "280: loss = 378.7073059082031\n",
      "290: loss = 378.6982421875\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[29.7353]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# preprocessing\n",
    "X_, y_ = datasets.make_regression(n_samples = 1000, n_features=1, noise=20, random_state=4)\n",
    "\n",
    "X = th.from_numpy(X_.astype(np.float32))\n",
    "y = th.from_numpy(y_.astype(np.float32))\n",
    "y = y.view(y.shape[0],1) # to rehsape as one column\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 300\n",
    "\n",
    "# design the model\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# define loss and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = th.optim.SGD(model.parameters(), learning_rate)\n",
    "\n",
    "# traning loop\n",
    "for i in range(epochs):\n",
    "    # forward\n",
    "    y_pred = model(X)\n",
    "    loss = loss_function(y_pred, y)\n",
    "    \n",
    "    # backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # avoid cumulative gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if(i%10==0):\n",
    "        print(f'{i}: loss = {loss.item()}')\n",
    "\n",
    "# plottting, with removing this part from computational graph\n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_, y_,'ro') # scatter plot\n",
    "plt.plot(X_, predicted,'b') # regression line\n",
    "plt.show()\n",
    "print(model.weight)\n",
    "# ??? how to get the quation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018ada9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features : 30\n",
      "Samples: 398\n",
      "0: loss = 0.7597928643226624\n",
      "10: loss = 0.5752801895141602\n",
      "20: loss = 0.47288861870765686\n",
      "30: loss = 0.40972068905830383\n",
      "40: loss = 0.3665858805179596\n",
      "50: loss = 0.3349401652812958\n",
      "60: loss = 0.3105252683162689\n",
      "70: loss = 0.2909853458404541\n",
      "80: loss = 0.2749064862728119\n",
      "90: loss = 0.2613849937915802\n",
      "100: loss = 0.2498140186071396\n",
      "110: loss = 0.23976951837539673\n",
      "120: loss = 0.23094545304775238\n",
      "130: loss = 0.22311486303806305\n",
      "140: loss = 0.21610556542873383\n",
      "150: loss = 0.20978431403636932\n",
      "160: loss = 0.20404617488384247\n",
      "170: loss = 0.19880734384059906\n",
      "180: loss = 0.19399985671043396\n",
      "190: loss = 0.18956810235977173\n",
      "200: loss = 0.185466006398201\n",
      "210: loss = 0.1816549301147461\n",
      "220: loss = 0.17810240387916565\n",
      "230: loss = 0.1747807264328003\n",
      "240: loss = 0.17166626453399658\n",
      "250: loss = 0.16873857378959656\n",
      "260: loss = 0.16597990691661835\n",
      "270: loss = 0.1633748710155487\n",
      "280: loss = 0.16090987622737885\n",
      "290: loss = 0.15857303142547607\n",
      "Accuracy : 0.9298245906829834\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# preprocessing\n",
    "data = datasets.load_breast_cancer() # binary classification dataset\n",
    "X, y  = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=43) # 70-30 split\n",
    "\n",
    "# scaleing the features, for 0 mean and 1 variance\n",
    "sc = StandardScaler() # ??? why fit transfor and just transform\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = th.from_numpy(X_train.astype(np.float32))\n",
    "X_test = th.from_numpy(X_test.astype(np.float32))\n",
    "y_train = th.from_numpy(y_train.astype(np.float32))\n",
    "y_test = th.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0],1) # to rehsape as one column\n",
    "y_test = y_test.view(y_test.shape[0],1) # to rehsape as one column\n",
    "\n",
    "n_samples, n_features = X_train.shape\n",
    "print(f'Features : {n_features}\\nSamples: {n_samples}')\n",
    "\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 300\n",
    "\n",
    "# design the model : g = sigmoid(f), f = wx+b\n",
    "class LogReg(nn.Module): # exteneding form nn.Module\n",
    "    def __init__(self, n_input_featuers):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_featuers, 1) # since the output has to be just one category\n",
    "    \n",
    "    def forward(self, data): #name should change\n",
    "        return th.sigmoid(self.linear(data)) # to get either 0 or 1\n",
    "\n",
    "model = LogReg(n_features)\n",
    "\n",
    "            \n",
    "# def forward(n_input_featuers, n_output, data):\n",
    "#     linear = nn.Linear(n_input_featuers, n_output)\n",
    "#     return th.sigmoid(linear(data))\n",
    "        \n",
    "# model = forward(n_features,1,X_train)\n",
    "\n",
    "# define loss and optimizer\n",
    "loss_function = nn.BCELoss() # binary entropy loss\n",
    "optimizer = th.optim.SGD(model.parameters(), learning_rate)\n",
    "\n",
    "# traning loop\n",
    "for i in range(epochs):\n",
    "    # forward\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    \n",
    "    # backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # avoid cumulative gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if(i%10==0):\n",
    "        print(f'{i}: loss = {loss.item()}')\n",
    "\n",
    "# finding out performance in test data set, with removing this part from computational graph\n",
    "with th.no_grad():\n",
    "    y_pred = model(X_test).round()\n",
    "    accuracy = y_pred.eq(y_test).sum()/y_test.shape[0]\n",
    "    print(f'Accuracy : {accuracy}')\n",
    "# ??? how to get the quation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c66891d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43597de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and dataloader : to use batches for traning and load hue datasets\n",
    "# epoch : forward and backward for all traning points\n",
    "# batch_size : forward and backward for one sample from traning\n",
    "# # of iterations : epoch size/ batch size\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db3dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset inherited from Dataset\n",
    "class WineDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        # load data\n",
    "        xy = np.loadtxt('./data/winedata.txt', delimiter=\",\", dtype=np.float32, skiprows=1)\n",
    "        self.x = th.from_numpy(xy[:,1:]) #except the first column which is the target\n",
    "        self.y = th.from_numpy(xy[:,[0]]) # only the first column\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # indexing \n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        # to get size\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41394ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
       "         3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
       "         1.0650e+03]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data using custom dataset class\n",
    "dataset = WineDataSet()\n",
    "first_row = dataset[0]\n",
    "features, label = first_row\n",
    "features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a0d5191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.2520e+01, 2.4300e+00, 2.1700e+00, 2.1000e+01, 8.8000e+01, 2.5500e+00,\n",
       "          2.2700e+00, 2.6000e-01, 1.2200e+00, 2.0000e+00, 9.0000e-01, 2.7800e+00,\n",
       "          3.2500e+02],\n",
       "         [1.1870e+01, 4.3100e+00, 2.3900e+00, 2.1000e+01, 8.2000e+01, 2.8600e+00,\n",
       "          3.0300e+00, 2.1000e-01, 2.9100e+00, 2.8000e+00, 7.5000e-01, 3.6400e+00,\n",
       "          3.8000e+02],\n",
       "         [1.2990e+01, 1.6700e+00, 2.6000e+00, 3.0000e+01, 1.3900e+02, 3.3000e+00,\n",
       "          2.8900e+00, 2.1000e-01, 1.9600e+00, 3.3500e+00, 1.3100e+00, 3.5000e+00,\n",
       "          9.8500e+02],\n",
       "         [1.1640e+01, 2.0600e+00, 2.4600e+00, 2.1600e+01, 8.4000e+01, 1.9500e+00,\n",
       "          1.6900e+00, 4.8000e-01, 1.3500e+00, 2.8000e+00, 1.0000e+00, 2.7500e+00,\n",
       "          6.8000e+02]]),\n",
       " tensor([[2.],\n",
       "         [2.],\n",
       "         [2.],\n",
       "         [2.]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True,\\\n",
    "#                        num_workers=2 # to make loading faster with 2 sub processes\n",
    "                       ) \n",
    "dataiter = iter(dataloader)\n",
    "data = dataiter.next()\n",
    "features, label = data\n",
    "features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc76ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset transforms\n",
    "# in torchvision, various transformations for image based data set \n",
    "# compose can be used to have a lit of transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9150f02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64391426, 0.23688282, 0.08714432, 0.0320586 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax functions : to convert numbers to probability for each class in classification\n",
    "import numpy as np\n",
    "x = [3,2,1,0]\n",
    "np.exp(x)/np.sum(np.exp(x)) # softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2590818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6439, 0.2369, 0.0871, 0.0321])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "x = th.tensor([3,2,1,0], dtype= th.float32)\n",
    "th.softmax(x, dim=0) # along the axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ffecba",
   "metadata": {},
   "source": [
    "after softmax, in multi class classifuication, cross entropy loss is calculated to see how far off the predictions are\n",
    "![](https://miro.medium.com/max/498/1*ulvZu4-sgPd8SLAAdaiwWw.png)\n",
    "\n",
    "> * -1/n(sum over (log of the probabilties from softmax*actual class))\n",
    "> * Where actual class is on hot encoded i.e. either 0 or 1 even in case of multiple classes\n",
    "> * Must be as low as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03bd465b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2887), tensor(2.6211))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in torch corss entorpy loss = log softmax + negative log likliehood loss\n",
    "# so no need for separte softmax layer ( predicted values shouldn be probability)\n",
    "# target shouldn be one hot\n",
    "import torch.nn as nn\n",
    "loss = nn.CrossEntropyLoss()\n",
    "y = th.tensor([0,1]) # for 3 class classification\n",
    "y_pred_good = th.tensor([[3.0,1.0,2.0],[0.0,3.0,1.0]])\n",
    "y_pred_bad = th.tensor([[0.0,1.0,2.0],[3.0,1.2, 3.5]])\n",
    "l1 = loss(y_pred_good,y)\n",
    "l2 = loss(y_pred_bad,y)\n",
    "l1,l2 # lesser the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272e144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "class NNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.Relu()\n",
    "        self.signmoid = nn.Signmoid()\n",
    "        self.linear2 = nn.Linear(hidden_size,1)\n",
    "    def forward(self,x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.signmoid(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "# or define only linear part of the function\n",
    "class NNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size,1)\n",
    "    def forward(self,x):\n",
    "        out = th.relu(self.linear1(x))\n",
    "        out = th.signmoid(self.linear2(out))\n",
    "        return out\n",
    "# th.softmax, th.tanh, \n",
    "# nn.relu, nn.softmax, nn.tanh,\n",
    "# F.leaky_relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd21ec4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07adc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mnist classification using feedforward NN\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as trans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# device configuration\n",
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "\n",
    "# hyperparameters\n",
    "input_size = 28*28\n",
    "hidden_size = 100 # ??? neurons in a layer\n",
    "num_classes = 10 # output category count \n",
    "n_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "training_dataset = tv.datasets.MNIST(root = \"./data\", train=True,\\\n",
    "                                    transform = trans.ToTensor(), download=True)\n",
    "testing_dataset =  tv.datasets.MNIST(root = \"./data\", train=False,\\\n",
    "                                    transform = trans.ToTensor())\n",
    "train_loader = th.utils.data.DataLoader(dataset=training_dataset, shuffle=True, batch_size= batch_size)\n",
    "test_loader = th.utils.data.DataLoader(dataset=testing_dataset, batch_size= batch_size)\n",
    "\n",
    "sample, label = iter(train_loader).next()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(sample[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2103fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification using 1 hiddne layer fully conenced NN\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, intput_size, hidden_size, num_classes):\n",
    "        super(NN, self).__init__()\n",
    "        self.L1 = nn.Linear(input_size, hidden_size)\n",
    "        self.Lrelu = nn.LeakyReLU()\n",
    "        self.L2 = nn.Linear( hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.L1(x)\n",
    "        out = self.Lrelu(out)\n",
    "        out = self.L2(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "model = NN(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b7823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss() # with softmax in it\n",
    "optimizer = th.optim.Adam(model.parameters() , lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a53ffc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0/2, Step : 0/600, Loss : 2.258216381072998\n",
      "Epoch : 0/2, Step : 100/600, Loss : 0.5925595164299011\n",
      "Epoch : 0/2, Step : 200/600, Loss : 0.34750375151634216\n",
      "Epoch : 0/2, Step : 300/600, Loss : 0.2630517780780792\n",
      "Epoch : 0/2, Step : 400/600, Loss : 0.25573158264160156\n",
      "Epoch : 0/2, Step : 500/600, Loss : 0.1336500197649002\n",
      "Epoch : 1/2, Step : 0/600, Loss : 0.19310316443443298\n",
      "Epoch : 1/2, Step : 100/600, Loss : 0.3001777231693268\n",
      "Epoch : 1/2, Step : 200/600, Loss : 0.21431808173656464\n",
      "Epoch : 1/2, Step : 300/600, Loss : 0.19498279690742493\n",
      "Epoch : 1/2, Step : 400/600, Loss : 0.18842555582523346\n",
      "Epoch : 1/2, Step : 500/600, Loss : 0.12866386771202087\n"
     ]
    }
   ],
   "source": [
    "# traning loop\n",
    "n_steps = len(train_loader)\n",
    "for epoch in range(n_epochs):\n",
    "    # loop over batches\n",
    "    for i, (sample, label) in enumerate(train_loader):\n",
    "        # resharpe to flatten it\n",
    "        images = sample.reshape(-1, input_size).to(device) # for gpu\n",
    "        labels = label.to(device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels) # compring precdited and actual\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad() # to aopvid cumulation\n",
    "        loss.backward()  # for back propagation\n",
    "        optimizer.step() # to udate paramters \n",
    "        if(i%100==0):\n",
    "            print(f'Epoch : {epoch}/{n_epochs}, Step : {i}/{n_steps}, Loss : {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7eaca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 95.39999999999999\n"
     ]
    }
   ],
   "source": [
    "# testing and evaluation\n",
    "with th.no_grad(): # to not include this in con[putational graph\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "    for i,l in test_loader:\n",
    "        images = i.reshape(-1, input_size).to(device) # for gpu\n",
    "        labels = l.to(device)\n",
    "        output = model(images) # to get predictions\n",
    "        \n",
    "        # values of classes\n",
    "        _, predictions= th.max(output, 1) # along the first dimension\n",
    "        samples+= labels.shape[0]\n",
    "        correct += (predictions==labels).sum().item()\n",
    "    acc = (correct/samples)*100\n",
    "    print(f\"Accuracy : {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e14d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.14.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614c7d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2c5b58c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df850bed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1472476",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09953222",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6b4f085",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7a8c8de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad4d5023",
   "metadata": {},
   "source": [
    "<a id='10.10'></a>\n",
    "### 10.10 PyCaret\n",
    "[SOURCE](https://www.youtube.com/watch?v=sL-4rWuEiVw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03881380",
   "metadata": {},
   "source": [
    "#### 10.10.1 About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091f3272",
   "metadata": {},
   "source": [
    "> * Automates the process of model selection and pre-processing after evaluating them on multiple metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8183b",
   "metadata": {},
   "source": [
    "#### 10.10.2 Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184dddb4",
   "metadata": {},
   "source": [
    "!pip3 install pycaret shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b96bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd932fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>feat_11</th>\n",
       "      <th>feat_12</th>\n",
       "      <th>feat_13</th>\n",
       "      <th>feat_14</th>\n",
       "      <th>feat_15</th>\n",
       "      <th>feat_16</th>\n",
       "      <th>feat_17</th>\n",
       "      <th>feat_18</th>\n",
       "      <th>feat_19</th>\n",
       "      <th>feat_20</th>\n",
       "      <th>feat_21</th>\n",
       "      <th>feat_22</th>\n",
       "      <th>feat_23</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>feat_25</th>\n",
       "      <th>feat_26</th>\n",
       "      <th>feat_27</th>\n",
       "      <th>feat_28</th>\n",
       "      <th>feat_29</th>\n",
       "      <th>feat_30</th>\n",
       "      <th>feat_31</th>\n",
       "      <th>feat_32</th>\n",
       "      <th>feat_33</th>\n",
       "      <th>feat_34</th>\n",
       "      <th>feat_35</th>\n",
       "      <th>feat_36</th>\n",
       "      <th>feat_37</th>\n",
       "      <th>feat_38</th>\n",
       "      <th>feat_39</th>\n",
       "      <th>feat_40</th>\n",
       "      <th>feat_41</th>\n",
       "      <th>feat_42</th>\n",
       "      <th>feat_43</th>\n",
       "      <th>feat_44</th>\n",
       "      <th>feat_45</th>\n",
       "      <th>feat_46</th>\n",
       "      <th>feat_47</th>\n",
       "      <th>feat_48</th>\n",
       "      <th>feat_49</th>\n",
       "      <th>feat_50</th>\n",
       "      <th>feat_51</th>\n",
       "      <th>feat_52</th>\n",
       "      <th>feat_53</th>\n",
       "      <th>feat_54</th>\n",
       "      <th>feat_55</th>\n",
       "      <th>feat_56</th>\n",
       "      <th>feat_57</th>\n",
       "      <th>feat_58</th>\n",
       "      <th>feat_59</th>\n",
       "      <th>feat_60</th>\n",
       "      <th>feat_61</th>\n",
       "      <th>feat_62</th>\n",
       "      <th>feat_63</th>\n",
       "      <th>feat_64</th>\n",
       "      <th>feat_65</th>\n",
       "      <th>feat_66</th>\n",
       "      <th>feat_67</th>\n",
       "      <th>feat_68</th>\n",
       "      <th>feat_69</th>\n",
       "      <th>feat_70</th>\n",
       "      <th>feat_71</th>\n",
       "      <th>feat_72</th>\n",
       "      <th>feat_73</th>\n",
       "      <th>feat_74</th>\n",
       "      <th>feat_75</th>\n",
       "      <th>feat_76</th>\n",
       "      <th>feat_77</th>\n",
       "      <th>feat_78</th>\n",
       "      <th>feat_79</th>\n",
       "      <th>feat_80</th>\n",
       "      <th>feat_81</th>\n",
       "      <th>feat_82</th>\n",
       "      <th>feat_83</th>\n",
       "      <th>feat_84</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   feat_10  feat_11  feat_12  feat_13  feat_14  feat_15  feat_16  feat_17  \\\n",
       "0        0        1        0        0        0        0        0        2   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        1   \n",
       "3        1        1        0        1        0        0        1        1   \n",
       "4        0        0        0        0        0        0        0        4   \n",
       "\n",
       "   feat_18  feat_19  feat_20  feat_21  feat_22  feat_23  feat_24  feat_25  \\\n",
       "0        0        0        0        0        1        0        4        1   \n",
       "1        2        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        7        2   \n",
       "4        0        0        0        0        0        0        1        0   \n",
       "\n",
       "   feat_26  feat_27  feat_28  feat_29  feat_30  feat_31  feat_32  feat_33  \\\n",
       "0        1        0        0        2        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        1   \n",
       "3        2        0        0        0       58        0       10        0   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   feat_34  feat_35  feat_36  feat_37  feat_38  feat_39  feat_40  feat_41  \\\n",
       "0        0        1        0        0        0        0        1        0   \n",
       "1        0        0        0        1        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        3        0        0        0   \n",
       "4        0        0        0        0        0        0        0        1   \n",
       "\n",
       "   feat_42  feat_43  feat_44  feat_45  feat_46  feat_47  feat_48  feat_49  \\\n",
       "0        5        0        0        0        0        0        2        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        1        0   \n",
       "3        0        0        2        0        2        0        1        2   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   feat_50  feat_51  feat_52  feat_53  feat_54  feat_55  feat_56  feat_57  \\\n",
       "0        0        0        0        0        1        0        0        2   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        1        0        0   \n",
       "3        1        3        0        0        3        1        0        0   \n",
       "4        2        0        0        0        0        0        0        0   \n",
       "\n",
       "   feat_58  feat_59  feat_60  feat_61  feat_62  feat_63  feat_64  feat_65  \\\n",
       "0        0        0       11        0        1        1        0        1   \n",
       "1        1        0        0        0        0        0        1        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        0        0        0        0        0        0        0        2   \n",
       "4        0        0        0        0        1        0        0        0   \n",
       "\n",
       "   feat_66  feat_67  feat_68  feat_69  feat_70  feat_71  feat_72  feat_73  \\\n",
       "0        0        7        0        0        0        1        0        0   \n",
       "1        0        1        0        0        0        0        0        2   \n",
       "2        0        6        0        0        2        0        0        0   \n",
       "3        1        5        0        0        4        0        0        2   \n",
       "4        0        0        1        0        0        3        0        0   \n",
       "\n",
       "   feat_74  feat_75  feat_76  feat_77  feat_78  feat_79  feat_80  feat_81  \\\n",
       "0        0        0        0        0        0        2        1        0   \n",
       "1        1        0        1        0        1        0        0        0   \n",
       "2        0        0        0        0        0        0        0        1   \n",
       "3        1        0        1        0        0        1        1        2   \n",
       "4        0        0        0        0        0        4        0        1   \n",
       "\n",
       "   feat_82  feat_83  feat_84  feat_85  feat_86  feat_87  feat_88  feat_89  \\\n",
       "0        0        0        0        1        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        2        0       22        0        1        2        0        0   \n",
       "4        0        0        0        1        0        0        0        0   \n",
       "\n",
       "   feat_90  feat_91  feat_92  feat_93   target  \n",
       "0        0        0        0        0  Class_1  \n",
       "1        0        0        0        0  Class_1  \n",
       "2        0        0        0        0  Class_1  \n",
       "3        0        0        0        0  Class_1  \n",
       "4        1        0        0        0  Class_1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_csv(\"./data/Otto product/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ca02b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"] = df[\"target\"].apply(lambda x : int(x.split(\"_\")[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a533c1a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61878 entries, 0 to 61877\n",
      "Data columns (total 96 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       61878 non-null  int64 \n",
      " 1   feat_1   61878 non-null  int64 \n",
      " 2   feat_2   61878 non-null  int64 \n",
      " 3   feat_3   61878 non-null  int64 \n",
      " 4   feat_4   61878 non-null  int64 \n",
      " 5   feat_5   61878 non-null  int64 \n",
      " 6   feat_6   61878 non-null  int64 \n",
      " 7   feat_7   61878 non-null  int64 \n",
      " 8   feat_8   61878 non-null  int64 \n",
      " 9   feat_9   61878 non-null  int64 \n",
      " 10  feat_10  61878 non-null  int64 \n",
      " 11  feat_11  61878 non-null  int64 \n",
      " 12  feat_12  61878 non-null  int64 \n",
      " 13  feat_13  61878 non-null  int64 \n",
      " 14  feat_14  61878 non-null  int64 \n",
      " 15  feat_15  61878 non-null  int64 \n",
      " 16  feat_16  61878 non-null  int64 \n",
      " 17  feat_17  61878 non-null  int64 \n",
      " 18  feat_18  61878 non-null  int64 \n",
      " 19  feat_19  61878 non-null  int64 \n",
      " 20  feat_20  61878 non-null  int64 \n",
      " 21  feat_21  61878 non-null  int64 \n",
      " 22  feat_22  61878 non-null  int64 \n",
      " 23  feat_23  61878 non-null  int64 \n",
      " 24  feat_24  61878 non-null  int64 \n",
      " 25  feat_25  61878 non-null  int64 \n",
      " 26  feat_26  61878 non-null  int64 \n",
      " 27  feat_27  61878 non-null  int64 \n",
      " 28  feat_28  61878 non-null  int64 \n",
      " 29  feat_29  61878 non-null  int64 \n",
      " 30  feat_30  61878 non-null  int64 \n",
      " 31  feat_31  61878 non-null  int64 \n",
      " 32  feat_32  61878 non-null  int64 \n",
      " 33  feat_33  61878 non-null  int64 \n",
      " 34  feat_34  61878 non-null  int64 \n",
      " 35  feat_35  61878 non-null  int64 \n",
      " 36  feat_36  61878 non-null  int64 \n",
      " 37  feat_37  61878 non-null  int64 \n",
      " 38  feat_38  61878 non-null  int64 \n",
      " 39  feat_39  61878 non-null  int64 \n",
      " 40  feat_40  61878 non-null  int64 \n",
      " 41  feat_41  61878 non-null  int64 \n",
      " 42  feat_42  61878 non-null  int64 \n",
      " 43  feat_43  61878 non-null  int64 \n",
      " 44  feat_44  61878 non-null  int64 \n",
      " 45  feat_45  61878 non-null  int64 \n",
      " 46  feat_46  61878 non-null  int64 \n",
      " 47  feat_47  61878 non-null  int64 \n",
      " 48  feat_48  61878 non-null  int64 \n",
      " 49  feat_49  61878 non-null  int64 \n",
      " 50  feat_50  61878 non-null  int64 \n",
      " 51  feat_51  61878 non-null  int64 \n",
      " 52  feat_52  61878 non-null  int64 \n",
      " 53  feat_53  61878 non-null  int64 \n",
      " 54  feat_54  61878 non-null  int64 \n",
      " 55  feat_55  61878 non-null  int64 \n",
      " 56  feat_56  61878 non-null  int64 \n",
      " 57  feat_57  61878 non-null  int64 \n",
      " 58  feat_58  61878 non-null  int64 \n",
      " 59  feat_59  61878 non-null  int64 \n",
      " 60  feat_60  61878 non-null  int64 \n",
      " 61  feat_61  61878 non-null  int64 \n",
      " 62  feat_62  61878 non-null  int64 \n",
      " 63  feat_63  61878 non-null  int64 \n",
      " 64  feat_64  61878 non-null  int64 \n",
      " 65  feat_65  61878 non-null  int64 \n",
      " 66  feat_66  61878 non-null  int64 \n",
      " 67  feat_67  61878 non-null  int64 \n",
      " 68  feat_68  61878 non-null  int64 \n",
      " 69  feat_69  61878 non-null  int64 \n",
      " 70  feat_70  61878 non-null  int64 \n",
      " 71  feat_71  61878 non-null  int64 \n",
      " 72  feat_72  61878 non-null  int64 \n",
      " 73  feat_73  61878 non-null  int64 \n",
      " 74  feat_74  61878 non-null  int64 \n",
      " 75  feat_75  61878 non-null  int64 \n",
      " 76  feat_76  61878 non-null  int64 \n",
      " 77  feat_77  61878 non-null  int64 \n",
      " 78  feat_78  61878 non-null  int64 \n",
      " 79  feat_79  61878 non-null  int64 \n",
      " 80  feat_80  61878 non-null  int64 \n",
      " 81  feat_81  61878 non-null  int64 \n",
      " 82  feat_82  61878 non-null  int64 \n",
      " 83  feat_83  61878 non-null  int64 \n",
      " 84  feat_84  61878 non-null  int64 \n",
      " 85  feat_85  61878 non-null  int64 \n",
      " 86  feat_86  61878 non-null  int64 \n",
      " 87  feat_87  61878 non-null  int64 \n",
      " 88  feat_88  61878 non-null  int64 \n",
      " 89  feat_89  61878 non-null  int64 \n",
      " 90  feat_90  61878 non-null  int64 \n",
      " 91  feat_91  61878 non-null  int64 \n",
      " 92  feat_92  61878 non-null  int64 \n",
      " 93  feat_93  61878 non-null  int64 \n",
      " 94  target   61878 non-null  object\n",
      " 95  class    61878 non-null  int64 \n",
      "dtypes: int64(95), object(1)\n",
      "memory usage: 45.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d03cd00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_c3b49_row44_col1{\n",
       "            background-color:  lightgreen;\n",
       "        }</style><table id=\"T_c3b49_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Description</th>        <th class=\"col_heading level0 col1\" >Value</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_c3b49_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_c3b49_row0_col0\" class=\"data row0 col0\" >session_id</td>\n",
       "                        <td id=\"T_c3b49_row0_col1\" class=\"data row0 col1\" >6879</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_c3b49_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "                        <td id=\"T_c3b49_row1_col1\" class=\"data row1 col1\" >class</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_c3b49_row2_col0\" class=\"data row2 col0\" >Target Type</td>\n",
       "                        <td id=\"T_c3b49_row2_col1\" class=\"data row2 col1\" >Multiclass</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_c3b49_row3_col0\" class=\"data row3 col0\" >Label Encoded</td>\n",
       "                        <td id=\"T_c3b49_row3_col1\" class=\"data row3 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_c3b49_row4_col0\" class=\"data row4 col0\" >Original Data</td>\n",
       "                        <td id=\"T_c3b49_row4_col1\" class=\"data row4 col1\" >(61878, 96)</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_c3b49_row5_col0\" class=\"data row5 col0\" >Missing Values</td>\n",
       "                        <td id=\"T_c3b49_row5_col1\" class=\"data row5 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_c3b49_row6_col0\" class=\"data row6 col0\" >Numeric Features</td>\n",
       "                        <td id=\"T_c3b49_row6_col1\" class=\"data row6 col1\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_c3b49_row7_col0\" class=\"data row7 col0\" >Categorical Features</td>\n",
       "                        <td id=\"T_c3b49_row7_col1\" class=\"data row7 col1\" >94</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_c3b49_row8_col0\" class=\"data row8 col0\" >Ordinal Features</td>\n",
       "                        <td id=\"T_c3b49_row8_col1\" class=\"data row8 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_c3b49_row9_col0\" class=\"data row9 col0\" >High Cardinality Features</td>\n",
       "                        <td id=\"T_c3b49_row9_col1\" class=\"data row9 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_c3b49_row10_col0\" class=\"data row10 col0\" >High Cardinality Method</td>\n",
       "                        <td id=\"T_c3b49_row10_col1\" class=\"data row10 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_c3b49_row11_col0\" class=\"data row11 col0\" >Transformed Train Set</td>\n",
       "                        <td id=\"T_c3b49_row11_col1\" class=\"data row11 col1\" >(43314, 3652)</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_c3b49_row12_col0\" class=\"data row12 col0\" >Transformed Test Set</td>\n",
       "                        <td id=\"T_c3b49_row12_col1\" class=\"data row12 col1\" >(18564, 3652)</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_c3b49_row13_col0\" class=\"data row13 col0\" >Shuffle Train-Test</td>\n",
       "                        <td id=\"T_c3b49_row13_col1\" class=\"data row13 col1\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_c3b49_row14_col0\" class=\"data row14 col0\" >Stratify Train-Test</td>\n",
       "                        <td id=\"T_c3b49_row14_col1\" class=\"data row14 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_c3b49_row15_col0\" class=\"data row15 col0\" >Fold Generator</td>\n",
       "                        <td id=\"T_c3b49_row15_col1\" class=\"data row15 col1\" >StratifiedKFold</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_c3b49_row16_col0\" class=\"data row16 col0\" >Fold Number</td>\n",
       "                        <td id=\"T_c3b49_row16_col1\" class=\"data row16 col1\" >10</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_c3b49_row17_col0\" class=\"data row17 col0\" >CPU Jobs</td>\n",
       "                        <td id=\"T_c3b49_row17_col1\" class=\"data row17 col1\" >-1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_c3b49_row18_col0\" class=\"data row18 col0\" >Use GPU</td>\n",
       "                        <td id=\"T_c3b49_row18_col1\" class=\"data row18 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_c3b49_row19_col0\" class=\"data row19 col0\" >Log Experiment</td>\n",
       "                        <td id=\"T_c3b49_row19_col1\" class=\"data row19 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_c3b49_row20_col0\" class=\"data row20 col0\" >Experiment Name</td>\n",
       "                        <td id=\"T_c3b49_row20_col1\" class=\"data row20 col1\" >clf-default-name</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_c3b49_row21_col0\" class=\"data row21 col0\" >USI</td>\n",
       "                        <td id=\"T_c3b49_row21_col1\" class=\"data row21 col1\" >fa44</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "                        <td id=\"T_c3b49_row22_col0\" class=\"data row22 col0\" >Imputation Type</td>\n",
       "                        <td id=\"T_c3b49_row22_col1\" class=\"data row22 col1\" >simple</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "                        <td id=\"T_c3b49_row23_col0\" class=\"data row23 col0\" >Iterative Imputation Iteration</td>\n",
       "                        <td id=\"T_c3b49_row23_col1\" class=\"data row23 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "                        <td id=\"T_c3b49_row24_col0\" class=\"data row24 col0\" >Numeric Imputer</td>\n",
       "                        <td id=\"T_c3b49_row24_col1\" class=\"data row24 col1\" >mean</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "                        <td id=\"T_c3b49_row25_col0\" class=\"data row25 col0\" >Iterative Imputation Numeric Model</td>\n",
       "                        <td id=\"T_c3b49_row25_col1\" class=\"data row25 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "                        <td id=\"T_c3b49_row26_col0\" class=\"data row26 col0\" >Categorical Imputer</td>\n",
       "                        <td id=\"T_c3b49_row26_col1\" class=\"data row26 col1\" >constant</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "                        <td id=\"T_c3b49_row27_col0\" class=\"data row27 col0\" >Iterative Imputation Categorical Model</td>\n",
       "                        <td id=\"T_c3b49_row27_col1\" class=\"data row27 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "                        <td id=\"T_c3b49_row28_col0\" class=\"data row28 col0\" >Unknown Categoricals Handling</td>\n",
       "                        <td id=\"T_c3b49_row28_col1\" class=\"data row28 col1\" >least_frequent</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "                        <td id=\"T_c3b49_row29_col0\" class=\"data row29 col0\" >Normalize</td>\n",
       "                        <td id=\"T_c3b49_row29_col1\" class=\"data row29 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "                        <td id=\"T_c3b49_row30_col0\" class=\"data row30 col0\" >Normalize Method</td>\n",
       "                        <td id=\"T_c3b49_row30_col1\" class=\"data row30 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "                        <td id=\"T_c3b49_row31_col0\" class=\"data row31 col0\" >Transformation</td>\n",
       "                        <td id=\"T_c3b49_row31_col1\" class=\"data row31 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "                        <td id=\"T_c3b49_row32_col0\" class=\"data row32 col0\" >Transformation Method</td>\n",
       "                        <td id=\"T_c3b49_row32_col1\" class=\"data row32 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "                        <td id=\"T_c3b49_row33_col0\" class=\"data row33 col0\" >PCA</td>\n",
       "                        <td id=\"T_c3b49_row33_col1\" class=\"data row33 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "                        <td id=\"T_c3b49_row34_col0\" class=\"data row34 col0\" >PCA Method</td>\n",
       "                        <td id=\"T_c3b49_row34_col1\" class=\"data row34 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "                        <td id=\"T_c3b49_row35_col0\" class=\"data row35 col0\" >PCA Components</td>\n",
       "                        <td id=\"T_c3b49_row35_col1\" class=\"data row35 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "                        <td id=\"T_c3b49_row36_col0\" class=\"data row36 col0\" >Ignore Low Variance</td>\n",
       "                        <td id=\"T_c3b49_row36_col1\" class=\"data row36 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "                        <td id=\"T_c3b49_row37_col0\" class=\"data row37 col0\" >Combine Rare Levels</td>\n",
       "                        <td id=\"T_c3b49_row37_col1\" class=\"data row37 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "                        <td id=\"T_c3b49_row38_col0\" class=\"data row38 col0\" >Rare Level Threshold</td>\n",
       "                        <td id=\"T_c3b49_row38_col1\" class=\"data row38 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "                        <td id=\"T_c3b49_row39_col0\" class=\"data row39 col0\" >Numeric Binning</td>\n",
       "                        <td id=\"T_c3b49_row39_col1\" class=\"data row39 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "                        <td id=\"T_c3b49_row40_col0\" class=\"data row40 col0\" >Remove Outliers</td>\n",
       "                        <td id=\"T_c3b49_row40_col1\" class=\"data row40 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "                        <td id=\"T_c3b49_row41_col0\" class=\"data row41 col0\" >Outliers Threshold</td>\n",
       "                        <td id=\"T_c3b49_row41_col1\" class=\"data row41 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "                        <td id=\"T_c3b49_row42_col0\" class=\"data row42 col0\" >Remove Multicollinearity</td>\n",
       "                        <td id=\"T_c3b49_row42_col1\" class=\"data row42 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "                        <td id=\"T_c3b49_row43_col0\" class=\"data row43 col0\" >Multicollinearity Threshold</td>\n",
       "                        <td id=\"T_c3b49_row43_col1\" class=\"data row43 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "                        <td id=\"T_c3b49_row44_col0\" class=\"data row44 col0\" >Remove Perfect Collinearity</td>\n",
       "                        <td id=\"T_c3b49_row44_col1\" class=\"data row44 col1\" >True</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "                        <td id=\"T_c3b49_row45_col0\" class=\"data row45 col0\" >Clustering</td>\n",
       "                        <td id=\"T_c3b49_row45_col1\" class=\"data row45 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "                        <td id=\"T_c3b49_row46_col0\" class=\"data row46 col0\" >Clustering Iteration</td>\n",
       "                        <td id=\"T_c3b49_row46_col1\" class=\"data row46 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
       "                        <td id=\"T_c3b49_row47_col0\" class=\"data row47 col0\" >Polynomial Features</td>\n",
       "                        <td id=\"T_c3b49_row47_col1\" class=\"data row47 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
       "                        <td id=\"T_c3b49_row48_col0\" class=\"data row48 col0\" >Polynomial Degree</td>\n",
       "                        <td id=\"T_c3b49_row48_col1\" class=\"data row48 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
       "                        <td id=\"T_c3b49_row49_col0\" class=\"data row49 col0\" >Trignometry Features</td>\n",
       "                        <td id=\"T_c3b49_row49_col1\" class=\"data row49 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
       "                        <td id=\"T_c3b49_row50_col0\" class=\"data row50 col0\" >Polynomial Threshold</td>\n",
       "                        <td id=\"T_c3b49_row50_col1\" class=\"data row50 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
       "                        <td id=\"T_c3b49_row51_col0\" class=\"data row51 col0\" >Group Features</td>\n",
       "                        <td id=\"T_c3b49_row51_col1\" class=\"data row51 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
       "                        <td id=\"T_c3b49_row52_col0\" class=\"data row52 col0\" >Feature Selection</td>\n",
       "                        <td id=\"T_c3b49_row52_col1\" class=\"data row52 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
       "                        <td id=\"T_c3b49_row53_col0\" class=\"data row53 col0\" >Feature Selection Method</td>\n",
       "                        <td id=\"T_c3b49_row53_col1\" class=\"data row53 col1\" >classic</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row54\" class=\"row_heading level0 row54\" >54</th>\n",
       "                        <td id=\"T_c3b49_row54_col0\" class=\"data row54 col0\" >Features Selection Threshold</td>\n",
       "                        <td id=\"T_c3b49_row54_col1\" class=\"data row54 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row55\" class=\"row_heading level0 row55\" >55</th>\n",
       "                        <td id=\"T_c3b49_row55_col0\" class=\"data row55 col0\" >Feature Interaction</td>\n",
       "                        <td id=\"T_c3b49_row55_col1\" class=\"data row55 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row56\" class=\"row_heading level0 row56\" >56</th>\n",
       "                        <td id=\"T_c3b49_row56_col0\" class=\"data row56 col0\" >Feature Ratio</td>\n",
       "                        <td id=\"T_c3b49_row56_col1\" class=\"data row56 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row57\" class=\"row_heading level0 row57\" >57</th>\n",
       "                        <td id=\"T_c3b49_row57_col0\" class=\"data row57 col0\" >Interaction Threshold</td>\n",
       "                        <td id=\"T_c3b49_row57_col1\" class=\"data row57 col1\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row58\" class=\"row_heading level0 row58\" >58</th>\n",
       "                        <td id=\"T_c3b49_row58_col0\" class=\"data row58 col0\" >Fix Imbalance</td>\n",
       "                        <td id=\"T_c3b49_row58_col1\" class=\"data row58 col1\" >False</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c3b49_level0_row59\" class=\"row_heading level0 row59\" >59</th>\n",
       "                        <td id=\"T_c3b49_row59_col0\" class=\"data row59 col0\" >Fix Imbalance Method</td>\n",
       "                        <td id=\"T_c3b49_row59_col1\" class=\"data row59 col1\" >SMOTE</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x246aad88370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categorical_features = [i for i in df.columns if \"_\" in i]\n",
    "target = \"class\"\n",
    "experiment = setup(df, target=target, categorical_features = categorical_features) # will ask for confirmation, hit enter if all good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af1df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5c130456bf4be696c4a9f256ae2760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Processing: ', max=74)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>13:17:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Fitting 10 Folds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      \n",
       "                                                                      \n",
       "Initiated  . . . . . . . . . . . . . . . . . .                13:17:31\n",
       "Status     . . . . . . . . . . . . . . . . . .        Fitting 10 Folds\n",
       "Estimator  . . . . . . . . . . . . . . . . . .  K Neighbors Classifier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_f83de_ th {\n",
       "          text-align: left;\n",
       "    }#T_f83de_row0_col0,#T_f83de_row0_col1,#T_f83de_row0_col2,#T_f83de_row0_col3,#T_f83de_row0_col4,#T_f83de_row0_col5,#T_f83de_row0_col6,#T_f83de_row0_col7,#T_f83de_row0_col8{\n",
       "            text-align:  left;\n",
       "        }</style><table id=\"T_f83de_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Model</th>        <th class=\"col_heading level0 col1\" >Accuracy</th>        <th class=\"col_heading level0 col2\" >AUC</th>        <th class=\"col_heading level0 col3\" >Recall</th>        <th class=\"col_heading level0 col4\" >Prec.</th>        <th class=\"col_heading level0 col5\" >F1</th>        <th class=\"col_heading level0 col6\" >Kappa</th>        <th class=\"col_heading level0 col7\" >MCC</th>        <th class=\"col_heading level0 col8\" >TT (Sec)</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_f83de_level0_row0\" class=\"row_heading level0 row0\" >lr</th>\n",
       "                        <td id=\"T_f83de_row0_col0\" class=\"data row0 col0\" >Logistic Regression</td>\n",
       "                        <td id=\"T_f83de_row0_col1\" class=\"data row0 col1\" >0.3000</td>\n",
       "                        <td id=\"T_f83de_row0_col2\" class=\"data row0 col2\" >0.3000</td>\n",
       "                        <td id=\"T_f83de_row0_col3\" class=\"data row0 col3\" >0.3000</td>\n",
       "                        <td id=\"T_f83de_row0_col4\" class=\"data row0 col4\" >0.3000</td>\n",
       "                        <td id=\"T_f83de_row0_col5\" class=\"data row0 col5\" >0.3000</td>\n",
       "                        <td id=\"T_f83de_row0_col6\" class=\"data row0 col6\" >0.3000</td>\n",
       "                        <td id=\"T_f83de_row0_col7\" class=\"data row0 col7\" >0.3000</td>\n",
       "                        <td id=\"T_f83de_row0_col8\" class=\"data row0 col8\" >142.2650</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x246844f4ee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to predict using multiple\n",
    "best_model = compare_models()\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48626b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d982f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61340a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f5dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36db6558",
   "metadata": {},
   "source": [
    "<a id='10.11'></a>\n",
    "### 10.11 AutoMl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45386636",
   "metadata": {},
   "source": [
    "#### 10.11.1 About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f09c1",
   "metadata": {},
   "source": [
    "> * Automates the process of hyperparamter tuning , model selection\n",
    "> * Avoids typing out all the models and their paramters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3401d0",
   "metadata": {},
   "source": [
    "#### 10.11.2 Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ccff37",
   "metadata": {},
   "source": [
    "> * Auto-keras\n",
    "> * Auto-sklearn\n",
    "> * Auto-H2O\n",
    "> * AutoWeka\n",
    "> * Auto-Pytorch\n",
    "> * TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc90ceb1",
   "metadata": {},
   "source": [
    "#### 10.11.3 Auto-keras\n",
    "[Classification SOURCE](https://automl.github.io/auto-sklearn/master/api.html#classification)\n",
    "[Regression SOURCE](https://automl.github.io/auto-sklearn/master/api.html#regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce905bf",
   "metadata": {},
   "source": [
    "#### 10.11.4 Auto-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b1d3a",
   "metadata": {},
   "source": [
    "CANT BE INSTLALLED ON WINDOWS\n",
    "https://automl.github.io/auto-sklearn/master/installation.html#windows-osx-compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a38524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cef1b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for regression\n",
    "import autosklearn.regression as ar\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split as ttsplit\n",
    "from sklearn.metrics import r2_score as r2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41519e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_csv('./data/Allstate/train.csv')\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efddc63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lable encoding for categorical columns\n",
    "label_encoder =LabelEncoder()\n",
    "\n",
    "for i in [i for i in training_data.columns if \"cat\" in i]:\n",
    "    training_data[f'cat_{i.replace(\"cat\",\"\")}'] = label_encoder.fit_transform(training_data[i])\n",
    "training_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e506c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data[[i for i in training_data.columns if \"_\" in i or \"cont\" in i]]\n",
    "y = training_data[\"loss\"]\n",
    "xtrain, xtest, ytrain, ytest = ttsplit(X,y, shuffle=True, train_size=0.75, test_size=0.25, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f000c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_left_for_this_task=1200 # 5 mins run time at max\n",
    "per_run_time_limit = 50 # how long should 1 model train\n",
    "n_jobs = 2 # how many in parallel\n",
    "memory_limit = 4000 # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb995dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = ar.AutoSklearnRegressor(time_left_for_this_task= time_left_for_this_task, per_run_time_limit = per_run_time_limit, n_jobs = n_jobs, memory_limit=memory_limit)\n",
    "reg_model.fit(xtrain,ytrain)\n",
    "reg_model.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb686d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model.sprint_statistics().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show which models where executed\n",
    "reg_model.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffa062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the models and performace\n",
    "train_predicted = reg_model.predict(xtrain)\n",
    "print(\"Train R2 score:\", r2(ytrain, train_predicted))\n",
    "test_predicted = reg_model.predict(xtest)\n",
    "print(\"Test R2 score:\", r2(ytest, test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fba219",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_predicted, ytrain, label=\"Train samples\", c='#d95f02')\n",
    "plt.scatter(test_predicted, ytest, label=\"Test samples\", c='#7570b3')\n",
    "plt.xlabel(\"Predicted value\")\n",
    "plt.ylabel(\"True value\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d07f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for classification\n",
    "import autosklearn.classification as cl\n",
    "from sklearn.metrics import accuracy_score as acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_csv('./data/Otto Product/train.csv')\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"class\"] = training_data[\"target\"].apply(lambda s: int(s.split(\"_\")[-1]))\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8fe91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data[[i for i in training_data.columns if \"_\" in i ]]\n",
    "y = training_data[\"class\"]\n",
    "xtrain, xtest, ytrain, ytest = ttsplit(X,y, shuffle=True, train_size=0.75, test_size=0.25, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_left_for_this_task=1200 # 5 mins run time at max\n",
    "per_run_time_limit = 50 # how long should 1 model train\n",
    "n_jobs = 2 # how many in parallel\n",
    "memory_limit = 4000 # \n",
    "cl_model = cl.AutoSklearnClassifier(time_left_for_this_task= time_left_for_this_task, per_run_time_limit = per_run_time_limit, n_jobs = n_jobs, memory_limit=memory_limit)\n",
    "cl_model.fit(xtrain,ytrain)\n",
    "cl_model.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_model.sprint_statistics().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b5ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_model.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the models and performace\n",
    "test_predicted = cl_model.predict(xtest)\n",
    "print(\"Accuracy score:\", acc(ytest, test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ae23b",
   "metadata": {},
   "source": [
    "<a id='chapter_11'></a>\n",
    "## Chapter 11 : Maintaining Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38c3b9",
   "metadata": {},
   "source": [
    "<a id='#11.1'></a>\n",
    "### **11.1 MLFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd2fe0e",
   "metadata": {},
   "source": [
    "#### 11.1.1 Working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e39263",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "011c890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow as mf\n",
    "import mlflow.sklearn as mfsk\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2393c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start ui of mlflow from cmd prompt\n",
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfefba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mf.start_run():\n",
    "    # to log parameters\n",
    "    mf.log_param(\"key\", value)\n",
    "    # to store metric\n",
    "    mf.log_metric(\"mse\",model.mse())\n",
    "    # to log anything\n",
    "    mf.log_artifact(\"some plot\",model.plot(test_df))\n",
    "    # to store model\n",
    "    mf.tensorflow.load_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edaee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a65b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b73571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273866c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987ce9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "807ed0ef",
   "metadata": {},
   "source": [
    "#### 11.1.2 Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e3d95",
   "metadata": {},
   "source": [
    "**TRACKING**\n",
    "> * Records code, configurations, resultd, logs\n",
    "\n",
    "**PROJECTS**\n",
    "> * Packages projects to reproduce anywhere\n",
    "\n",
    "**MODELS**\n",
    "> * Model deployment\n",
    "\n",
    "**REGISTRY**\n",
    "> * ui ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b1d6b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6a17038",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d505fc46",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cfcc53e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8669e15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ffe5ef2",
   "metadata": {},
   "source": [
    "<a id='#11.3'></a>\n",
    "### **11.3 Sagemaker**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58017b",
   "metadata": {},
   "source": [
    "[SOURCE](https://www.youtube.com/watch?v=ezrJHmVU0jc) : train and deplot\n",
    "\n",
    "[SOURCE](https://www.youtube.com/watch?v=J9T0X9Jxl_w) : Drift in production\n",
    "\n",
    "[SOURCE](https://www.youtube.com/watch?v=CK_xC4T1blk) : General overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e4d8f",
   "metadata": {},
   "source": [
    "#### 11.3.1 Working\n",
    "[SOURCE](https://youtu.be/CK_xC4T1blk?list=PL69hGgpJyHTBTTDVLU4QPjjocA9VUS0O4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81849ea",
   "metadata": {},
   "source": [
    "> * ecosystem of many tools required for end to end ml project\n",
    "\n",
    "> * UI : \n",
    "    * AWS console\n",
    "    * sagemaker notebooks (online ipynb)\n",
    "    * sagemaker studio : notebook + plugins, multiple servers, efs for storage \n",
    "    \n",
    "> * Machine interface:\n",
    "    * API : for all configurations\n",
    "    * SDK : boto3 in python using apis internally\n",
    "    * Sagemaker SDK : built in functions for varius tasks in a pipeline like estimator etc\n",
    "    * ec2 instances for scaling\n",
    "    * distributed systems for parallel processing\n",
    "    \n",
    "> * Infrastructures\n",
    "    * Containers :  to deploy and how things actually work behind the ui, has many libraries in them\n",
    "    * Built in algorithms : optimized for AWS\n",
    "    * Orchestration : run containers and deploy models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23456da7",
   "metadata": {},
   "source": [
    "**Post model deployment**\n",
    "\n",
    "> * Cloudwatch : While monitoring the model performance if there are drifts, can be used to alert\n",
    "> * Endpoint :  helps with predicting and using model further\n",
    "> * Baseline data perparation, to compare and identify drifts\n",
    "> * Both accuracy drift and data drift can be dealt with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d88a2",
   "metadata": {},
   "source": [
    "#### 11.3.2 Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2164956",
   "metadata": {},
   "source": [
    "> * Data Wrangler :\n",
    "> * Feature store : store, update, retirve and share features for model \n",
    "> * Clarify : detect bias and understand predictions\n",
    "\n",
    "> * Managed traning : distributed traning management\n",
    "> * Automatic model tuning : hyperparameter optimizaiton\n",
    "> * Distributed traning : \n",
    "> * Managed spot traning : to reduce cost\n",
    "\n",
    "> * Mutil model end point\n",
    "> * Model monitoring\n",
    "> * Edge manager \n",
    "> * Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d98e61",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc09dfee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e596cd4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "958f7406",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b3f89e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "896bd6da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "148cd7bd",
   "metadata": {},
   "source": [
    "<a id='#chapter_12'></a>\n",
    "## Chapter 12 : Git stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a856b21",
   "metadata": {},
   "source": [
    "<a id='#chapter_13'></a>\n",
    "## Chapter 13 : Cloud Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efcef6e",
   "metadata": {},
   "source": [
    "<a id='#chapter_14'></a>\n",
    "## Chapter 14 : Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de948a6",
   "metadata": {},
   "source": [
    "<a id='#chapter_15'></a>\n",
    "## Chapter 15 : Data Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ca2c0",
   "metadata": {},
   "source": [
    "<a id='#chapter_16'></a>\n",
    "## Chapter 16 : Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e96b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349cfa65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324a000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bdaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c974b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e404a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581c275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425019ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96734134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9f138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efe105",
   "metadata": {},
   "outputs": [],
   "source": [
    "** **\n",
    "> *\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
