{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43752a9f",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "### **[Preface](#preface)**\n",
    "## [Chapter 1](#chapter_1): Maths\n",
    "### **1.1 Stats**\n",
    "    [] 1.1.1 Mean, Median and Mode\n",
    "    [] 1.1.2 Correlation\n",
    "### **1.2 Probability**\n",
    "    [] 1.2.1 Conditional Probability\n",
    "    [] 1.2.2 P-value\n",
    "    [] 1.2.3 Likelihood\n",
    "### **1.3 Distributions**\n",
    "    [] 1.3.1 Normal\n",
    "    [] 1.3.2  \n",
    "    [] 1.3.3 Tests  \n",
    "### **1.4 Linear Algebra (Matrix)**\n",
    "    [] 1.4.1 Matrix Multiplication\n",
    "    [] 1.4.2 Inverse\n",
    "### [**1.5 Diffrentiation**](#1.5)\n",
    "    [X] 1.5.1 Gradient Descent\n",
    "### [**1.6 Useful Functions**](#1.6)\n",
    "    [X] 1.6.1 Activation Functions\n",
    "    [\\] 1.6.2 Normal Functions\n",
    "### **1.7 Stationarity**\n",
    "\n",
    "## [Chapter 2](#chapter_2) : Exploring data\n",
    "### **2.1 **\n",
    "### **2.2 Missing Value**\n",
    "### **2.3 Plots**\n",
    "### **2.4 Relations**\n",
    "### **2.5 Preprocessing**\n",
    "    [] 2.5.1 Language data\n",
    "    [] 2.5.2 One-Hot Encoding\n",
    "\n",
    "\n",
    "## [Chapter 3](#chapter_3) : Regression\n",
    "### [**3.1 Why?**](#3.1)\n",
    "### [**3.2 Linear Regression**](#3.2)\n",
    "    [X] 3.2.1 Assumptions\n",
    "    [X] 3.2.2 Working\n",
    "    [] 3.2.3 Pros and Cons\n",
    "### [**3.3 Polynomial Regression**](#3.3)\n",
    "    [] 3.3.1 Working\n",
    "### [**3.4 Regularization**](#3.4)\n",
    "    [X] 3.4.1 Lasso\n",
    "    [X] 3.4.2 Ridge\n",
    "    [X] 3.4.3 Elastic\n",
    "### [**3.5 Validation**](#3.5)\n",
    "### [**3.6 Metirics**](#3.6)\n",
    "    [X]] 3.6.1 R2\n",
    "    [X] 3.6.2 Adjusted R2\n",
    "\n",
    "## [Chapter 4](#chapter_4) : Classification\n",
    "### [**4.1 Why?**](#4.1)\n",
    "### [**4.2 Logistic Regression**](#4.2)\n",
    "    [\\] 4.2.1 Working\n",
    "    [] 4.2.2 Pros and cons\n",
    "### **4.3 Decision Tree**\n",
    "    [] 4.3.1 Classification\n",
    "    [] 4.3.2 Regression\n",
    "    [] 4.3.3 Pros and Cons\n",
    "### **4.4 Random Forest**\n",
    "    [] 4.4.1 Working\n",
    "    [] 4.4.2 Types\n",
    "    [] 4.4.3 Pros and Cons\n",
    "### **4.5 Support Vector Machine**\n",
    "    [] 4.5.1 Working\n",
    "    [] 4.5.2 Pros and cons\n",
    "### **4.6 K-nearest Neighbour**\n",
    "    [] 4.6.1 Working\n",
    "    [] 4.6.2 Pros and cons\n",
    "### **4.7 Naive Bayes**\n",
    "    [] 4.7.1 Working\n",
    "    [] 4.7.2 Pros and cons\n",
    "### **4.8 Validation**\n",
    "    [] 4.8.1 K-Fold\n",
    "### **4.9 Imbalanced Data**\n",
    "### **4.10 Metrics**\n",
    "    [] 4.10.1 Confusion Matrix\n",
    "\n",
    "## [Chapter 5](#chapter_5) : Unsupervised\n",
    "### **5.1 K-Means**\n",
    "    [] 5.1.1 Working\n",
    "    [] 5.1.2 Pros and cons\n",
    "### **5.2 K-Medoid**\n",
    "    [] 5.2.1 Working\n",
    "    [] 5.2.2 Pros and cons\n",
    "### **5.3 Hierarchical**\n",
    "    [] 5.3.1 Working\n",
    "    [] 5.3.2 Pros and cons\n",
    "### **5.4 Assosiative**\n",
    "    [] 5.4.1 Working\n",
    "    [] 5.4.2 Pros and cons\n",
    "### **5.5 TSNE**\n",
    "    [] 5.5.1 Working\n",
    "    [] 5.5.2 Pros and cons\n",
    "### **5.6 LDA**\n",
    "    [] 5.6.1 Working\n",
    "    [] 5.6.2 Pros and cons\n",
    "### **5.7 PCA**\n",
    "    [] 5.7.1 Working\n",
    "    [] 5.7.2 Pros and cons\n",
    "\n",
    "## [Chapter 6](#chapter_6) : Time Series\n",
    "### **6.1 ARIMA**\n",
    "    [] 6.1.1 AR\n",
    "    [] 6.1.2 MA\n",
    "    [] 6.1.3 ARIMAX\n",
    "    [] 6.1.4 SARIMA\n",
    "### **6.2 VAR**\n",
    "### **6.3 **\n",
    "### **6.4 Validation**\n",
    "### **6.5 Metrics**\n",
    "\n",
    "## [Chapter 7](#chapter_7) : Neural Networks\n",
    "### **7.1 Motivation**\n",
    "### **7.2 CNN**\n",
    "    [] 7.2.1 Working\n",
    "    [] 7.2.2 Pros and cons\n",
    "### **7.3 RNN**\n",
    "    [] 7.3.1 Working\n",
    "    [] 7.3.2 Pros and cons\n",
    "### **7.4 LSTM**\n",
    "    [] 7.4.1 Working\n",
    "    [] 7.4.2 Pros and cons\n",
    "### **7.5 Auto Encoder**\n",
    "    [] 7.5.1 Working\n",
    "    [] 7.5.2 Pros and cons\n",
    "### **7.6 Generative Adversarial Network**\n",
    "    [] 7.6.1 Working\n",
    "    [] 7.6.2 Pros and cons\n",
    "### **7.7 Transformers**\n",
    "    [] 7.7.1 Working\n",
    "    [] 7.7.2 Pros and cons\n",
    "\n",
    "## [Chapter 8](#chapter_8) : Language Processing\n",
    "### **8.1 TF-IDF**\n",
    "    [] 8.1.1 Working\n",
    "    [] 8.1.2 Pros and cons\n",
    "### **8.2 Word Embedding**\n",
    "    [] 8.2.1 Working\n",
    "    [] 8.2.2 Pros and cons\n",
    "### **8.3 Summarization**\n",
    "    [] 8.3.1 Working\n",
    "    [] 8.3.2 Pros and cons\n",
    "### **8.4 Sentiment Analysis**\n",
    "    [] 8.4.1 Working\n",
    "    [] 8.4.2 Pros and cons\n",
    "### **8.5 BERT**\n",
    "    [] 8.5.1 Working\n",
    "    [] 8.5.2 Pros and cons\n",
    "\n",
    "## [Chapter 9](#chapter_9) : Recommender System\n",
    "### **9.1 **\n",
    "\n",
    "## [Chapter 10](#chapter_10) : Python\n",
    "### **10.1 Pandas**\n",
    "    [] 10.1.1 \n",
    "    [] 10.1.2 \n",
    "    [] 10.1.3 \n",
    "    [] 10.1.4 \n",
    "    [] 10.1.5 \n",
    "    [] 10.1.6 \n",
    "    [] 10.1.7 \n",
    "    [] 10.1.8 \n",
    "    [] 10.1.9 \n",
    "### **10.2 Numpy**\n",
    "    [] 10.2.1 \n",
    "    [] 10.2.2 \n",
    "    [] 10.2.3 \n",
    "    [] 10.2.4 \n",
    "    [] 10.2.5 \n",
    "    [] 10.2.6 \n",
    "    [] 10.2.7 \n",
    "    [] 10.2.8 \n",
    "    [] 10.2.9 \n",
    "### **10.3 Scikit-learn**\n",
    "    [] 10.3.1 \n",
    "    [] 10.3.2 \n",
    "    [] 10.3.3 \n",
    "    [] 10.3.4 \n",
    "    [] 10.3.5 \n",
    "    [] 10.3.6 \n",
    "    [] 10.3.7 \n",
    "    [] 10.3.8 \n",
    "    [] 10.3.9 \n",
    "### **10.4 statsmodel**\n",
    "### **10.5 TensorFlow**\n",
    "    [] 10.5.1 \n",
    "    [] 10.5.2 \n",
    "    [] 10.5.3 \n",
    "    [] 10.5.4 \n",
    "    [] 10.5.5 \n",
    "    [] 10.5.6 \n",
    "    [] 10.5.7 \n",
    "    [] 10.5.8 \n",
    "    [] 10.5.9 \n",
    "### **10.6 Pytorch**\n",
    "### **10.7 NLTK**\n",
    "### **10.8 Scpacy**\n",
    "### **10.9 Gensim**\n",
    "### **10.10 Pycaret**\n",
    "### **10.11 AutoML**\n",
    "### **10.12 EDA**\n",
    "    [] 10.12.1 Sweetviz\n",
    "    [] 10.12.2 D-tale\n",
    "    [] 10.12.3 DataPrep\n",
    "    [] 10.12.4 Mito\n",
    "\n",
    "## [Chapter 11](#chapter_11) : Maintaining Models\n",
    "### **11.1 MLFlow**\n",
    "    [] 11.1.1 Working\n",
    "    [] 11.1.2 Parts\n",
    "    [] 11.1.3 Pros and cons\n",
    "### **11.2 KubeFlow**\n",
    "    [] 11.2.1 Working\n",
    "    [] 11.2.2 Parts\n",
    "    [] 11.2.3 Pros and cons\n",
    "### **11.3 SageMaker**\n",
    "    [] 11.3.1 Working\n",
    "    [] 11.3.2 Parts\n",
    "    [] 11.3.3 Pros and cons\n",
    "### **11.4 Metaflow**\n",
    "    [] 11.4.1 Working\n",
    "    [] 11.4.2 Parts\n",
    "    [] 11.4.3 Pros and cons\n",
    "### **11.5 TFx**\n",
    "    [] 11.5.1 Working\n",
    "    [] 11.5.2 Parts\n",
    "    [] 11.5.3 Pros and cons\n",
    "### **11.6 MLCOmet**\n",
    "    [] 11.6.1 Working\n",
    "    [] 11.6.2 Parts\n",
    "    [] 11.6.3 Pros and cons\n",
    "### **11.7 CookieCutter**\n",
    "    [] 11.7.1 Working\n",
    "    [] 11.7.2 Parts\n",
    "    [] 11.7.3 Pros and cons\n",
    "### **11.8 Kendro**\n",
    "    [] 11.8.1 Working\n",
    "    [] 11.8.2 Parts\n",
    "    [] 11.8.3 Pros and cons\n",
    "### **11.9 Kale**\n",
    "    [] 11.9.1 Working\n",
    "    [] 11.9.2 Parts\n",
    "    [] 11.9.3 Pros and cons\n",
    "\n",
    "## [Chapter 12](#chapter_12) : Git stuff\n",
    "### **12.1 Commands**\n",
    "### **12.2**\n",
    "### **12.5 DVC**\n",
    "\n",
    "## [Chapter 13](#chapter_13) : Cloud Stuff\n",
    "### **13.1 AWS**\n",
    "### **13.2 GCP**\n",
    "### **13.3 Azure**\n",
    "### **13.4**\n",
    "### **13.5**\n",
    "\n",
    "## [Chapter 14](#chapter_14) : Containers\n",
    "### **14.1 Docker**\n",
    "### **14.2 Kubernetes**\n",
    "### **14.3**\n",
    "### **14.4**\n",
    "### **14.5**\n",
    "\n",
    "## [Chapter 15](#chapter_15) : Data Storage\n",
    "### **15.1 Spark**\n",
    "    [] 15.1.1 Working\n",
    "    [] 15.1.2 Parts\n",
    "    [] 15.1.3 Pros and cons\n",
    "### **15.2 Hadoop**\n",
    "    [] 15.2.1 Working\n",
    "    [] 15.2.2 Parts\n",
    "    [] 15.2.3 Pros and cons\n",
    "### **15.3 SQL**\n",
    "### **15.4 NoSQL**\n",
    "\n",
    "## [Chapter 16](#chapter_16) : Misc\n",
    "### **16.1 API**\n",
    "### **16.2 Flask**\n",
    "### **16.3 Django**\n",
    "### **16.4 Streamlit**\n",
    "\n",
    " ---\n",
    "### Resources\n",
    "### Videos\n",
    "### Books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2fd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a111bf3",
   "metadata": {},
   "source": [
    "<a id='preface'></a>\n",
    "## Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f2b9e",
   "metadata": {},
   "source": [
    "A refresher for all thats related to contemporary (2021) data science. \n",
    "\n",
    "#### **DISCLAIMER** : This is all that I know and understand. If you find anything incorrect or error feel free to correct me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569b2ec",
   "metadata": {},
   "source": [
    "<a id='chapter_1'></a>\n",
    "## Chapter 1: Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d4c01",
   "metadata": {},
   "source": [
    "### 1.1 Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110b5bf",
   "metadata": {},
   "source": [
    "#### 1.1.1 Mean, Median, Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33830f44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9329544c",
   "metadata": {},
   "source": [
    "#### 1.1.2 Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a537e7",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://www.youtube.com/watch?v=9dr8rJ9fE7o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a255f80",
   "metadata": {},
   "source": [
    "**Pearson Correlation**\n",
    "> * Since covariance doesn provide the magnitude on how two variables act againste ach other, correlation is used not just for the direction (+/-) but also the magnitude\n",
    "> * Range is [-1,1] : -1 being negative slope, and + otherwise, 0 means no apparent correlation\n",
    "> * Pearson Correlation = Covariance/(std. deviation of variable 1 * std. deviation of variable 2)\n",
    "> * Pearson correlation will not tell cause and effect relation\n",
    "> * Correlation has no unit while covariance does and does not have a range\n",
    "> * Correlation can be visible in a scatter chart beween two linearly related variables\n",
    "> * As a general rule if the abs(correlation)>= 2/sqrt(# of points) then relation ship exists\n",
    "> * CON: falls prey ot Outliers \n",
    "\n",
    "**Spearman Correlation**\n",
    "> * To prevent outlier affecting the correlation, it sorts the data and then ranks both the variables, if there are ties it uses mean\n",
    "> * These ranks are used to find correlation between the variables\n",
    "> * PRO : Handles both continuous and ordinal\n",
    "> * PRO : Outliers are ranked so looses its affect to mislead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10135af",
   "metadata": {},
   "source": [
    "### 1.2 Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b52fe",
   "metadata": {},
   "source": [
    "#### 1.2.1 Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd1db1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a5cfa3",
   "metadata": {},
   "source": [
    "####  1.2.2 P-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f32070",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b32f1a",
   "metadata": {},
   "source": [
    "####  1.2.3 Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b7e40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f841c62c",
   "metadata": {},
   "source": [
    "<a id='1.5'></a>\n",
    "### 1.5 Diffrentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb1cd4",
   "metadata": {},
   "source": [
    "####  1.5.1 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370cb07",
   "metadata": {},
   "source": [
    "> * Usually used to find the lowest point in the graph of a function created by mapping loss function against weights\n",
    "> * But different starting points may lead to different minima which might not be the lowest of all (global minima) as the descent is said to move in the direction where the slope reduces\n",
    "> * To find the gradient/slope at a point it needs to be partially differentiated w.r.t. all the weights\n",
    "> * Then after finding which way is down the hill, the point is moved controlled by hyperparameter learning rate/step size\n",
    "> * Bigger steps might not converge and might move around skipping the mininma\n",
    "> * Smaller steps will take more time to reach the minima\n",
    "> * Modification to these steps and the way they move are in the form of adam, rmsprop etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b40d6",
   "metadata": {},
   "source": [
    "<a id='1.6'></a>\n",
    "### 1.6 Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d4db4",
   "metadata": {},
   "source": [
    "####  1.6.1 Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae86cd",
   "metadata": {},
   "source": [
    "> * Used mostyl in Neural networks within neurons as a function which takes in input with weights and bias and applys non-linear function on it to classify a non-linear data\n",
    "> * output of a neuron = F(wx+b)\n",
    "\n",
    "**SIGMOID FUNCTION**\n",
    "![](https://ai-master.gitbooks.io/logistic-regression/content/assets/sigmoid_function.png)\n",
    "> * f(x) = 1/(1+e^-(x))\n",
    "> * Looks like elonged S with base on negative X and top parallel to positive X\n",
    "> * value ranges from 0-1\n",
    "> * Differentiation of sigmoid function ranges from 0-0.25 (for backpropagation)\n",
    "> * CONS : Affected by vanishing gradient problem\n",
    "> * CONS : Since the output of the function is not 0 centered, its time consuming to converge\n",
    "\n",
    "**TANH FUNCTION**\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/8/87/Hyperbolic_Tangent.svg)\n",
    "> * f(x) = (e^x - e^-x)/(e^x + e^-x)\n",
    "> * Looks like elonged S with base parallel to negative X and top parallel to positive X\n",
    "> * Values : [-1, 1]\n",
    "> * differentiated values : [0, 1]\n",
    "> * CONS : Affected by vanishing gradient problem\n",
    "\n",
    "**RELU FUNCTION**\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/ReLU_and_GELU.svg/1024px-ReLU_and_GELU.svg.png)\n",
    "rectified linear unit\n",
    "> * f(x) = max(0,x)\n",
    "> * Looks like ramp with 0 in negative\n",
    "> * Values : [0,x]\n",
    "> * differentiated values : [0,1]\n",
    "> * Cant be differentiated at 0 so sub gradient is taken\n",
    "> * Faster as it linear function unlike exponents in tanh and sigmoid\n",
    "> * CONS : Since negative is cast to 0, in case of negative weight, new weight and old weight will not change : dying relu/dead activation function\n",
    "\n",
    "**Leaky RELU FUNCTION**\n",
    "![](https://www.researchgate.net/profile/Aditi-Shenoy-3/publication/334389306/figure/fig8/AS:779352161677313@1562823443351/Illustration-of-output-of-ELU-vs-ReLU-vs-Leaky-ReLU-function-with-varying-input-values.ppm)\n",
    "> * f(x) = max(0.01x,x)\n",
    "> * special case of PRelu (parametric relu where 0.01 can vary)\n",
    "> * Looks like ramp with 0.01x in negative\n",
    "> * Values : [0.01x,x]\n",
    "> * differentiated values : [0.01x,1]\n",
    "> * The small value can be hyperparameter\n",
    "> * Solves dying relu issue\n",
    "> * CONS : Vanishing gradient can happen if multiple neagative weights can lead to very very small change\n",
    "\n",
    "**ELU FUNCTION** \n",
    "![]()\n",
    "exponential linear units\n",
    "> * f(x) = x if x is +ve\n",
    "> * f(x) = a(e^x -1) if x is 0/-ve\n",
    "> * Looks like ramp with small expotential to 0 for negative\n",
    "> * The small value for negative/0 can be hyperparameter\n",
    "> * Solves dying relu issue as diffentiable at 0\n",
    "> * CONS : A bit more complicated as exponential is involved\n",
    "\n",
    "**SWISH RELU FUNCTION** \n",
    "![]()\n",
    "Self gated\n",
    "> * f(x) = x * sigmoid(x)\n",
    "> * Graph is similar to elu but with left extreme starting from near 0 \n",
    "> * The small value for negative/0 can be hyperparameter\n",
    "> * Used when layers are 40 or more \n",
    "> * CONS : Computationally expensive\n",
    "\n",
    "**SOFTPLUS FUNCTION**\n",
    "![]()\n",
    "> * f(x) = ln(1+e^x)\n",
    "> * Graph is similar to relu but lifetd and smooth with value for 0\n",
    "> * CONS : Computationally expensive\n",
    "\n",
    "**SOFTMAX FUNCTION**\n",
    "![]()\n",
    "used in the final layer of the classification network\n",
    "> * Used when the output is of category with multiple classes\n",
    "> * Since the output will be a number for each class, exponent of that is divided by the sum of exponent of all\n",
    "> * To use the maximum probability as the class category output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac3c1d",
   "metadata": {},
   "source": [
    "####  1.6.2 Normal Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35947663",
   "metadata": {},
   "source": [
    "**LOG FUNCTION**\n",
    "![Image of Log function](http://wiki.engageeducation.org.au/wp-content/uploads/2015/10/funclog1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4609525",
   "metadata": {},
   "source": [
    "**ODDS FUNCTION**\n",
    "> * probability of something occuring/ probability of not occuring\n",
    "> * Log(Odds function) is used in logistic regression = regression line equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db6ae2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faf98d71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10053cfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8849c430",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b7b51ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42878677",
   "metadata": {},
   "source": [
    "<a id='chapter_2'></a>\n",
    "## Chapter 2 : Exploring data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107d0b2d",
   "metadata": {},
   "source": [
    "<a id='chapter_3'></a>\n",
    "## Chapter 3 : Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b298a57",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d84aa",
   "metadata": {},
   "source": [
    "* To get an equation of a line/plane/multi-dimensional plane with all the features learning their weights (coefficients) and a bias term to predict the target variable\n",
    "* Simple to explain as weights tell how important the feature is while predicting the target\n",
    "* Explains a unit change in a feature and its affect on target while keeping all the other weights and bias same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ecfd1",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "### 3.2 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b9ebb",
   "metadata": {},
   "source": [
    "* When the relation between the features and the target is linear i.e. while plotting each feature against the target if there exists a straight line that can explain most of the points or points fall close to the straight line\n",
    "> Example: Target = Bias + w1\\*Feature1 + w2\\*Feature2 +  w3\\*Feature3 + ... +  wN\\*FeatureN + error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a537768",
   "metadata": {},
   "source": [
    "#### 3.2.1 Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126814bf",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://www.youtube.com/playlist?list=PLTNMv857s9WUI1Nz4SssXDKAELESXz-bi)\n",
    "* Is these assumptions are not meet, the values of wieghts and bias derived will not be reliable on unknown inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee36b1",
   "metadata": {},
   "source": [
    "1. There is a linear relation between the features and the target : plot scatter plot and check for each feature against target\n",
    "\n",
    "> **What does it mean?**\n",
    "> * The function/equation in this case, should be of specific type with weights multiplied with features or transforme version of features and summed up\n",
    "\n",
    "\n",
    "> **What if its not?**\n",
    "> * The relation is not linear and the line/equation formed will not be the best fit line\n",
    "\n",
    "> **How to find out?**\n",
    "> * Residual Plot (when the line drawn from the relation between feature and target is parallel along the x-axis)\n",
    "> * In this plot, the points should show any pattern like a curve and must be random\n",
    "> * Likelihood ratio test : TODO\n",
    "\n",
    "> **How to fix this?**\n",
    "> * By adding/transforming the feature with functions like log or polynomials and making the equation such that the line fits well and there is no pattern in the residual plot (trail and error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659a8da",
   "metadata": {},
   "source": [
    "2. The correlation between features is as low as possible (No multicolleniarity) : \n",
    "\n",
    "> **What does it mean?**\n",
    "> * Correlation between the fetures is high\n",
    "\n",
    "> **What if its not?**\n",
    "> * Multiple effects on target by feature. \n",
    "> * While interpreting, the other features will not be constant as they are related\n",
    "> * Equation is unreliable completely\n",
    "\n",
    "> **How to find out?**\n",
    "> *  Correlation among features \n",
    "> * Variance inflation factor (VIF) : Finds how is the model's variance, if one of the related variables is removed. If the VIF of a feature is more, the info provided by that feature already exists in the other features\n",
    "\n",
    "> **How to fix this?**\n",
    "> * Remove a feature which has high correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57618777",
   "metadata": {},
   "source": [
    "3. The variance of the residuals should not vary as the target is increased in residual plot (No Heteroscedasticity) : \n",
    "\n",
    "> **What does it mean?**\n",
    "> * If a feature has a wide range of numbers then this can happen as the feature might not be a good predictor\n",
    "> * The variance of points when plotted X vs Y, is not constant\n",
    "\n",
    "> **What if its not?**\n",
    "> * Std. erros of the equation will not be reliable\n",
    "\n",
    "> **How to find out?**\n",
    "> * In residual plot of the particular feature, the poitns would be funnel shaped as the variance is not constant\n",
    "> * Goldfledt-Quant test : TODO\n",
    "> * Breusch-Pagan test : TODO\n",
    "\n",
    "> **How to fix this?**\n",
    "> * Log transformation on the feture might help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34faffef",
   "metadata": {},
   "source": [
    "4. Each error should be independent of each other (No Autocorrelation/Serial Corrleation)\n",
    "\n",
    "> **What does it mean?**\n",
    "> * Usually occurs in time series data.\n",
    "> * If there is any order/time relation to feature, rare\n",
    "> * Successive value of feature depends on its past values, might even show seasonality or trend\n",
    "\n",
    "> **What if its not?**\n",
    "> * Std errors cant be relied on ergo hypothesis testing on the feature's coefficient will be difficult\n",
    "\n",
    "> **How to find out?**\n",
    "> * Residual plot might show wave like patterns\n",
    "> * Durbin-Watson test\n",
    "> * Breusch-Godfrey test\n",
    "\n",
    "> **How to fix this?**\n",
    "> * Look for features that are affecting the target (Missing features)\n",
    "> * Target can be modified by finding weight difference like AR(1) or Cochrane-Orchutt method \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc5fba",
   "metadata": {},
   "source": [
    "5. The spread of the errors at a given point, should be normally distributed\n",
    "\n",
    "> **What does it mean?**\n",
    "> * At any given point in the residual plot, the errors occuring, must fit in nicely with a normal distributed bell curve\n",
    "\n",
    "\n",
    "> **What if its not?**\n",
    "> * Not that big of an issue when data points are more\n",
    "> * For small size of data, std error are not reliable\n",
    "\n",
    "> **How to find out?**\n",
    "> * Histogram of residuals : must fit a bell curve\n",
    "> * Q-Q Plot : Quantiles from feature vs quantiles of normally distributed data, must fit the line to be used\n",
    "> * Shapiro-Wilk test\n",
    "> * Komolgorov-Smirnov test\n",
    "> * Anderson-Darling test\n",
    "\n",
    "> **How to fix this?**\n",
    "> * Get more data points\n",
    "> * Transform the feature by log etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89dc7d",
   "metadata": {},
   "source": [
    "6. Exogeneity\n",
    "\n",
    "> **What does it mean?**\n",
    "> * When an equation doesn have all the required and missing feature affects other features and the target, there exists a ommited variable bias\n",
    ">* This would cause error term to indicate something is missing\n",
    "\n",
    "> **What if its not?**\n",
    "> * Causation cant be known completely\n",
    "\n",
    "> **How to find out?**\n",
    "> * Domain knowledge/logic : to find whats missing \n",
    "\n",
    "> **How to fix this?**\n",
    "> * Instrumental variables : TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01026a17",
   "metadata": {},
   "source": [
    "#### 3.2.2 Working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceedd5a4",
   "metadata": {},
   "source": [
    "*Terminology*\n",
    "1. **SSE** : Squared distance between actual and predicted value\n",
    "1. **SSR** : squared distance between predicted value and mean of feature (unexplained error)\n",
    "1. **SST** : Squared distance between target and mean of feature (SSR+SSE) (explained error)\n",
    "1. **R2** : 1-(SSR/SST)\n",
    "1. **Adjusted R2** : 1-((1-R2)(# of datapoints-1)/(# of data points-1-# of features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d074f94",
   "metadata": {},
   "source": [
    "1. Gradient Descent technique\n",
    "\n",
    "> * Like many other ML algorithms, this uses a cost function to get learn the values of weights and bias\n",
    "> * The slope on which the global minima to be found is created using error vs all possible combinations of the coefficients\n",
    "\n",
    "> * The cost function can vary as per requirement ex: MSE, MAE/MAD, MAPE, MASE, Hubier\n",
    "> *MSE* : Mean square error (predicted - actual)^2. (-) Outlier handelling is bad, as the error is sqaured\n",
    "> *MAE* : mean absolute error |predicted - actual|. (-) Computationally expensive, cant be differentiated but good with outliers\n",
    "> *Hubier* : If the error is small acts like MSE else MAE, (-) Needs extra hyperparameter tuning \n",
    "> *MAPE* : TODO\n",
    "> *MASE* : TODO\n",
    "\n",
    "> 1. A line is drawn, and these predicted values are compared with the actual values. Since difference between these values (error) can be nullified either a square (MSE) or an absolute (MAE) or combination of both (Hubier) is used to find the loss value\n",
    "> 2. The idea is to find a combination of weights (coefficients of features) that gives the least error \n",
    "> 3. This is done using finding global minima in the landscape where error is plotted against all the coefficients\n",
    "> 4. Gradient descent (using partial derivative w.r.t. each feature) finds the slope of a particular combination and moves one step in a direction, if the slope is negative then it keeps moving till it reaches a point where the slope becomes positive or error doesn seem to change in any direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3262098",
   "metadata": {},
   "source": [
    "2. Oridinary least square method (y = mx+c)\n",
    "\n",
    "> * To find slope m  = sum((each feature value - mean of the feature)\\*(each target - mean of target))/sum((each feature value - mean of the feature)^2)\n",
    "> * To find intercept c = mean of target - m*(mean of feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba037b2",
   "metadata": {},
   "source": [
    "3. Using Linear Algebra\n",
    "\n",
    "[**SOURCE**](https://www.youtube.com/watch?v=RDQNc5xYQYM)\n",
    "[**SOURCE**](https://www.youtube.com/watch?v=S8UsRS2YvsM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf0547",
   "metadata": {},
   "source": [
    "> * When all the data points needs to be expressed, it not enough to show y = b0 + w1x1 + w2x2+ ... + E\n",
    "> * It needs to be shown as matrix exquation\n",
    "> * Via which weights are found using : B = Inverse(Xt*X) * (Xt*Y), where X is a design matrix with a column of 1 (for intercept) and a column of Xs\n",
    "> * Xt*X gives 2x2 matrix  = [[N, mean(x)],[mean(x),mean(x^2]]\n",
    "> * Xt*Y gives a 2x1 amtrix = [[Sum(Y)],[Sum(XY)]]\n",
    "> * Multiplying them gives the learned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8603d",
   "metadata": {},
   "source": [
    "4. Maximum likelihood Estimator\n",
    "\n",
    "[**SOURCE**](https://www.youtube.com/watch?v=UdADuHJUX6Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be56ff",
   "metadata": {},
   "source": [
    "> * Assumption here is the error terms are noramlly identically independent distributed = N(0, sigma^2)\n",
    "> * Unlike in OLS, where distance is used to optimize, in MLE, all the points of feature are assumed to be from a normally > distributed curve with varying mean (that value of y)\n",
    "> * The MLE formula here will be maximized to get weights instead of reducing the loss function in OLS/GD\n",
    "> * When the features are Normally distributed, the cost function here is negative of loss function in OLS. So the weights are the same.\n",
    "> * Log is taken while increasing the likelihood, hence log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e0598",
   "metadata": {},
   "source": [
    "### 3.3 Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c5a80",
   "metadata": {},
   "source": [
    "NOTE : If an equation has x and x^2 as 2 features, though the are derivable, they are not linearly correlated so thier existence will  not lead to multicolliarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63089dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import corrcoef\n",
    "# import matplotlib.pyplot as plt\n",
    "# x = [1,2,3,4,5,6,7,8]\n",
    "# x2 = [i*i for i in x]\n",
    "# x3 = [i*i*i for i in x]\n",
    "# print(f\"Square {corrcoef(x,x2)[0][1]}\")\n",
    "# plt.plot(x,x2)\n",
    "# plt.show()\n",
    "# print(f\"Cube {corrcoef(x,x3)[0][1]}\")\n",
    "# plt.plot(x,x3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6eb976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ef13936",
   "metadata": {},
   "source": [
    "<a id='3.4'></a>\n",
    "### 3.4 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8804d1",
   "metadata": {},
   "source": [
    "* Adding a little bias to decrease variance by a lot\n",
    "* To avoid overfitting\n",
    "* To penalize model for having high test error\n",
    "* Variables must be standardized before applying regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde90e3",
   "metadata": {},
   "source": [
    "#### 3.4.1 Lasso (L1) Least Absolute Shrinkage and selection Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8608b",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://www.youtube.com/watch?v=NGf0voTMlcs)\n",
    "[**SOURCE**](https://www.youtube.com/watch?v=t_1ZSWGDkX4&list=PL2GWo47BFyUPWL5fBZSn6FFHRr1bSkX_J&index=16)\n",
    "\n",
    "> * Main difference here, w.r.t. Ridge is the sum of squared residual is added with Lambda * abs(slope) (Manhattan distance) instead of square of slope\n",
    "> * This causes line to have slope of extacly 0 for high values of lambda unlike Ridge which comes close\n",
    "> * Making a slope of feature 0 causes removal of that feature there by making it useful for feature reduction\n",
    "> * GD can be used to reduce the loss function\n",
    "> * While plotting Possible values for weights, a diamond shape is formed around the origin and the predicted weights countours using mean square error , if overlapped gives the values and can at times, meet at the edges and make predicted weight 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378d519",
   "metadata": {},
   "source": [
    "#### 3.4.2 Ridge (L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798615a6",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://www.youtube.com/watch?v=Q81RR3yKn30)\n",
    "> * Along with the loss/cost function, Lambda * slope^2 is added (Eucledian distance), where lambda is the hyperpararmeter (0-infinity)\n",
    "> * Slope square acts as a penalty if the line is steep\n",
    "> * This adds small bias and there by reducing variance of the equation\n",
    "> * Value of lambda is directly proportional to reducing affect of the feature on target\n",
    "> * Value of lambda is found by using random value and k-fold cross validation\n",
    "> * By addding the penalty, sensitivity of the line is decreased and unit change in feature doesn show more than required change in target\n",
    "> * In case of categorical feature, instead of slope, the distance between the means of categories is used\n",
    "> * In case of multiple features, lambda is multiplied with sum of square of slope of each numnerical feature and mean distance of categorical except the intercept value\n",
    "> * While plotting Possible values for weights, a circle is formed around the origin and the predicted weights countours using mean square error , if overlapped gives the values and can at times, meet at the edges and make values close to 0 but not exactly 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04ac24",
   "metadata": {},
   "source": [
    "#### 3.4.3 Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a10ef",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://youtu.be/iJE2fZcNPlA?list=PL2GWo47BFyUPWL5fBZSn6FFHRr1bSkX_J)\n",
    "> * Its Hubier loss of regularization. A combo of L1 and L2 above\n",
    "> * The penalty term is both the absolute and Square of slope making the equation : MSE + L1 + L2\n",
    "> * There are 2 lambdas involved in hyperparameter tuning\n",
    "> * This can be used as feature removal + reducing the magnitude of others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987d7c5",
   "metadata": {},
   "source": [
    "**DECIDING FACTOR**\n",
    "> * If all the feautres must be present go with Ridge\n",
    "> * If features can be removed then dont use Ridge\n",
    "> * If outliers are present use Lasso\n",
    "> * If gradient descent cant be used, go with Ridge\n",
    "> * For unique solution avoid Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b5a13",
   "metadata": {},
   "source": [
    "<a id='3.5'></a>\n",
    "### 3.5 Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88fde9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "889dfafc",
   "metadata": {},
   "source": [
    "<a id='3.6'></a>\n",
    "### 3.6 Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6070a8",
   "metadata": {},
   "source": [
    "#### 3.6.1 R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17845fd1",
   "metadata": {},
   "source": [
    "> * To measure how good the equation of a linear regression is by finding the proportion of variance in the target that can be explained by the features\n",
    "            R2 = 1-(SSR/SST)\n",
    "> * Bigger the R2 better the model but 1 would be overfitting\n",
    "> * Usually the range is between 0 to 1 unless the fit is actually worse than just fitting a horizontal line then R-square can be negative\n",
    "> * CON : Adding more features may increase the R2 but that is not the right way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1feb8",
   "metadata": {},
   "source": [
    "#### 3.6.2 Adjusted R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccab7d",
   "metadata": {},
   "source": [
    "> * Adding more feature will increase R2 so adjusted R2 is used to penalize the model for having unwanted features\n",
    "            Adjusted R2 = 1-((1-R2)(# of point -1)/(# of points - # of feautres -1))\n",
    "> * If R2 is 0, Adjusted R2 might even be neagtive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd27df2",
   "metadata": {},
   "source": [
    "<a id='chapter_4'></a>\n",
    "## Chapter 4 : Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20effa61",
   "metadata": {},
   "source": [
    "<a id='4.1'></a>\n",
    "### 4.1 Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bf910",
   "metadata": {},
   "source": [
    "> * When the target is categorical and not numerical/continuous\n",
    "> * Since linear rgerssion is sussptible to outliers, this can lead to unreliable model\n",
    "> * Linear regression can give value thats not applicable to category of target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308694e3",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>\n",
    "### 4.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e04f19",
   "metadata": {},
   "source": [
    "#### 4.2.1 Working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7e606",
   "metadata": {},
   "source": [
    "> * The value to be predicted is a probability of it being in a category\n",
    "> * Since probability is not linear, linear Regression cant be used\n",
    "> * A signmoid funciton is used in case of binary classification to get the probability\n",
    "> * Sigmoid function makes o/p 1 if value is more than 0.5 and 0 otherwise\n",
    "> * p(X) = sigmoid(w0+w1x1+...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ae459",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/875/1*6oBgYMy4wOls9zGC-frSQg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92eec7",
   "metadata": {},
   "source": [
    "> * COST FUNCTION : \n",
    "    if y = 1 ; -log(predicted probability) \n",
    "    if y = 0 ; -log(1-predicted probability) \n",
    "    ![](https://miro.medium.com/max/1192/1*wilGXrItaMAJmZNl6RJq9Q.png)\n",
    "> * If the output is actually 1 but predcited is 0, the cost fucntion penalizes heavily as -log(0) = infinity\n",
    "> * Same otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697373c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa2e39f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "964b2a7d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a3be0d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b668e032",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51bebdc0",
   "metadata": {},
   "source": [
    "<a id='chapter_5'></a>\n",
    "## Chapter 5 : Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e23e9c",
   "metadata": {},
   "source": [
    "<a id='chapter_6'></a>\n",
    "## Chapter 6 : Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f732f",
   "metadata": {},
   "source": [
    "<a id='chapter_7'></a>\n",
    "## Chapter 7 : Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dce687",
   "metadata": {},
   "source": [
    "<a id='chapter_8'></a>\n",
    "## Chapter 8 : Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71122c2",
   "metadata": {},
   "source": [
    "<a id='chapter_9'></a>\n",
    "## Chapter 9 : Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb539049",
   "metadata": {},
   "source": [
    "<a id='#chapter_10'></a>\n",
    "## Chapter 10 : Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ae23b",
   "metadata": {},
   "source": [
    "<a id='#chapter_11'></a>\n",
    "## Chapter 11 : Maintaining Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148cd7bd",
   "metadata": {},
   "source": [
    "<a id='#chapter_12'></a>\n",
    "## Chapter 12 : Git stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a856b21",
   "metadata": {},
   "source": [
    "<a id='#chapter_13'></a>\n",
    "## Chapter 13 : Cloud Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efcef6e",
   "metadata": {},
   "source": [
    "<a id='#chapter_14'></a>\n",
    "## Chapter 14 : Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de948a6",
   "metadata": {},
   "source": [
    "<a id='#chapter_15'></a>\n",
    "## Chapter 15 : Data Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ca2c0",
   "metadata": {},
   "source": [
    "<a id='#chapter_16'></a>\n",
    "## Chapter 16 : Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e96b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349cfa65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324a000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bdaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c974b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e404a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581c275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425019ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96734134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9f138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efe105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
