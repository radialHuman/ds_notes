{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43752a9f",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "### **[Preface](#preface)**\n",
    "## [Chapter 1](#chapter_1): Maths\n",
    "### **1.1 Stats**\n",
    "    [] 1.1.1 Mean, Median and Mode\n",
    "    [] 1.1.2 Correlation\n",
    "### **1.2 Probability**\n",
    "    [] 1.2.1 Conditional Probability\n",
    "    [] 1.2.2 P-value\n",
    "    [] 1.2.3 Likelihood\n",
    "### **1.3 Distributions**\n",
    "    [] 1.3.1 Normal\n",
    "    [] 1.3.2  \n",
    "    [] 1.3.3 Tests  \n",
    "### **1.4 Linear Algebra (Matrix)**\n",
    "    [] 1.4.1 Matrix Multiplication\n",
    "    [] 1.4.2 Inverse\n",
    "### [**1.5 Diffrentiation**](#1.5)\n",
    "    [X] 1.5.1 Gradient Descent\n",
    "### [**1.6 Useful Functions**](#1.6)\n",
    "    [X] 1.6.1 Activation Functions\n",
    "    [\\] 1.6.2 Normal Functions\n",
    "### **1.7 Stationarity**\n",
    "\n",
    "## [Chapter 2](#chapter_2) : Exploring data\n",
    "### **2.1 **\n",
    "### **2.2 Missing Value**\n",
    "### **2.3 Plots**\n",
    "### **2.4 Relations**\n",
    "### **2.5 Preprocessing**\n",
    "    [] 2.5.1 Language data\n",
    "    [] 2.5.2 One-Hot Encoding\n",
    "\n",
    "\n",
    "## [Chapter 3](#chapter_3) : Regression\n",
    "### [**3.1 Why?**](#3.1)\n",
    "### [**3.2 Linear Regression**](#3.2)\n",
    "    [X] 3.2.1 Assumptions\n",
    "    [X] 3.2.2 Working\n",
    "    [] 3.2.3 Pros and Cons\n",
    "### [**3.3 Polynomial Regression**](#3.3)\n",
    "    [] 3.3.1 Working\n",
    "### [**3.4 Regularization**](#3.4)\n",
    "    [X] 3.4.1 Lasso\n",
    "    [X] 3.4.2 Ridge\n",
    "    [X] 3.4.3 Elastic\n",
    "### [**3.5 Validation**](#3.5)\n",
    "### [**3.6 Metirics**](#3.6)\n",
    "    [X]] 3.6.1 R2\n",
    "    [X] 3.6.2 Adjusted R2\n",
    "\n",
    "## [Chapter 4](#chapter_4) : Classification\n",
    "### [**4.1 Why?**](#4.1)\n",
    "### [**4.2 Logistic Regression**](#4.2)\n",
    "    [\\] 4.2.1 Working\n",
    "    [] 4.2.2 Pros and cons\n",
    "### [**4.3 Decision Tree**](#4.3)\n",
    "    [] 4.3.1 What?\n",
    "    [] 4.3.2 Classification\n",
    "    [] 4.3.3 Regression\n",
    "    [] 4.3.4 Pros and Cons\n",
    "### **4.4 Random Forest**\n",
    "    [] 4.4.1 What?\n",
    "    [] 4.4.2 Working\n",
    "    [] 4.4.3 Types\n",
    "    [] 4.4.4 Pros and Cons\n",
    "### **4.5 Support Vector Machine**\n",
    "    [] 4.5.1 What?\n",
    "    [] 4.5.2 Working\n",
    "    [] 4.5.3 Pros and cons\n",
    "### **4.6 K-nearest Neighbour**\n",
    "    [] 4.6.1 What?\n",
    "    [] 4.6.2 Working\n",
    "    [] 4.6.3 Pros and cons\n",
    "### **4.7 Naive Bayes**\n",
    "    [] 4.7.1 What?\n",
    "    [] 4.7.4 Working\n",
    "    [] 4.7.5 Pros and cons\n",
    "### **4.8 Validation**\n",
    "    [] 4.8.1 K-Fold\n",
    "### **4.9 Imbalanced Data**\n",
    "### **4.10 Metrics**\n",
    "    [] 4.10.1 Confusion Matrix\n",
    "\n",
    "## [Chapter 5](#chapter_5) : Unsupervised\n",
    "### **5.1 K-Means**\n",
    "    [] 5.1.1 Working\n",
    "    [] 5.1.2 Pros and cons\n",
    "### **5.2 K-Medoid**\n",
    "    [] 5.2.1 Working\n",
    "    [] 5.2.2 Pros and cons\n",
    "### **5.3 Hierarchical**\n",
    "    [] 5.3.1 Working\n",
    "    [] 5.3.2 Pros and cons\n",
    "### **5.4 Assosiative**\n",
    "    [] 5.4.1 Working\n",
    "    [] 5.4.2 Pros and cons\n",
    "### **5.5 TSNE**\n",
    "    [] 5.5.1 Working\n",
    "    [] 5.5.2 Pros and cons\n",
    "### **5.6 LDA**\n",
    "    [] 5.6.1 Working\n",
    "    [] 5.6.2 Pros and cons\n",
    "### **5.7 PCA**\n",
    "    [X] 5.7.1 Working\n",
    "    [X] 5.7.2 Pros and cons\n",
    "\n",
    "## [Chapter 6](#chapter_6) : Time Series\n",
    "### **6.1 Filters**\n",
    "### **6.2 ARIMA**\n",
    "    [] 6.2.1 AR\n",
    "    [] 6.2.2 MA\n",
    "    [] 6.2.3 ARIMAX\n",
    "    [] 6.2.4 SARIMA\n",
    "### **6.3 VAR**\n",
    "### **6.4 Validation**\n",
    "### **6.5 Metrics**\n",
    "\n",
    "## [Chapter 7](#chapter_7) : Neural Networks\n",
    "### **7.1 Motivation**\n",
    "### **7.2 CNN**\n",
    "    [] 7.2.1 Working\n",
    "    [] 7.2.2 Pros and cons\n",
    "### **7.3 RNN**\n",
    "    [] 7.3.1 Working\n",
    "    [] 7.3.2 Pros and cons\n",
    "### **7.4 LSTM**\n",
    "    [] 7.4.1 Working\n",
    "    [] 7.4.2 Pros and cons\n",
    "### **7.5 Auto Encoder**\n",
    "    [] 7.5.1 Working\n",
    "    [] 7.5.2 Pros and cons\n",
    "### **7.6 Generative Adversarial Network**\n",
    "    [] 7.6.1 Working\n",
    "    [] 7.6.2 Pros and cons\n",
    "### **7.7 Transformers**\n",
    "    [] 7.7.1 Working\n",
    "    [] 7.7.2 Pros and cons\n",
    "\n",
    "## [Chapter 8](#chapter_8) : Language Processing\n",
    "### **8.1 TF-IDF**\n",
    "    [] 8.1.1 Working\n",
    "    [] 8.1.2 Pros and cons\n",
    "### **8.2 Word Embedding**\n",
    "    [] 8.2.1 Working\n",
    "    [] 8.2.2 Pros and cons\n",
    "### **8.3 Summarization**\n",
    "    [] 8.3.1 Working\n",
    "    [] 8.3.2 Pros and cons\n",
    "### **8.4 Sentiment Analysis**\n",
    "    [] 8.4.1 Working\n",
    "    [] 8.4.2 Pros and cons\n",
    "### **8.5 BERT**\n",
    "    [] 8.5.1 Working\n",
    "    [] 8.5.2 Pros and cons\n",
    "\n",
    "## [Chapter 9](#chapter_9) : Recommender System\n",
    "### **9.1 **\n",
    "\n",
    "## [Chapter 10](#chapter_10) : Python\n",
    "### **10.1 Pandas**\n",
    "    [] 10.1.1 \n",
    "    [] 10.1.2 \n",
    "    [] 10.1.3 \n",
    "    [] 10.1.4 \n",
    "    [] 10.1.5 \n",
    "    [] 10.1.6 \n",
    "    [] 10.1.7 \n",
    "    [] 10.1.8 \n",
    "    [] 10.1.9 \n",
    "### **10.2 Numpy**\n",
    "    [] 10.2.1 \n",
    "    [] 10.2.2 \n",
    "    [] 10.2.3 \n",
    "    [] 10.2.4 \n",
    "    [] 10.2.5 \n",
    "    [] 10.2.6 \n",
    "    [] 10.2.7 \n",
    "    [] 10.2.8 \n",
    "    [] 10.2.9 \n",
    "### **10.3 Scikit-learn**\n",
    "    [] 10.3.1 \n",
    "    [] 10.3.2 \n",
    "    [] 10.3.3 \n",
    "    [] 10.3.4 \n",
    "    [] 10.3.5 \n",
    "    [] 10.3.6 \n",
    "    [] 10.3.7 \n",
    "    [] 10.3.8 \n",
    "    [] 10.3.9 \n",
    "### **10.4 statsmodel**\n",
    "### **10.5 TensorFlow**\n",
    "    [] 10.5.1 \n",
    "    [] 10.5.2 \n",
    "    [] 10.5.3 \n",
    "    [] 10.5.4 \n",
    "    [] 10.5.5 \n",
    "    [] 10.5.6 \n",
    "    [] 10.5.7 \n",
    "    [] 10.5.8 \n",
    "    [] 10.5.9 \n",
    "### **10.6 Pytorch**\n",
    "### **10.7 NLTK**\n",
    "### **10.8 Scpacy**\n",
    "### **10.9 Gensim**\n",
    "### **10.10 Pycaret**\n",
    "### **10.11 AutoML**\n",
    "### **10.12 EDA**\n",
    "    [] 10.12.1 Sweetviz\n",
    "    [] 10.12.2 D-tale\n",
    "    [] 10.12.3 DataPrep\n",
    "    [] 10.12.4 Mito\n",
    "### **10.13 Numba**\n",
    "### **10.14 Skippa**\n",
    "\n",
    "\n",
    "## [Chapter 11](#chapter_11) : Maintaining Models\n",
    "### **11.1 MLFlow**\n",
    "    [] 11.1.1 Working\n",
    "    [] 11.1.2 Parts\n",
    "    [] 11.1.3 Pros and cons\n",
    "### **11.2 KubeFlow**\n",
    "    [] 11.2.1 Working\n",
    "    [] 11.2.2 Parts\n",
    "    [] 11.2.3 Pros and cons\n",
    "### **11.3 SageMaker**\n",
    "    [] 11.3.1 Working\n",
    "    [] 11.3.2 Parts\n",
    "    [] 11.3.3 Pros and cons\n",
    "### **11.4 Metaflow**\n",
    "    [] 11.4.1 Working\n",
    "    [] 11.4.2 Parts\n",
    "    [] 11.4.3 Pros and cons\n",
    "### **11.5 TFx**\n",
    "    [] 11.5.1 Working\n",
    "    [] 11.5.2 Parts\n",
    "    [] 11.5.3 Pros and cons\n",
    "### **11.6 MLComet**\n",
    "    [] 11.6.1 Working\n",
    "    [] 11.6.2 Parts\n",
    "    [] 11.6.3 Pros and cons\n",
    "### **11.7 CookieCutter**\n",
    "    [] 11.7.1 Working\n",
    "    [] 11.7.2 Parts\n",
    "    [] 11.7.3 Pros and cons\n",
    "### **11.8 Kendro**\n",
    "    [] 11.8.1 Working\n",
    "    [] 11.8.2 Parts\n",
    "    [] 11.8.3 Pros and cons\n",
    "### **11.9 Kale**\n",
    "    [] 11.9.1 Working\n",
    "    [] 11.9.2 Parts\n",
    "    [] 11.9.3 Pros and cons\n",
    "\n",
    "## [Chapter 12](#chapter_12) : Git stuff\n",
    "### **12.1 Commands**\n",
    "### **12.2**\n",
    "### **12.5 DVC**\n",
    "\n",
    "## [Chapter 13](#chapter_13) : Cloud Stuff\n",
    "### **13.1 AWS**\n",
    "### **13.2 GCP**\n",
    "### **13.3 Azure**\n",
    "### **13.4**\n",
    "### **13.5**\n",
    "\n",
    "## [Chapter 14](#chapter_14) : Containers\n",
    "### **14.1 Docker**\n",
    "### **14.2 Kubernetes**\n",
    "### **14.3**\n",
    "### **14.4**\n",
    "### **14.5**\n",
    "\n",
    "## [Chapter 15](#chapter_15) : Data Storage\n",
    "### **15.1 Spark**\n",
    "    [] 15.1.1 Working\n",
    "    [] 15.1.2 Parts\n",
    "    [] 15.1.3 Pros and cons\n",
    "### **15.2 Hadoop**\n",
    "    [] 15.2.1 Working\n",
    "    [] 15.2.2 Parts\n",
    "    [] 15.2.3 Pros and cons\n",
    "### **15.3 SQL**\n",
    "### **15.4 NoSQL**\n",
    "\n",
    "## [Chapter 16](#chapter_16) : Misc\n",
    "### **16.1 API**\n",
    "### **16.2 Flask**\n",
    "### **16.3 Django**\n",
    "### **16.4 Streamlit**\n",
    "\n",
    " ---\n",
    "### Resources\n",
    "### Videos\n",
    "### Books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2fd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a111bf3",
   "metadata": {},
   "source": [
    "<a id='preface'></a>\n",
    "## Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f2b9e",
   "metadata": {},
   "source": [
    "A refresher for all thats related to contemporary (2021) data science. \n",
    "\n",
    "#### **DISCLAIMER** : This is all that I know and understand. If you find anything incorrect or error feel free to correct me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569b2ec",
   "metadata": {},
   "source": [
    "<a id='chapter_1'></a>\n",
    "## Chapter 1: Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d4c01",
   "metadata": {},
   "source": [
    "### 1.1 Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110b5bf",
   "metadata": {},
   "source": [
    "#### 1.1.1 Mean, Median, Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33830f44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9329544c",
   "metadata": {},
   "source": [
    "#### 1.1.2 Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a537e7",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://www.youtube.com/watch?v=9dr8rJ9fE7o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a255f80",
   "metadata": {},
   "source": [
    "**Pearson Correlation**\n",
    "> * Since covariance doesn provide the magnitude on how two variables act againste ach other, correlation is used not just for the direction (+/-) but also the magnitude\n",
    "> * Range is [-1,1] : -1 being negative slope, and + otherwise, 0 means no apparent correlation\n",
    "> * Pearson Correlation = Covariance/(std. deviation of variable 1 * std. deviation of variable 2)\n",
    "> * Pearson correlation will not tell cause and effect relation\n",
    "> * Correlation has no unit while covariance does and does not have a range\n",
    "> * Correlation can be visible in a scatter chart beween two linearly related variables\n",
    "> * As a general rule if the abs(correlation)>= 2/sqrt(# of points) then relation ship exists\n",
    "> * CON: falls prey ot Outliers \n",
    "\n",
    "**Spearman Correlation**\n",
    "> * To prevent outlier affecting the correlation, it sorts the data and then ranks both the variables, if there are ties it uses mean\n",
    "> * These ranks are used to find correlation between the variables\n",
    "> * PRO : Handles both continuous and ordinal\n",
    "> * PRO : Outliers are ranked so looses its affect to mislead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10135af",
   "metadata": {},
   "source": [
    "### 1.2 Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b52fe",
   "metadata": {},
   "source": [
    "#### 1.2.1 Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd1db1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a5cfa3",
   "metadata": {},
   "source": [
    "####  1.2.2 P-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f32070",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b32f1a",
   "metadata": {},
   "source": [
    "####  1.2.3 Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b7e40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f841c62c",
   "metadata": {},
   "source": [
    "<a id='1.5'></a>\n",
    "### 1.5 Diffrentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb1cd4",
   "metadata": {},
   "source": [
    "####  1.5.1 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370cb07",
   "metadata": {},
   "source": [
    "> * After predicting/classifying, using a model based on input, to check how far off is it from the least amount of error possible, and moving towards that\n",
    "> * Moving by changing model paramters, checking if the minima is reached by checking the rate of change from previous set of parameters\n",
    "> * Usually used to find the lowest point in the graph of a function created by mapping loss function against weights\n",
    "> * But different starting points may lead to different minima which might not be the lowest of all (global minima) as the descent is said to move in the direction where the slope reduces\n",
    "> * To find the gradient/slope at a point it needs to be partially differentiated w.r.t. all the weights\n",
    "> * Then after finding which way is down the hill, the point is moved controlled by hyperparameter learning rate/step size\n",
    "> * Bigger steps might not converge and might move around skipping the mininma\n",
    "> * Smaller steps will take more time to reach the minima\n",
    "> * Modification to these steps and the way they move are in the form of adam, rmsprop etc\n",
    "\n",
    "[SOURCE](https://towardsdatascience.com/stochastic-batch-and-mini-batch-gradient-descent-demystified-8b28978f7f5)\n",
    "\n",
    "**BATCH**\n",
    "> * Takes the entire data set in one go (epoch) and builds the model\n",
    "> * PROS:\n",
    "    * Simple, efficient, stable error gradient\n",
    "    * stable convergence : calulates weights at every epoch and takes the average\n",
    "> * CONS : \n",
    "    * Slow as updates occur once after the epoch\n",
    "    * Might get stuck in local minima or saddle as its smooth\n",
    "\n",
    "\n",
    "**STOCHATIC**\n",
    "> * Updates model one data input/point at a time (picked at random)\n",
    "> * Since the data to trian on is small for updating the model, the errors might be all over the space\n",
    "> * Due to multiple updates, its computationally expensive\n",
    "> * PROS: \n",
    "    * Faster, SGD can be used for larger datasets.\n",
    "    * It converges faster when the dataset is large as it causes updates to the parameters more frequently.\n",
    "> * CONS :\n",
    "    * Computationally expensive as it updates at every data point\n",
    "    * Due to jumpy nature, might not stay in global minima\n",
    "\n",
    "\n",
    "**MINI-BATCH**\n",
    "> * Best of both the worlds: Takes random samples of data (50-256 data points/rows)\n",
    "> * PROS : Efficient , stable, faster\n",
    "> * CONS : new hyperparamter for the bathc size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98c697",
   "metadata": {},
   "source": [
    "####  1.5.2 Optimizers based on gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36dda8",
   "metadata": {},
   "source": [
    "[SOURCE](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)\n",
    "\n",
    "[SOURCE](https://ruder.io/optimizing-gradient-descent/)\n",
    "\n",
    "**MOMENTUM**\n",
    "> * To accelarate SGD in right direction and reduce its mosiynes by adding a fraction of the past update usually 0.9\n",
    "> * PROS:\n",
    "    * Makes SGD faster\n",
    "> * CONS:\n",
    "    *\n",
    "\n",
    "**ADAGRAD**\n",
    "> *\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *\n",
    "    \n",
    "**ADADELTA**\n",
    "> * Extension of adagrad\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *\n",
    "\n",
    "**RMSPROP**\n",
    "> * Variation of RMSPROP\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *\n",
    "    \n",
    "**ADAM**\n",
    "> *\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *\n",
    "\n",
    "**ADAMAX**\n",
    "> *\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *\n",
    "    \n",
    "**NADAM**\n",
    "> * Extention of ADAM\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *\n",
    "\n",
    "**AMSGRAD**\n",
    "> * \n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b40d6",
   "metadata": {},
   "source": [
    "<a id='1.6'></a>\n",
    "### 1.6 Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d4db4",
   "metadata": {},
   "source": [
    "####  1.6.1 Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae86cd",
   "metadata": {},
   "source": [
    "> * Used mostyl in Neural networks within neurons as a function which takes in input with weights and bias and applys non-linear function on it to classify a non-linear data\n",
    "> * output of a neuron = F(wx+b)\n",
    "\n",
    "**SIGMOID FUNCTION**\n",
    "![](https://ai-master.gitbooks.io/logistic-regression/content/assets/sigmoid_function.png)\n",
    "> * f(x) = 1/(1+e^-(x))\n",
    "> * Looks like elonged S with base on negative X and top parallel to positive X\n",
    "> * value ranges from 0-1\n",
    "> * Differentiation of sigmoid function ranges from 0-0.25 (for backpropagation)\n",
    "> * CONS : Affected by vanishing gradient problem\n",
    "> * CONS : Since the output of the function is not 0 centered, its time consuming to converge\n",
    "\n",
    "**TANH FUNCTION**\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/8/87/Hyperbolic_Tangent.svg)\n",
    "> * f(x) = (e^x - e^-x)/(e^x + e^-x)\n",
    "> * Looks like elonged S with base parallel to negative X and top parallel to positive X\n",
    "> * Values : [-1, 1]\n",
    "> * differentiated values : [0, 1]\n",
    "> * CONS : Affected by vanishing gradient problem\n",
    "\n",
    "**RELU FUNCTION**\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/ReLU_and_GELU.svg/1024px-ReLU_and_GELU.svg.png)\n",
    "rectified linear unit\n",
    "> * f(x) = max(0,x)\n",
    "> * Looks like ramp with 0 in negative\n",
    "> * Values : [0,x]\n",
    "> * differentiated values : [0,1]\n",
    "> * Cant be differentiated at 0 so sub gradient is taken\n",
    "> * Faster as it linear function unlike exponents in tanh and sigmoid\n",
    "> * CONS : Since negative is cast to 0, in case of negative weight, new weight and old weight will not change : dying relu/dead activation function\n",
    "\n",
    "**Leaky RELU FUNCTION**\n",
    "![](https://www.researchgate.net/profile/Aditi-Shenoy-3/publication/334389306/figure/fig8/AS:779352161677313@1562823443351/Illustration-of-output-of-ELU-vs-ReLU-vs-Leaky-ReLU-function-with-varying-input-values.ppm)\n",
    "> * f(x) = max(0.01x,x)\n",
    "> * special case of PRelu (parametric relu where 0.01 can vary)\n",
    "> * Looks like ramp with 0.01x in negative\n",
    "> * Values : [0.01x,x]\n",
    "> * differentiated values : [0.01x,1]\n",
    "> * The small value can be hyperparameter\n",
    "> * Solves dying relu issue\n",
    "> * CONS : Vanishing gradient can happen if multiple neagative weights can lead to very very small change\n",
    "\n",
    "**ELU FUNCTION** \n",
    "![]()\n",
    "exponential linear units\n",
    "> * f(x) = x if x is +ve\n",
    "> * f(x) = a(e^x -1) if x is 0/-ve\n",
    "> * Looks like ramp with small expotential to 0 for negative\n",
    "> * The small value for negative/0 can be hyperparameter\n",
    "> * Solves dying relu issue as diffentiable at 0\n",
    "> * CONS : A bit more complicated as exponential is involved\n",
    "\n",
    "**SWISH RELU FUNCTION** \n",
    "![]()\n",
    "Self gated\n",
    "> * f(x) = x * sigmoid(x)\n",
    "> * Graph is similar to elu but with left extreme starting from near 0 \n",
    "> * The small value for negative/0 can be hyperparameter\n",
    "> * Used when layers are 40 or more \n",
    "> * CONS : Computationally expensive\n",
    "\n",
    "**SOFTPLUS FUNCTION**\n",
    "![]()\n",
    "> * f(x) = ln(1+e^x)\n",
    "> * Graph is similar to relu but lifetd and smooth with value for 0\n",
    "> * CONS : Computationally expensive\n",
    "\n",
    "**SOFTMAX FUNCTION**\n",
    "![]()\n",
    "used in the final layer of the classification network\n",
    "> * Used when the output is of category with multiple classes\n",
    "> * Since the output will be a number for each class, exponent of that is divided by the sum of exponent of all\n",
    "> * To use the maximum probability as the class category output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac3c1d",
   "metadata": {},
   "source": [
    "####  1.6.2 Normal Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35947663",
   "metadata": {},
   "source": [
    "**LOG FUNCTION**\n",
    "![Image of Log function](http://wiki.engageeducation.org.au/wp-content/uploads/2015/10/funclog1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4609525",
   "metadata": {},
   "source": [
    "**ODDS FUNCTION**\n",
    "> * probability of something occuring/ probability of not occuring\n",
    "> * Log(Odds function) is used in logistic regression = regression line equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db6ae2c",
   "metadata": {},
   "source": [
    "**LOSS FUNCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf98d71",
   "metadata": {},
   "source": [
    "> * Cross-Entropy loss function (classification)\n",
    "![](https://miro.medium.com/proxy/1*ftQegucUqLc7RQ-UNrs-Wg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10053cfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8849c430",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b7b51ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42878677",
   "metadata": {},
   "source": [
    "<a id='chapter_2'></a>\n",
    "## Chapter 2 : Exploring data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107d0b2d",
   "metadata": {},
   "source": [
    "<a id='chapter_3'></a>\n",
    "## Chapter 3 : Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b298a57",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d84aa",
   "metadata": {},
   "source": [
    "* To get an equation of a line/plane/multi-dimensional plane with all the features learning their weights (coefficients) and a bias term to predict the target variable\n",
    "* Simple to explain as weights tell how important the feature is while predicting the target\n",
    "* Explains a unit change in a feature and its affect on target while keeping all the other weights and bias same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ecfd1",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "### 3.2 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b9ebb",
   "metadata": {},
   "source": [
    "* When the relation between the features and the target is linear i.e. while plotting each feature against the target if there exists a straight line that can explain most of the points or points fall close to the straight line\n",
    "> Example: Target = Bias + w1\\*Feature1 + w2\\*Feature2 +  w3\\*Feature3 + ... +  wN\\*FeatureN + error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a537768",
   "metadata": {},
   "source": [
    "#### 3.2.1 Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126814bf",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://www.youtube.com/playlist?list=PLTNMv857s9WUI1Nz4SssXDKAELESXz-bi)\n",
    "* Is these assumptions are not meet, the values of wieghts and bias derived will not be reliable on unknown inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee36b1",
   "metadata": {},
   "source": [
    "1. There is a linear relation between the features and the target : plot scatter plot and check for each feature against target\n",
    "\n",
    "> **What does it mean?**\n",
    "> * The function/equation in this case, should be of specific type with weights multiplied with features or transforme version of features and summed up\n",
    "\n",
    "\n",
    "> **What if its not?**\n",
    "> * The relation is not linear and the line/equation formed will not be the best fit line\n",
    "\n",
    "> **How to find out?**\n",
    "> * Residual Plot (when the line drawn from the relation between feature and target is parallel along the x-axis)\n",
    "> * In this plot, the points should show any pattern like a curve and must be random\n",
    "> * Likelihood ratio test : TODO\n",
    "\n",
    "> **How to fix this?**\n",
    "> * By adding/transforming the feature with functions like log or polynomials and making the equation such that the line fits well and there is no pattern in the residual plot (trail and error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659a8da",
   "metadata": {},
   "source": [
    "2. The correlation between features is as low as possible (No multicolleniarity) : \n",
    "\n",
    "> **What does it mean?**\n",
    "> * Correlation between the fetures is high\n",
    "\n",
    "> **What if its not?**\n",
    "> * Multiple effects on target by feature. \n",
    "> * While interpreting, the other features will not be constant as they are related\n",
    "> * Equation is unreliable completely\n",
    "\n",
    "> **How to find out?**\n",
    "> *  Correlation among features \n",
    "> * Variance inflation factor (VIF) : Finds how is the model's variance, if one of the related variables is removed. If the VIF of a feature is more, the info provided by that feature already exists in the other features\n",
    "\n",
    "> **How to fix this?**\n",
    "> * Remove a feature which has high correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57618777",
   "metadata": {},
   "source": [
    "3. The variance of the residuals should not vary as the target is increased in residual plot (No Heteroscedasticity) : \n",
    "\n",
    "> **What does it mean?**\n",
    "> * If a feature has a wide range of numbers then this can happen as the feature might not be a good predictor\n",
    "> * The variance of points when plotted X vs Y, is not constant\n",
    "\n",
    "> **What if its not?**\n",
    "> * Std. erros of the equation will not be reliable\n",
    "\n",
    "> **How to find out?**\n",
    "> * In residual plot of the particular feature, the poitns would be funnel shaped as the variance is not constant\n",
    "> * Goldfledt-Quant test : TODO\n",
    "> * Breusch-Pagan test : TODO\n",
    "\n",
    "> **How to fix this?**\n",
    "> * Log transformation on the feture might help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34faffef",
   "metadata": {},
   "source": [
    "4. Each error should be independent of each other (No Autocorrelation/Serial Corrleation)\n",
    "\n",
    "> **What does it mean?**\n",
    "> * Usually occurs in time series data.\n",
    "> * If there is any order/time relation to feature, rare\n",
    "> * Successive value of feature depends on its past values, might even show seasonality or trend\n",
    "\n",
    "> **What if its not?**\n",
    "> * Std errors cant be relied on ergo hypothesis testing on the feature's coefficient will be difficult\n",
    "\n",
    "> **How to find out?**\n",
    "> * Residual plot might show wave like patterns\n",
    "> * Durbin-Watson test\n",
    "> * Breusch-Godfrey test\n",
    "\n",
    "> **How to fix this?**\n",
    "> * Look for features that are affecting the target (Missing features)\n",
    "> * Target can be modified by finding weight difference like AR(1) or Cochrane-Orchutt method \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc5fba",
   "metadata": {},
   "source": [
    "5. The spread of the errors at a given point, should be normally distributed\n",
    "\n",
    "> **What does it mean?**\n",
    "> * At any given point in the residual plot, the errors occuring, must fit in nicely with a normal distributed bell curve\n",
    "\n",
    "\n",
    "> **What if its not?**\n",
    "> * Not that big of an issue when data points are more\n",
    "> * For small size of data, std error are not reliable\n",
    "\n",
    "> **How to find out?**\n",
    "> * Histogram of residuals : must fit a bell curve\n",
    "> * Q-Q Plot : Quantiles from feature vs quantiles of normally distributed data, must fit the line to be used\n",
    "> * Shapiro-Wilk test\n",
    "> * Komolgorov-Smirnov test\n",
    "> * Anderson-Darling test\n",
    "\n",
    "> **How to fix this?**\n",
    "> * Get more data points\n",
    "> * Transform the feature by log etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89dc7d",
   "metadata": {},
   "source": [
    "6. Exogeneity\n",
    "\n",
    "> **What does it mean?**\n",
    "> * When an equation doesn have all the required and missing feature affects other features and the target, there exists a ommited variable bias\n",
    ">* This would cause error term to indicate something is missing\n",
    "\n",
    "> **What if its not?**\n",
    "> * Causation cant be known completely\n",
    "\n",
    "> **How to find out?**\n",
    "> * Domain knowledge/logic : to find whats missing \n",
    "\n",
    "> **How to fix this?**\n",
    "> * Instrumental variables : TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01026a17",
   "metadata": {},
   "source": [
    "#### 3.2.2 Working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceedd5a4",
   "metadata": {},
   "source": [
    "*Terminology*\n",
    "1. **SSE** : Squared distance between actual and predicted value\n",
    "1. **SSR** : squared distance between predicted value and mean of feature (unexplained error)\n",
    "1. **SST** : Squared distance between target and mean of feature (SSR+SSE) (explained error)\n",
    "1. **R2** : 1-(SSR/SST)\n",
    "1. **Adjusted R2** : 1-((1-R2)(# of datapoints-1)/(# of data points-1-# of features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d074f94",
   "metadata": {},
   "source": [
    "1. Gradient Descent technique\n",
    "\n",
    "> * Like many other ML algorithms, this uses a cost function to get learn the values of weights and bias\n",
    "> * The slope on which the global minima to be found is created using error vs all possible combinations of the coefficients\n",
    "\n",
    "> * The cost function can vary as per requirement ex: MSE, MAE/MAD, MAPE, MASE, Hubier\n",
    "> *MSE* : Mean square error (predicted - actual)^2. (-) Outlier handelling is bad, as the error is sqaured\n",
    "> *MAE* : mean absolute error |predicted - actual|. (-) Computationally expensive, cant be differentiated but good with outliers\n",
    "> *Hubier* : If the error is small acts like MSE else MAE, (-) Needs extra hyperparameter tuning \n",
    "> *MAPE* : TODO\n",
    "> *MASE* : TODO\n",
    "\n",
    "> 1. A line is drawn, and these predicted values are compared with the actual values. Since difference between these values (error) can be nullified either a square (MSE) or an absolute (MAE) or combination of both (Hubier) is used to find the loss value\n",
    "> 2. The idea is to find a combination of weights (coefficients of features) that gives the least error \n",
    "> 3. This is done using finding global minima in the landscape where error is plotted against all the coefficients\n",
    "> 4. Gradient descent (using partial derivative w.r.t. each feature) finds the slope of a particular combination and moves one step in a direction, if the slope is negative then it keeps moving till it reaches a point where the slope becomes positive or error doesn seem to change in any direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3262098",
   "metadata": {},
   "source": [
    "2. Oridinary least square method (y = mx+c)\n",
    "\n",
    "> * To find slope m  = sum((each feature value - mean of the feature)\\*(each target - mean of target))/sum((each feature value - mean of the feature)^2)\n",
    "> * To find intercept c = mean of target - m*(mean of feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba037b2",
   "metadata": {},
   "source": [
    "3. Using Linear Algebra\n",
    "\n",
    "[**SOURCE**](https://www.youtube.com/watch?v=RDQNc5xYQYM)\n",
    "[**SOURCE**](https://www.youtube.com/watch?v=S8UsRS2YvsM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf0547",
   "metadata": {},
   "source": [
    "> * When all the data points needs to be expressed, it not enough to show y = b0 + w1x1 + w2x2+ ... + E\n",
    "> * It needs to be shown as matrix exquation\n",
    "> * Via which weights are found using : B = Inverse(Xt*X) * (Xt*Y), where X is a design matrix with a column of 1 (for intercept) and a column of Xs\n",
    "> * Xt*X gives 2x2 matrix  = [[N, mean(x)],[mean(x),mean(x^2]]\n",
    "> * Xt*Y gives a 2x1 amtrix = [[Sum(Y)],[Sum(XY)]]\n",
    "> * Multiplying them gives the learned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8603d",
   "metadata": {},
   "source": [
    "4. Maximum likelihood Estimator\n",
    "\n",
    "[**SOURCE**](https://www.youtube.com/watch?v=UdADuHJUX6Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be56ff",
   "metadata": {},
   "source": [
    "> * Assumption here is the error terms are noramlly identically independent distributed = N(0, sigma^2)\n",
    "> * Unlike in OLS, where distance is used to optimize, in MLE, all the points of feature are assumed to be from a normally > distributed curve with varying mean (that value of y)\n",
    "> * The MLE formula here will be maximized to get weights instead of reducing the loss function in OLS/GD\n",
    "> * When the features are Normally distributed, the cost function here is negative of loss function in OLS. So the weights are the same.\n",
    "> * Log is taken while increasing the likelihood, hence log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e0598",
   "metadata": {},
   "source": [
    "### 3.3 Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c5a80",
   "metadata": {},
   "source": [
    "NOTE : If an equation has x and x^2 as 2 features, though the are derivable, they are not linearly correlated so thier existence will  not lead to multicolliarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63089dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import corrcoef\n",
    "# import matplotlib.pyplot as plt\n",
    "# x = [1,2,3,4,5,6,7,8]\n",
    "# x2 = [i*i for i in x]\n",
    "# x3 = [i*i*i for i in x]\n",
    "# print(f\"Square {corrcoef(x,x2)[0][1]}\")\n",
    "# plt.plot(x,x2)\n",
    "# plt.show()\n",
    "# print(f\"Cube {corrcoef(x,x3)[0][1]}\")\n",
    "# plt.plot(x,x3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6eb976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ef13936",
   "metadata": {},
   "source": [
    "<a id='3.4'></a>\n",
    "### 3.4 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8804d1",
   "metadata": {},
   "source": [
    "* Adding a little bias to decrease variance by a lot\n",
    "* To avoid overfitting\n",
    "* To penalize model for having high test error\n",
    "* Variables must be standardized before applying regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde90e3",
   "metadata": {},
   "source": [
    "#### 3.4.1 Lasso (L1) Least Absolute Shrinkage and selection Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8608b",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://www.youtube.com/watch?v=NGf0voTMlcs)\n",
    "[**SOURCE**](https://www.youtube.com/watch?v=t_1ZSWGDkX4&list=PL2GWo47BFyUPWL5fBZSn6FFHRr1bSkX_J&index=16)\n",
    "\n",
    "> * Main difference here, w.r.t. Ridge is the sum of squared residual is added with Lambda * abs(slope) (Manhattan distance) instead of square of slope\n",
    "> * This causes line to have slope of extacly 0 for high values of lambda unlike Ridge which comes close\n",
    "> * Making a slope of feature 0 causes removal of that feature there by making it useful for feature reduction\n",
    "> * GD can be used to reduce the loss function\n",
    "> * While plotting Possible values for weights, a diamond shape is formed around the origin and the predicted weights countours using mean square error , if overlapped gives the values and can at times, meet at the edges and make predicted weight 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378d519",
   "metadata": {},
   "source": [
    "#### 3.4.2 Ridge (L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798615a6",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://www.youtube.com/watch?v=Q81RR3yKn30)\n",
    "> * Along with the loss/cost function, Lambda * slope^2 is added (Eucledian distance), where lambda is the hyperpararmeter (0-infinity)\n",
    "> * Slope square acts as a penalty if the line is steep\n",
    "> * This adds small bias and there by reducing variance of the equation\n",
    "> * Value of lambda is directly proportional to reducing affect of the feature on target\n",
    "> * Value of lambda is found by using random value and k-fold cross validation\n",
    "> * By addding the penalty, sensitivity of the line is decreased and unit change in feature doesn show more than required change in target\n",
    "> * In case of categorical feature, instead of slope, the distance between the means of categories is used\n",
    "> * In case of multiple features, lambda is multiplied with sum of square of slope of each numnerical feature and mean distance of categorical except the intercept value\n",
    "> * While plotting Possible values for weights, a circle is formed around the origin and the predicted weights countours using mean square error , if overlapped gives the values and can at times, meet at the edges and make values close to 0 but not exactly 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04ac24",
   "metadata": {},
   "source": [
    "#### 3.4.3 Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a10ef",
   "metadata": {},
   "source": [
    "[**SOURCE**](https://youtu.be/iJE2fZcNPlA?list=PL2GWo47BFyUPWL5fBZSn6FFHRr1bSkX_J)\n",
    "> * Its Hubier loss of regularization. A combo of L1 and L2 above\n",
    "> * The penalty term is both the absolute and Square of slope making the equation : MSE + L1 + L2\n",
    "> * There are 2 lambdas involved in hyperparameter tuning\n",
    "> * This can be used as feature removal + reducing the magnitude of others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987d7c5",
   "metadata": {},
   "source": [
    "**DECIDING FACTOR**\n",
    "> * If all the feautres must be present go with Ridge\n",
    "> * If features can be removed then dont use Ridge\n",
    "> * If outliers are present use Lasso\n",
    "> * If gradient descent cant be used, go with Ridge\n",
    "> * For unique solution avoid Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b5a13",
   "metadata": {},
   "source": [
    "<a id='3.5'></a>\n",
    "### 3.5 Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88fde9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "889dfafc",
   "metadata": {},
   "source": [
    "<a id='3.6'></a>\n",
    "### 3.6 Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6070a8",
   "metadata": {},
   "source": [
    "#### 3.6.1 R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17845fd1",
   "metadata": {},
   "source": [
    "> * To measure how good the equation of a linear regression is by finding the proportion of variance in the target that can be explained by the features\n",
    "            R2 = 1-(SSR/SST)\n",
    "> * Bigger the R2 better the model but 1 would be overfitting\n",
    "> * Usually the range is between 0 to 1 unless the fit is actually worse than just fitting a horizontal line then R-square can be negative\n",
    "> * CON : Adding more features may increase the R2 but that is not the right way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1feb8",
   "metadata": {},
   "source": [
    "#### 3.6.2 Adjusted R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccab7d",
   "metadata": {},
   "source": [
    "> * Adding more feature will increase R2 so adjusted R2 is used to penalize the model for having unwanted features\n",
    "            Adjusted R2 = 1-((1-R2)(# of point -1)/(# of points - # of feautres -1))\n",
    "> * If R2 is 0, Adjusted R2 might even be neagtive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd27df2",
   "metadata": {},
   "source": [
    "<a id='chapter_4'></a>\n",
    "## Chapter 4 : Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20effa61",
   "metadata": {},
   "source": [
    "<a id='4.1'></a>\n",
    "### 4.1 Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bf910",
   "metadata": {},
   "source": [
    "> * When the target is categorical and not numerical/continuous\n",
    "> * Since linear rgerssion is sussptible to outliers, this can lead to unreliable model\n",
    "> * Linear regression can give value thats not applicable to category of target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308694e3",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>\n",
    "### 4.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e04f19",
   "metadata": {},
   "source": [
    "#### 4.2.1 Working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7e606",
   "metadata": {},
   "source": [
    "> * The value to be predicted is a probability of it being in a category\n",
    "> * Since probability is not linear, linear Regression cant be used\n",
    "> * A signmoid funciton is used in case of binary classification to get the probability\n",
    "> * Sigmoid function makes o/p 1 if value is more than 0.5 and 0 otherwise\n",
    "> * p(X) = sigmoid(w0+w1x1+...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ae459",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/875/1*6oBgYMy4wOls9zGC-frSQg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92eec7",
   "metadata": {},
   "source": [
    "> * COST FUNCTION : \n",
    "    if y = 1 ; -log(predicted probability) \n",
    "    if y = 0 ; -log(1-predicted probability) \n",
    "    ![](https://miro.medium.com/max/1192/1*wilGXrItaMAJmZNl6RJq9Q.png)\n",
    "> * If the output is actually 1 but predcited is 0, the cost fucntion penalizes heavily as -log(0) = infinity\n",
    "> * Same otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697373c",
   "metadata": {},
   "source": [
    "<a id='4.3'></a>\n",
    "### 4.3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e4179f",
   "metadata": {},
   "source": [
    "> * Based on impurity functions which measures how pure a given set it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e39f4",
   "metadata": {},
   "source": [
    "#### 4.3.1 What?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964b2a7d",
   "metadata": {},
   "source": [
    "> * Entropy :  0 is pure, while 1 is most impure\n",
    "![](https://miro.medium.com/max/1010/1*-ZZAH1q_DHbVxobuXKY2ng.png)\n",
    "> * Gini : 0 means all belong to one class and is pure. 0.5 is the most impure class. Faster then entropy as its not log based\n",
    "![](https://miro.medium.com/max/664/1*otdoiyIwxJI-UV0ukkyutw.png)\n",
    "> * Information gain :\n",
    "> * Node : Usually splits into 2 nodes/leaves. Signifies a feature with condition for splitting\n",
    "> * Leaf : end of the line with label associated with it to classify\n",
    "\n",
    "CART uses Gini; ID3 and C4.5 use Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad926b1a",
   "metadata": {},
   "source": [
    "#### 4.3.2 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3be0d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2045a17",
   "metadata": {},
   "source": [
    "#### 4.3.3 Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668e032",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5513a698",
   "metadata": {},
   "source": [
    "#### 4.3.4 Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c643e",
   "metadata": {},
   "source": [
    "> * Usually overfits as it memorizes rules of traning data\n",
    "> * Low depth trees leads to high bias (solved by boosting)\n",
    "> * High depth leads to high variance (solved by bagging)\n",
    "> * More data increases time exponentially to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bebdc0",
   "metadata": {},
   "source": [
    "<a id='chapter_5'></a>\n",
    "## Chapter 5 : Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c32066a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2b097a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85be3f65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af1386f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c73c5607",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21ff8ab2",
   "metadata": {},
   "source": [
    "<a id='5.7'></a>\n",
    "### 5.7 : PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34f85e",
   "metadata": {},
   "source": [
    "#### 5.7.1 Working\n",
    "[SOURCE](https://www.youtube.com/watch?v=TJdH6rPA-TI)\n",
    "[SOURCE](https://medium.com/analytics-vidhya/understanding-principle-component-analysis-pca-step-by-step-e7a4bb4031d9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5f1ad",
   "metadata": {},
   "source": [
    "> * Though called a reduction technique, its a transforming technique. \n",
    "> * It transforms the data so that it looks more meaningful\n",
    "> * For this to work the data must be stabdardized i.e. mean 0 and sd 1\n",
    "> * Deciding between two features by plotting scatter chart and finding an aixs which helps to retain the information as much as possible while allowing to remove a feature \n",
    "> * The new direction of axis where the variance is the highest and the error along the new axis is the least\n",
    "> * The next Principle axis is at 90* with the previous line\n",
    "> * This is repeated for all the features and # of PC are == # of features\n",
    "> * But as the # of PC increases, since variance decreases, they become less useful\n",
    "> * This way the PC with most information can be retained and rest eliminated\n",
    "> * The PCs that add upto 99% of the variance accountability can be used to represent the data withotu losing much information\n",
    "> * The resulting projected data are linear combinations of the original data\n",
    "\n",
    "??? How/in what order and why are features selected to create PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf033a",
   "metadata": {},
   "source": [
    "> * 1: Standardize the dataset if not defined by same unit\n",
    "> * 2: Calculate the covariance matrix for the features in the dataset\n",
    "> * 3: Calculate the eigenvalues and eigenvectors for the covariance matrix\n",
    "> * 4: Sort eigenvalues and their corresponding eigenvectors\n",
    "> * 5: Pick k eigenvalues and form a matrix of eigenvectors\n",
    "> * 6: Transform the original matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee660689",
   "metadata": {},
   "source": [
    "#### 5.7.2 Pros and cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ae0ef",
   "metadata": {},
   "source": [
    "**PROS**\n",
    "> * Less features, fast computation\n",
    "\n",
    "**CONS**\n",
    "> * The PCs will not be easily interpretable\n",
    "> * Might reduce the accuracy if modelled\n",
    "> * Measurements from all of the original variables are used in the projection to the lower dimensional space\n",
    "> * Only linear relationships are considered, Doesnâ€™t work well for non linearly correlated data.\n",
    "> * Always uses orthogonal PC\n",
    "> * Sometime lo wvariance features can add value but gets removed, while something with outlier might be included\n",
    "> * PCA or SVD-based methods, as well as univariate screening methods (t-test, correlation, etc.), do not take into account the potential multivariate nature of the data structure (e.g., higher order interaction between variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e23e9c",
   "metadata": {},
   "source": [
    "<a id='chapter_6'></a>\n",
    "## Chapter 6 : Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8203215",
   "metadata": {},
   "source": [
    "> * The mean and the variance must be constant over the time period\n",
    "> * If the mean is not constant then there can be a trend\n",
    "> * If variance is not, then it is highly volatile\n",
    "> * There can also be repeated pattern, visually looking like a cycle called seasonality\n",
    "> * If all the values are taken form the TS and plotted like in a bell curve, and then the probability is found for a certain point it would still not help because the occurance of a data point might be dependent on its previous n values i.e. autocorrelation, correlation with self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020e3a7",
   "metadata": {},
   "source": [
    "<a id='6.1'></a>\n",
    "### 6.1 : Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f27c407",
   "metadata": {},
   "source": [
    "> * To get signal (informative part) from data and leave out noise\n",
    "> * Used for preprocessing TS data to\n",
    "    - remove seasonality (detected by making box plots for each time interval)\n",
    "    - detrending it (by taking a linear regression line equation and subtracting it with original data)\n",
    "    - differencing it (if the change in data from time t ot t+n is same for any time period, then the difference b/w the points can be used as TS, if n=1 it is first difference )\n",
    "    - make it stationary \n",
    "    - remove outliers \n",
    "    - Smoothing out by moving average or exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226aee3e",
   "metadata": {},
   "source": [
    "[SOURCE](https://www.youtube.com/playlist?list=PLvcbYUQ5t0UHOLnBzl46_Q6QKtFgfMGc3)\n",
    "#### 6.1.1 Autocorrelation\n",
    "> * The TS might have memory, i.e. values at time t will be dependent on previous values unless it a random walk data\n",
    "> * Finding out how value at T is getting affected by value at t-n gives the correlation or ACF\n",
    "> * This can have two types of affect one is direct i.e. t-n value affecting t's value or  t-n affecting t via t-(n+1) t-(n+2) ... t-(n-n-1)\n",
    "> * Can be found by having x as original value and y as -n values and finding pearson correlation between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0005daf",
   "metadata": {},
   "source": [
    "#### 6.1.2 Partial Autocorrelation\n",
    "> * If only the direct affect is considered on t frmo t-n then its partial autocorrelation\n",
    "> * This ignores the indirect effect\n",
    "> * Can be found using regression equation giving weights to past values\n",
    "        t(n) = w1*t(n-1)+w2*t(n-2)+w3*t(n-3)+w4*t(n-4)+... + error \n",
    "> * where wn is the the direct effect of that time to the present value\n",
    "> * values of the direct effect (wn) if more than the threshold will be considered having significant effect in finding present value given the past of these values of weights\n",
    "> * This equation is AR (auto rgessive model) which has a decaying Auto correlation plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270d8c0",
   "metadata": {},
   "source": [
    "<a id='6.2'></a>\n",
    "### 6.2 : ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f8614",
   "metadata": {},
   "source": [
    "> * AR(n) : y(t) = w1*y(t-1)+w2*y(t-2)+w3*y(t-3)+...wn*y(t-n)+E\n",
    "> * MA(n) : y(t) = mean + w1*e(t-1)+w2*e(t-2)+w3*e(t-3)+...wn*e(t-n)\n",
    "> * If ACF wanes over time it indicates MA elements\n",
    "> * If ACF oscillates over time it indicates AR elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109eac1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0419add",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21248849",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fc23b93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6768449a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4ecfd6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d505435",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d5aa91c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf9a26b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7bd861b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aa41069",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b14545d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "574264c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b5f732f",
   "metadata": {},
   "source": [
    "<a id='chapter_7'></a>\n",
    "## Chapter 7 : Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dce687",
   "metadata": {},
   "source": [
    "<a id='chapter_8'></a>\n",
    "## Chapter 8 : Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71122c2",
   "metadata": {},
   "source": [
    "<a id='chapter_9'></a>\n",
    "## Chapter 9 : Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb539049",
   "metadata": {},
   "source": [
    "<a id='#chapter_10'></a>\n",
    "## Chapter 10 : Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ae23b",
   "metadata": {},
   "source": [
    "<a id='#chapter_11'></a>\n",
    "## Chapter 11 : Maintaining Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38c3b9",
   "metadata": {},
   "source": [
    "<a id='#11.1'></a>\n",
    "### **11.1 MLFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd2fe0e",
   "metadata": {},
   "source": [
    "#### 11.1.1 Working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e39263",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "807ed0ef",
   "metadata": {},
   "source": [
    "#### 11.1.2 Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e3d95",
   "metadata": {},
   "source": [
    "**TRACKING**\n",
    "> * Records code, configurations, resultd, logs\n",
    "\n",
    "**PROJECTS**\n",
    "> * Packages projects to reproduce anywhere\n",
    "\n",
    "**MODELS**\n",
    "> * Model deployment\n",
    "\n",
    "**REGISTRY**\n",
    "> * ui ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b1d6b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6a17038",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d505fc46",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "148cd7bd",
   "metadata": {},
   "source": [
    "<a id='#chapter_12'></a>\n",
    "## Chapter 12 : Git stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a856b21",
   "metadata": {},
   "source": [
    "<a id='#chapter_13'></a>\n",
    "## Chapter 13 : Cloud Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efcef6e",
   "metadata": {},
   "source": [
    "<a id='#chapter_14'></a>\n",
    "## Chapter 14 : Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de948a6",
   "metadata": {},
   "source": [
    "<a id='#chapter_15'></a>\n",
    "## Chapter 15 : Data Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ca2c0",
   "metadata": {},
   "source": [
    "<a id='#chapter_16'></a>\n",
    "## Chapter 16 : Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e96b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349cfa65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324a000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bdaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c974b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e404a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581c275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425019ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96734134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9f138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efe105",
   "metadata": {},
   "outputs": [],
   "source": [
    "** **\n",
    "> *\n",
    "> * PROS:\n",
    "    *\n",
    "> * CONS:\n",
    "    *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
