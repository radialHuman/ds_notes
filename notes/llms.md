# Large Language Models & AI Models - Comprehensive List

## OpenAI Models
GPT-4
GPT-4 Turbo
GPT-4o
GPT-4o mini
GPT-3.5-turbo
GPT-3
Gpt-oss
Codex
InstructGPT
DALL-E 3
DALL-E 2
Whisper
Text-Embedding-Ada-002
Text-Embedding-3-Small
Text-Embedding-3-Large

## Anthropic Models
Claude 3.5 Sonnet
Claude 3 Opus
Claude 3 Sonnet
Claude 3 Haiku
Claude 2.1
Claude 2
Claude Instant

## Google Models
Gemini Ultra
Gemini Pro
Gemini Pro Vision
Gemini Nano
Gemini 1.5 Pro
Gemini 1.5 Flash
PaLM 2
Bard
LaMDA

## Meta Models
Llama 3.1 (405B, 70B, 8B)
Llama 3 (70B, 8B)
Llama 2 (70B, 13B, 7B)
Llama 2 Chat
Code Llama
Purple Llama

## Alibaba/Qwen Models
Qwen 2.5 VL 32B
Qwen 2.5 (72B, 32B, 14B, 7B)
Qwen 2.5 Instruct
Qwen 2.5 Coder
Qwen 3 8B
Qwen 2
Qwen
QwQ-32B-Preview
Qwen-VL
Qwen Embedding (various sizes)

## Mistral AI Models
Mistral Large
Mistral Medium
Mistral Small
Mistral 7B
Mixtral 8x7B
Mixtral 8x22B
Codestral
Mathstral

## Cohere Models
Command R+
Command R
Command
Command Light
Embed v3
Rerank v3

## Microsoft Models
Phi-3 (Mini, Small, Medium)
Phi-2
Phi-1.5
Orca 2
WizardLM
Florence-2

## Hugging Face Models
BLOOM
BLOOMZ
Falcon 180B
Falcon 40B
Falcon 7B
StarCoder
StarChat
Zephyr

## DeepSeek Models
DeepSeek-V2
DeepSeek-Coder-V2
DeepSeek-Coder
DeepSeek-LLM

## ByteDance Models
Doubao 1.5 Pro
Doubao Pro
Volcengine

## xAI Models
Grok-1
Grok-1.5
Grok-2

## AI21 Labs Models
Jurassic-2 Ultra
Jurassic-2 Mid
Jurassic-2 Light
Jamba

## Stability AI Models
Stable LM 2
Stable LM
StableCode
Stable Diffusion XL
Stable Diffusion 3

## Amazon Models
Titan Text Express
Titan Text Lite
Titan Embeddings
Titan Multimodal Embeddings

## BERT Family (Encoder Models)
BERT (Base, Large)
RoBERTa
ALBERT
DistilBERT
DeBERTa
ELECTRA
BioBERT
SciBERT
ClinicalBERT
FinBERT
LegalBERT

## T5 Family
T5 (Small, Base, Large, 3B, 11B)
Flan-T5
UL2
mT5

## Sequence-to-Sequence Models
BART (Base, Large)
PEGASUS
ProphetNet
MASS

## Chinese Language Models
ChatGLM-6B
ChatGLM2-6B
ChatGLM3-6B
ERNIE (各版本)
Baichuan (7B, 13B)
Baichuan2
Aquila
InternLM
InternLM2
Yi (6B, 34B)
Yi-VL

## Japanese Models
Rinna
Stockmark
Japanese StableLM
CyberAgent OpenLM

## Korean Models
KoGPT
Polyglot-Ko
KoAlpaca

## Open Source Models
MPT-7B
MPT-30B
RedPajama
OpenLLaMA
Vicuna
Alpaca
Dolly
Pythia
GPT-J
GPT-NeoX
OPT (各サイズ)
RWKV

## Specialized Coding Models
Code Llama
StarCoder
StarCoderBase
CodeGen
CodeT5
CodeBERT
GraphCodeBERT
InCoder
SantaCoder
Replit Code
WizardCoder
Phind-CodeLlama
DeepSeek-Coder

## Embedding Models
OpenAI Ada-002
OpenAI Embedding-3
BGE (Base, Large, M3)
BGE Large Chinese v1.5
BGE M3
E5 (Small, Base, Large)
Instructor Embeddings
all-mpnet-base-v2
all-MiniLM-L6-v2
all-MiniLM-L12-v2
SGPT
GTE (Base, Large)
Jina Embeddings v3
Cohere Embed
Voyage AI Embeddings

## Multimodal Models
CLIP
BLIP
BLIP-2
LLaVA
MiniGPT-4
InstructBLIP
Flamingo
KOSMOS-1
KOSMOS-2
Qwen-VL
CogVLM
Otter
OpenFlamingo

## Math & Reasoning Models
Minerva
Galactica
Llemma
MathPrompter
WizardMath
MetaMath
MAmmoTH

## Long Context Models
LongChat
LongLLaMA
Yarn-Llama
Together Llama-2-7B-32K
MPT-7B-StoryWriter
Claude 2 (100k)
GPT-4 Turbo (128k)

## Instruction-Tuned Models
Flan-T5
Flan-UL2
Alpaca
Vicuna
Koala
WizardLM
Orca
OpenAssistant
Guanaco

## Small/Efficient Models
TinyLlama
MobileLLaMA
phi-1
phi-1.5
phi-2
phi-3
Gemma 2B
Gemma 7B
Gemma 2 (9B, 27B)
MiniCPM
OpenELM

## Medical/Healthcare Models
Med-PaLM
Med-PaLM 2
BioGPT
BioBERT
ClinicalBERT
GatorTron
BioMedLM

## Legal Models
LegalBERT
Pile of Law
Lawyer LLaMA

## Finance Models
BloombergGPT
FinGPT
FinBERT

## Multilingual Models
mBERT
XLM
XLM-RoBERTa
mT5
mBART
XGLM
BLOOM
Aya-101

## Agent/Tool-Use Models
Toolformer
Gorilla
ToolLLM
GPT-4 with Functions
Claude with Tools

## Vision Language Models
GPT-4V
Claude 3 Vision
Gemini Pro Vision
LLaVA
CogVLM
Qwen-VL
Fuyu-8B

## Audio/Speech Models
Whisper (Tiny, Base, Small, Medium, Large)
Wav2Vec2
HuBERT
WavLM
SpeechT5
VALL-E
AudioLM
MusicLM
Gpt-realtime

## Retrieval-Augmented Models
REALM
RAG
FiD (Fusion-in-Decoder)
Atlas

## Research/Experimental Models
Chinchilla
Gopher
Jurassic-1
MT-NLG
GLM-130B
PanGu-α
CPM-Bee
Megatron-Turing NLG

## Open Assistant Models
OpenAssistant (Pythia, LLaMA variants)
Guanaco
Nous-Hermes
Nous-Hermes 2
Hermes (various)

## StableLM Variants
StableLM Alpha
StableLM Tuned
StableLM Zephyr

## MPT Variants
MPT-7B
MPT-7B-Instruct
MPT-7B-Chat
MPT-30B
MPT-30B-Instruct
MPT-30B-Chat

## Yi Models
Yi-6B
Yi-34B
Yi-6B-200K
Yi-34B-200K
Yi-VL-6B
Yi-VL-34B

## Baichuan Models
Baichuan-7B
Baichuan-13B
Baichuan2-7B
Baichuan2-13B

## InternLM Models
InternLM-7B
InternLM-20B
InternLM2-7B
InternLM2-20B

## Other Notable Models
Kimi (Moonshot AI)
Granite (IBM)
Megatron (NVIDIA)
NeMo (NVIDIA)
Cerebras-GPT
GLM-4
ChatGLM
Ernie Bot (百度)
Spark (讯飞)
SenseChat (商汤)
Tongyi Qianwen (通义千问)


---

# LLM Innovations & Architectural Breakthroughs

## OpenAI Models

### GPT-4
- **Innovations**: Mixture of Experts (MoE) architecture (rumored), enhanced reasoning, multimodal capabilities
- **Key Changes**: Improved instruction following, better factual accuracy, reduced hallucinations

### GPT-4 Turbo
- **Innovations**: Extended 128K context window, optimized inference speed
- **Key Changes**: JSON mode, function calling improvements, vision capabilities

### GPT-4o
- **Innovations**: Native multimodal processing (text, vision, audio in single model)
- **Key Changes**: Real-time voice interaction, faster inference, lower latency

### GPT-4o mini
- **Innovations**: Distillation techniques for efficiency
- **Key Changes**: Cost-optimized while maintaining GPT-4 capabilities

### GPT-3.5-turbo
- **Innovations**: RLHF (Reinforcement Learning from Human Feedback) optimization
- **Key Changes**: Faster inference, instruction tuning improvements

### Codex
- **Innovations**: Code-specialized training corpus
- **Key Changes**: Enhanced code understanding and generation through targeted pretraining

### InstructGPT
- **Innovations**: Pioneered RLHF methodology
- **Key Changes**: Aligned model outputs with human preferences, reduced harmful outputs

### DALL-E 3
- **Innovations**: Improved text-image alignment, prompt following
- **Key Changes**: Better compositional understanding, reduced bias

### Whisper
- **Innovations**: Weakly supervised training on 680K hours of data
- **Key Changes**: Robust multilingual speech recognition, noise handling

### Text-Embedding-Ada-002
- **Innovations**: Contrastive learning optimization
- **Key Changes**: Superior semantic search performance at lower cost

### Text-Embedding-3
- **Innovations**: Matryoshka embeddings (variable dimensions)
- **Key Changes**: Flexible embedding sizes, improved efficiency

## Anthropic Models

### Claude 3.5 Sonnet
- **Innovations**: Constitutional AI improvements, extended thinking
- **Key Changes**: Enhanced coding abilities, better instruction following

### Claude 3 Opus
- **Innovations**: Larger context processing (200K tokens)
- **Key Changes**: Improved reasoning on complex tasks, better factual accuracy

### Claude 3 (Sonnet/Haiku)
- **Innovations**: Vision capabilities, Constitutional AI
- **Key Changes**: Safer outputs, reduced harmful content generation

### Claude 2
- **Innovations**: 100K context window
- **Key Changes**: Long document understanding, improved coding

### Constitutional AI
- **Innovations**: Self-critique and revision mechanism
- **Key Changes**: Models learn to identify and correct harmful outputs

## Google Models

### Gemini Ultra
- **Innovations**: Native multimodal architecture (not bolted-on)
- **Key Changes**: Joint training on text, image, video, audio from scratch

### Gemini 1.5 Pro
- **Innovations**: 1 million token context window (later 2M)
- **Key Changes**: Sparse attention mechanisms, efficient long-context processing

### Gemini 1.5 Flash
- **Innovations**: Distillation with speed optimization
- **Key Changes**: Maintains quality while reducing latency significantly

### PaLM 2
- **Innovations**: Improved multilingual training, Pathways system
- **Key Changes**: Better reasoning, more efficient architecture

### LaMDA
- **Innovations**: Dialog-specific pretraining
- **Key Changes**: Grounding in external knowledge, factuality improvements

## Meta Models

### Llama 3.1 (405B)
- **Innovations**: Grouped-Query Attention (GQA), 128K context
- **Key Changes**: Open weights at massive scale, multilingual improvements

### Llama 3
- **Innovations**: Improved tokenizer (128K vocab), better data curation
- **Key Changes**: Enhanced instruction following, reasoning capabilities

### Llama 2
- **Innovations**: Ghost Attention for multi-turn consistency
- **Key Changes**: Open commercial license, RLHF fine-tuning

### Code Llama
- **Innovations**: Infilling capabilities, long context for code (100K)
- **Key Changes**: Specialized code training, repository-level understanding

### Purple Llama
- **Innovations**: Cybersecurity-focused guardrails
- **Key Changes**: Safety evaluation and mitigation tools

## Alibaba/Qwen Models

### Qwen 2.5
- **Innovations**: Dual-chunk attention for long sequences
- **Key Changes**: Enhanced multilingual (29 languages), better math/code

### QwQ-32B-Preview
- **Innovations**: Deep reasoning with chain-of-thought
- **Key Changes**: Competitive with O1 on reasoning tasks

### Qwen-VL
- **Innovations**: Vision-language pre-training with grounding
- **Key Changes**: Fine-grained image understanding, OCR capabilities

### Qwen Embedding
- **Innovations**: Bidirectional attention for embeddings
- **Key Changes**: State-of-art retrieval performance

## Mistral AI Models

### Mixtral 8x7B
- **Innovations**: Sparse Mixture of Experts (only 2/8 experts active)
- **Key Changes**: 47B params but 13B active, efficient scaling

### Mixtral 8x22B
- **Innovations**: Larger-scale sparse MoE
- **Key Changes**: 141B total, 39B active parameters

### Mistral 7B
- **Innovations**: Sliding Window Attention (4K window)
- **Key Changes**: Efficient attention with local + global tokens

### Codestral
- **Innovations**: Fill-in-the-middle training
- **Key Changes**: 32K context for code, 80+ languages

### Mathstral
- **Innovations**: Math-specific tokenization and training
- **Key Changes**: Enhanced numerical reasoning

## Cohere Models

### Command R+
- **Innovations**: RAG-optimized architecture
- **Key Changes**: Built-in citations, grounded generation

### Command R
- **Innovations**: Tool use optimization
- **Key Changes**: Function calling, multi-step reasoning

### Embed v3
- **Innovations**: Compression-aware embeddings
- **Key Changes**: Efficient storage, maintained quality

### Rerank v3
- **Innovations**: Cross-encoder for reranking
- **Key Changes**: Improved relevance scoring for search

## Microsoft Models

### Phi-3
- **Innovations**: Small model with synthetic data training
- **Key Changes**: 3.8B params competitive with 7B models

### Phi-2
- **Innovations**: Textbook-quality training data
- **Key Changes**: 2.7B params, strong reasoning for size

### Orca 2
- **Innovations**: Explanation tuning, cautious reasoning
- **Key Changes**: Teaches when to use different strategies

### WizardLM
- **Innovations**: Evol-Instruct for complex instructions
- **Key Changes**: Progressively more complex training data

### Florence-2
- **Innovations**: Unified vision tasks architecture
- **Key Changes**: Single model for detection, segmentation, captioning

## Hugging Face Models

### BLOOM
- **Innovations**: Multilingual pretraining (46 languages)
- **Key Changes**: Open collaborative training, ALiBi positional encoding

### Falcon
- **Innovations**: RefinedWeb dataset (high-quality web data)
- **Key Changes**: Efficient architecture, FlashAttention

### StarCoder
- **Innovations**: Fill-in-the-middle objective
- **Key Changes**: Multi-query attention, 8K context

### Zephyr
- **Innovations**: Direct preference optimization (DPO)
- **Key Changes**: Alternative to RLHF, simpler alignment

## DeepSeek Models

### DeepSeek-V2
- **Innovations**: Multi-head Latent Attention (MLA)
- **Key Changes**: Reduced KV cache, MoE with 236B total params

### DeepSeek-Coder-V2
- **Innovations**: Code-specific MLA optimization
- **Key Changes**: 128K context, 338 languages support

## xAI Models

### Grok-1
- **Innovations**: Real-time X (Twitter) data integration
- **Key Changes**: Open weights (314B MoE)

### Grok-2
- **Innovations**: Enhanced reasoning, multimodal
- **Key Changes**: Improved factuality, reduced latency

## AI21 Labs Models

### Jamba
- **Innovations**: Hybrid SSM-Transformer architecture
- **Key Changes**: Combines Mamba (SSM) with attention, 256K context

## Stability AI Models

### Stable LM 2
- **Innovations**: Small model efficiency (1.6B, 12B)
- **Key Changes**: Rotary embeddings, grouped-query attention

### StableCode
- **Innovations**: Long context for code (16K)
- **Key Changes**: Fill-in-middle, multi-language

## Amazon Models

### Titan Text
- **Innovations**: Enterprise-focused safety features
- **Key Changes**: Watermarking, bias mitigation

### Titan Embeddings
- **Innovations**: Normalized embeddings
- **Key Changes**: Optimized for semantic search

## BERT Family

### BERT
- **Innovations**: Bidirectional pretraining, masked language modeling
- **Key Changes**: Revolutionized NLP, context from both directions

### RoBERTa
- **Innovations**: Removed NSP, dynamic masking
- **Key Changes**: Longer training, larger batches

### ALBERT
- **Innovations**: Parameter sharing across layers
- **Key Changes**: Factorized embeddings, 18x fewer params

### DistilBERT
- **Innovations**: Knowledge distillation
- **Key Changes**: 40% smaller, 60% faster, 97% performance

### DeBERTa
- **Innovations**: Disentangled attention (content + position separate)
- **Key Changes**: Enhanced mask decoder, better position encoding

### ELECTRA
- **Innovations**: Replaced token detection (discriminator)
- **Key Changes**: More efficient pretraining, less compute needed

## T5 Family

### T5
- **Innovations**: Unified text-to-text framework
- **Key Changes**: Every task as text generation, transfer learning

### Flan-T5
- **Innovations**: Instruction finetuning at scale
- **Key Changes**: 1,800+ tasks, better zero-shot performance

### UL2
- **Innovations**: Unified pretraining (mixture of denoisers)
- **Key Changes**: Combines different masking strategies

## Sequence-to-Sequence Models

### BART
- **Innovations**: Denoising autoencoder for seq2seq
- **Key Changes**: Combines BERT+GPT-style training

### PEGASUS
- **Innovations**: Gap sentence generation pretraining
- **Key Changes**: Optimized for summarization tasks

## Chinese Language Models

### ChatGLM
- **Innovations**: Bidirectional + autoregressive hybrid
- **Key Changes**: GLM pretraining objective, Chinese focus

### ERNIE
- **Innovations**: Knowledge-enhanced pretraining
- **Key Changes**: Entity and phrase masking

### Baichuan
- **Innovations**: Chinese-optimized tokenizer
- **Key Changes**: High-quality Chinese corpus

### InternLM
- **Innovations**: Progressive training strategy
- **Key Changes**: Multi-stage pretraining with different data mixes

### Yi
- **Innovations**: 200K context window capability
- **Key Changes**: Depth-up attention, efficient long context

## Open Source Models

### MPT
- **Innovations**: ALiBi positional encoding (no learned positions)
- **Key Changes**: FlashAttention, commercial license

### Pythia
- **Innovations**: Training dynamics research
- **Key Changes**: Checkpoints at every training step

### GPT-J
- **Innovations**: Rotary positional embeddings (RoPE)
- **Key Changes**: Open alternative to GPT-3

### GPT-NeoX
- **Innovations**: Parallel attention + FF layers
- **Key Changes**: 20B params, open training

### RWKV
- **Innovations**: RNN-like architecture with transformer performance
- **Key Changes**: Linear complexity, no attention mechanism

## Specialized Coding Models

### CodeT5
- **Innovations**: Code-aware pretraining tasks
- **Key Changes**: Identifier tagging, bimodal training

### CodeBERT
- **Innovations**: Code-documentation pairs training
- **Key Changes**: Programming language understanding

### GraphCodeBERT
- **Innovations**: Data flow graph integration
- **Key Changes**: Semantic-level code structure

### WizardCoder
- **Innovations**: Evol-Instruct for code
- **Key Changes**: Complexity-increasing code instructions

## Embedding Models

### BGE (BAAI General Embedding)
- **Innovations**: RetroMAE pretraining
- **Key Changes**: Bidirectional attention with reconstruction

### BGE M3
- **Innovations**: Multi-functionality, multi-linguality, multi-granularity
- **Key Changes**: Dense + sparse + multi-vector retrieval

### E5
- **Innovations**: Contrastive pretraining with weak supervision
- **Key Changes**: Text pair mining from web

### Instructor Embeddings
- **Innovations**: Task instruction prefix
- **Key Changes**: Single model for multiple embedding tasks

### GTE
- **Innovations**: General text embeddings with multi-stage training
- **Key Changes**: Large-scale contrastive learning

### Voyage AI
- **Innovations**: Context-aware embeddings
- **Key Changes**: Domain adaptation capabilities

## Multimodal Models

### CLIP
- **Innovations**: Contrastive language-image pretraining
- **Key Changes**: Zero-shot image classification

### BLIP-2
- **Innovations**: Querying Transformer (Q-Former)
- **Key Changes**: Efficient vision-language alignment

### LLaVA
- **Innovations**: Visual instruction tuning
- **Key Changes**: Connects vision encoder to LLM

### MiniGPT-4
- **Innovations**: Single projection layer alignment
- **Key Changes**: Minimal training for vision-language

### Flamingo
- **Innovations**: Perceiver Resampler for images
- **Key Changes**: Few-shot multimodal learning

### CogVLM
- **Innovations**: Visual expert attention mechanism
- **Key Changes**: Deep fusion of vision and language

## Math & Reasoning Models

### Minerva
- **Innovations**: Math-focused pretraining corpus
- **Key Changes**: LaTeX rendering, symbolic reasoning

### Galactica
- **Innovations**: Scientific knowledge organization
- **Key Changes**: Citations, reasoning steps, formulas

### Llemma
- **Innovations**: Math-code continual pretraining
- **Key Changes**: AlgebraicStack dataset

### WizardMath
- **Innovations**: Reinforcement learning from evol-instruct
- **Key Changes**: GSM8K and MATH optimized

### MAmmoTH
- **Innovations**: Hybrid reasoning (CoT + PoT)
- **Key Changes**: Chain-of-thought + program-of-thought

## Long Context Models

### LongChat
- **Innovations**: Sparse attention patterns
- **Key Changes**: Condensed memory mechanisms

### Yarn-Llama
- **Innovations**: YaRN (Yet another RoPE extensioN)
- **Key Changes**: Extended context without fine-tuning

### MPT-7B-StoryWriter
- **Innovations**: 65K context with ALiBi
- **Key Changes**: Book-length understanding

## Small/Efficient Models

### TinyLlama
- **Innovations**: 3T tokens on 1.1B params
- **Key Changes**: Extended training for small models

### Gemma
- **Innovations**: Knowledge distillation from Gemini
- **Key Changes**: Safe, aligned small models

### MiniCPM
- **Innovations**: WSD (Warmup-Stable-Decay) learning rate
- **Key Changes**: 2.4B model competitive with 7B

### OpenELM
- **Innovations**: Layer-wise scaling strategy
- **Key Changes**: Efficient parameter allocation

## Medical Models

### Med-PaLM 2
- **Innovations**: Medical domain instruction tuning
- **Key Changes**: MultiMedQA benchmark, uncertainty estimation

### BioGPT
- **Innovations**: Domain-specific pretraining (biomedical literature)
- **Key Changes**: PubMed corpus training

### GatorTron
- **Innovations**: Clinical notes pretraining (90B tokens)
- **Key Changes**: HIPAA-compliant training

## Finance Models

### BloombergGPT
- **Innovations**: Finance domain pretraining (700B tokens)
- **Key Changes**: Bloomberg terminal data integration

### FinGPT
- **Innovations**: Sentiment analysis for finance
- **Key Changes**: Real-time market data training

## Multilingual Models

### XLM-RoBERTa
- **Innovations**: Multilingual masked language modeling (100 languages)
- **Key Changes**: No language-specific tokens

### mT5
- **Innovations**: Multilingual text-to-text
- **Key Changes**: 101 languages, unified pretraining

### mBART
- **Innovations**: Multilingual denoising pretraining
- **Key Changes**: Translation and generation

### Aya-101
- **Innovations**: Massively multilingual (101 languages)
- **Key Changes**: Instruction tuning for diverse languages

## Agent Models

### Toolformer
- **Innovations**: Self-supervised tool use learning
- **Key Changes**: API calling without demonstrations

### Gorilla
- **Innovations**: APIBench training dataset
- **Key Changes**: API documentation understanding

### ToolLLM
- **Innovations**: Tool learning with depth-first search
- **Key Changes**: 16,000+ APIs training

## Audio/Speech Models

### Whisper
- **Innovations**: Weakly supervised multitask learning
- **Key Changes**: Transcription + translation jointly

### Wav2Vec2
- **Innovations**: Self-supervised speech representation
- **Key Changes**: Contrastive learning on audio

### VALL-E
- **Innovations**: Neural codec language modeling
- **Key Changes**: Zero-shot voice cloning

### AudioLM
- **Innovations**: Audio generation as language modeling
- **Key Changes**: Semantic + acoustic tokens

## Research Models

### Chinchilla
- **Innovations**: Compute-optimal scaling laws
- **Key Changes**: Fewer params, more tokens (70B with 1.4T tokens)

### Gopher
- **Innovations**: 280B dense model
- **Key Changes**: Large-scale analysis of scaling

### GLM-130B
- **Innovations**: General Language Model pretraining
- **Key Changes**: Autoregressive blank infilling

### Megatron-Turing NLG
- **Innovations**: 530B dense model, 3D parallelism
- **Key Changes**: Tensor + pipeline + data parallelism

---

# Alternative (Non-Transformer) Architectures

## State Space Models (SSMs)

### Mamba
- **Innovation**: Selective State Space Models
- **Architecture**: Linear-time sequence modeling with data-dependent transitions
- **Key Advantage**: O(N) complexity vs transformer's O(N²), better long sequences
- **Mechanism**: Selective copying mechanism, hardware-aware parallel scan

### Mamba-2
- **Innovation**: Structured State Space Duality (SSD)
- **Architecture**: Combines SSM with attention-like computation
- **Key Advantage**: 2-8x faster than Mamba, better quality

### S4 (Structured State Spaces)
- **Innovation**: HiPPO initialization, diagonal plus low-rank structure
- **Architecture**: Continuous-time SSM discretized for sequences
- **Key Advantage**: Long-range dependencies without attention

### H3 (Hungry Hungry Hippos)
- **Innovation**: Combines shift SSM with multiplicative interactions
- **Architecture**: Hierarchical state space layers
- **Key Advantage**: Faster than S4, competitive with transformers

## Hybrid Architectures

### Jamba (AI21)
- **Innovation**: Mamba + Transformer hybrid with MoE
- **Architecture**: Alternating Mamba and attention layers, 8 experts
- **Key Advantage**: 256K context, efficient memory usage
- **Ratio**: 1:7 attention-to-Mamba layers

### StripedHyena
- **Innovation**: Hyena operator with gating
- **Architecture**: Combines convolutions with gating mechanisms
- **Key Advantage**: 32K context, subquadratic attention alternative

### RWKV (Receptance Weighted Key Value)
- **Innovation**: RNN with transformer-level performance
- **Architecture**: Linear attention approximation, recurrent processing
- **Key Advantage**: O(N) time and space, parallelizable training

### RetNet (Retentive Networks)
- **Innovation**: Retention mechanism (dual form of recurrence and parallelism)
- **Architecture**: Can operate as RNN or parallel like transformers
- **Key Advantage**: Training parallelism + inference efficiency

## Convolutional Approaches

### Hyena
- **Innovation**: Long convolutions with implicit parametrization
- **Architecture**: Replaces attention with long convolutional filters
- **Key Advantage**: Subquadratic complexity, 100K+ sequences

### Conv-Mixer
- **Innovation**: Pure convolutional architecture for vision
- **Architecture**: Depth-wise + point-wise convolutions
- **Key Advantage**: Simpler than ViT, competitive performance

## Recursive/Recurrent Models

### Fast Weight Programmers
- **Innovation**: Fast weight memory with slow weight control
- **Architecture**: Outer product-based associative memory
- **Key Advantage**: Differentiable neural computer alternative

### Liquid Neural Networks
- **Innovation**: Time-continuous RNN with ODE solver
- **Architecture**: Continuous-time dynamics with causality
- **Key Advantage**: Better on temporal data, interpretable

## Memory-Augmented Models

### Memorizing Transformers
- **Innovation**: kNN-augmented attention over external memory
- **Architecture**: Retrieval from long-term memory bank
- **Key Advantage**: Unbounded context without retraining

### Compressive Transformer
- **Innovation**: Compressed past activations
- **Architecture**: Compressed memory + recent context
- **Key Advantage**: Extended effective context window

## Self-Attention Alternatives

### Performer
- **Innovation**: FAVOR+ (Fast Attention Via Orthogonal Random features)
- **Architecture**: Kernelized attention approximation
- **Key Advantage**: O(N) complexity, unbiased estimator

### Linformer
- **Innovation**: Low-rank attention approximation
- **Architecture**: Projects keys/values to lower dimension
- **Key Advantage**: Linear complexity in sequence length

### Nyströmformer
- **Innovation**: Nyström method for attention approximation
- **Architecture**: Landmark-based attention
- **Key Advantage**: Better approximation than Linformer

### FNet
- **Innovation**: Fourier Transform replaces attention
- **Architecture**: FFT for token mixing
- **Key Advantage**: 7x faster on GPUs, 92% accuracy on BERT tasks

## Energy-Based Models

### Diffusion Models (for language)
- **Innovation**: Continuous diffusion in embedding space
- **Architecture**: Score-based generative modeling
- **Examples**: Diffusion-LM, GENIE
- **Key Advantage**: Better likelihood modeling, controllability

## Graph Neural Architectures

### Graph Transformers
- **Innovation**: Attention with graph structure bias
- **Architecture**: Node features + edge features in attention
- **Key Advantage**: Structured data understanding

## Capsule Networks (for NLP)

### Capsule-BERT
- **Innovation**: Capsule routing for linguistic features
- **Architecture**: Part-whole hierarchies in text
- **Key Advantage**: Better compositional understanding

## Associative Architectures

### Hopfield Networks (Modern)
- **Innovation**: Continuous modern Hopfield networks
- **Architecture**: Energy-based associative memory
- **Key Advantage**: Exponential storage capacity

## Self-Organizing Models

### Kohonen Self-Organizing Maps (for embeddings)
- **Innovation**: Topology-preserving mapping
- **Architecture**: Competitive learning with neighborhood
- **Key Advantage**: Interpretable embedding spaces

## Joint Embedding Architectures

### JEPA (Joint Embedding Predictive Architecture) - Meta
- **Innovation**: Learns representations by predicting in latent space
- **Architecture**: Encoder predicts missing parts without pixel reconstruction
- **Key Advantage**: Energy-efficient, learns abstract representations
- **Variants**: I-JEPA (images), V-JEPA (video)
- **Key Difference**: No generative loss, prediction in representation space

### Image-JEPA (I-JEPA)
- **Innovation**: Masked image modeling in embedding space
- **Architecture**: Vision Transformer with context + target encoders
- **Key Advantage**: 2-10x more sample efficient than MAE

### Video-JEPA (V-JEPA)
- **Innovation**: Temporal prediction in latent space
- **Architecture**: Predicts future frame representations
- **Key Advantage**: Better video understanding, no pixel generation

## Sparse Models

### Switch Transformer
- **Innovation**: Simplified MoE with single expert routing
- **Architecture**: One expert per token (hard routing)
- **Key Advantage**: 1.6T params, easier to train than soft MoE

### GShard
- **Innovation**: MoE with sharded experts
- **Architecture**: Top-2 gating with load balancing
- **Key Advantage**: 600B+ params, efficient scaling

## Hardware-Optimized Architectures

### FlashAttention Models
- **Innovation**: IO-aware attention algorithm
- **Architecture**: Tiled computation to reduce memory reads/writes
- **Key Advantage**: 2-4x faster, enables longer contexts

### BitNet
- **Innovation**: 1-bit quantization during training
- **Architecture**: Binary or ternary weights
- **Key Advantage**: 10x faster inference, 90% energy saving

### BitNet b1.58
- **Innovation**: Ternary weights (-1, 0, +1)
- **Architecture**: 1.58 bits per parameter
- **Key Advantage**: Matches full precision, extreme efficiency

## Emerging Architectures

### Megalodon
- **Innovation**: Exponential Moving Average + gating
- **Architecture**: Hybrid EMA-attention with complex gating
- **Key Advantage**: Better than Mamba on some benchmarks

### Griffin (Google DeepMind)
- **Innovation**: Gated linear RNN + local attention
- **Architecture**: Alternating recurrent and local attention blocks
- **Key Advantage**: Efficient training and inference

### Hawk (Google DeepMind)
- **Innovation**: Pure RNN-based with gated linear units
- **Architecture**: No attention at all
- **Key Advantage**: Constant memory during inference

### StripedHyena-2
- **Innovation**: Improved Hyena with better gating
- **Architecture**: Enhanced long convolutions
- **Key Advantage**: Competitive with LLaMA-2

### Hatchling (Cartesia AI)
- **Innovation**: State space model optimized for mobile
- **Architecture**: Efficient SSM variant for edge devices
- **Key Advantage**: Runs on phones, low latency

### Dragon (Together AI)
- **Innovation**: SSM with attention augmentation
- **Architecture**: Long-range SSM with sparse attention
- **Key Advantage**: 100K+ context efficiently

## Attention Mechanism Innovations

### Multi-Query Attention (MQA)
- **Innovation**: Single KV head shared across query heads
- **Used in**: PaLM, StarCoder
- **Key Advantage**: 10x faster decoding, reduced memory

### Grouped-Query Attention (GQA)
- **Innovation**: Multiple query heads share KV heads (compromise between MHA and MQA)
- **Used in**: Llama 3.1, Mistral
- **Key Advantage**: Better quality than MQA, faster than MHA

### Sliding Window Attention
- **Innovation**: Each token attends to fixed window
- **Used in**: Mistral, Longformer
- **Key Advantage**: Linear memory, local+global patterns

### Flash Attention
- **Innovation**: Tiling and recomputation strategy
- **Architecture**: IO-aware attention algorithm
- **Key Advantage**: 2-4x speedup, enables longer contexts

### Multi-Head Latent Attention (MLA)
- **Innovation**: Low-rank KV compression
- **Used in**: DeepSeek-V2
- **Key Advantage**: 93% KV cache reduction

### Mixture of Depths (MoD)
- **Innovation**: Token-level compute allocation
- **Architecture**: Some tokens skip layers
- **Key Advantage**: Reduced compute for less important tokens

## Positional Encoding Innovations

### RoPE (Rotary Position Embedding)
- **Innovation**: Rotary matrix encoding of positions
- **Used in**: GPT-J, Llama, Qwen
- **Key Advantage**: Relative positions, extrapolates to longer sequences

### ALiBi (Attention with Linear Biases)
- **Innovation**: Linear bias to attention scores based on distance
- **Used in**: BLOOM, MPT
- **Key Advantage**: Zero-shot extrapolation to longer contexts

### xPos (Exponential Positional Encoding)
- **Innovation**: Exponentially decaying position information
- **Architecture**: Optimizes length extrapolation
- **Key Advantage**: Better than RoPE for very long sequences

### YaRN (Yet another RoPE extensioN)
- **Innovation**: Non-uniform interpolation of RoPE
- **Architecture**: Attention-aware scaling
- **Key Advantage**: 128K context without retraining

## Normalization Innovations

### RMSNorm
- **Innovation**: Simplified LayerNorm (no mean subtraction)
- **Used in**: Llama, Mistral
- **Key Advantage**: 10-40% faster than LayerNorm

### DeepNorm
- **Innovation**: Post-LN with residual scaling
- **Used in**: Large-scale models
- **Key Advantage**: Stable training of 1000+ layers

## Activation Function Innovations

### SwiGLU (Swish-Gated Linear Unit)
- **Innovation**: Swish activation with gating
- **Used in**: Llama, PaLM
- **Key Advantage**: Better than ReLU/GELU for language

### GeGLU (GELU-Gated Linear Unit)
- **Innovation**: GELU activation with gating
- **Used in**: T5, various models
- **Key Advantage**: Improved expressiveness

## Training Objective Innovations

### Next Token Prediction (Causal LM)
- **Standard**: GPT-style autoregressive
- **Advantage**: Simple, effective for generation

### Masked Language Modeling (MLM)
- **Innovation**: Bidirectional context (BERT)
- **Advantage**: Better for understanding tasks

### Prefix LM
- **Innovation**: Bidirectional prefix + autoregressive suffix
- **Used in**: PaLM
- **Advantage**: Combines benefits of both

### UL2 Mixture of Denoisers
- **Innovation**: Mix of masking strategies
- **Architecture**: R-denoising, S-denoising, X-denoising
- **Advantage**: Single model for understanding + generation

### GLM (General Language Model)
- **Innovation**: Autoregressive blank infilling
- **Used in**: ChatGLM
- **Advantage**: Unified pre-training objective



## Additional Model Innovations

### GPT-3
- **Innovations**: Demonstrated few-shot learning at scale (175B params)
- **Key Changes**: In-context learning without fine-tuning, emergent abilities at scale
- **Impact**: Showed that scaling alone can unlock new capabilities

### GPT-o1 / o1-preview
- **Innovations**: Extended chain-of-thought reasoning, reinforcement learning for thinking
- **Key Changes**: Internal reasoning steps before answering, improved mathematical and logical reasoning
- **Impact**: State-of-art on competitive programming and math benchmarks

### DALL-E 2
- **Innovations**: CLIP-guided diffusion for text-to-image
- **Key Changes**: Unclip architecture, hierarchical image generation
- **Impact**: Pioneered high-quality text-to-image generation

### Claude 2.1
- **Innovations**: Reduced hallucination rates, improved accuracy
- **Key Changes**: Enhanced tool use, better refusal behavior
- **Impact**: 200K context with better factuality

### Claude Instant
- **Innovations**: Faster, more affordable Claude variant
- **Key Changes**: Distilled model with lower latency
- **Impact**: Enables high-throughput applications

### Gemini Nano
- **Innovations**: On-device LLM (1.8B, 3.25B variants)
- **Key Changes**: Quantization-aware training, mobile optimization
- **Impact**: Runs locally on Pixel phones

### Bard
- **Innovations**: Real-time internet access integration
- **Key Changes**: Direct Google Search integration, up-to-date information
- **Impact**: Reduced knowledge cutoff limitations

## ByteDance/Doubao Models

### Doubao (豆包)
- **Innovations**: Chinese-optimized multimodal model
- **Key Changes**: Integrated with ByteDance services, video understanding
- **Impact**: Competitive with GPT-4 on Chinese benchmarks

### Volcengine
- **Innovations**: Enterprise-grade API platform
- **Key Changes**: Fine-tuning capabilities, domain adaptation
- **Impact**: ByteDance's commercial LLM offering

## xAI Models (Detailed)

### Grok-1.5
- **Innovations**: Extended context to 128K, improved reasoning
- **Key Changes**: Better math and coding performance
- **Impact**: Open weights available for research

## AI21 Labs Jurassic Series

### Jurassic-2 Series
- **Innovations**: Modular architecture with task-specific heads
- **Key Changes**: Ultra (178B), Mid (larger than GPT-3.5), Light (smaller)
- **Impact**: Multilingual (Spanish, French, German, Portuguese, Italian, Dutch)

### Jurassic-1 Jumbo
- **Innovations**: 178B parameters, early GPT-3 competitor
- **Key Changes**: Superior in-context learning for non-English
- **Impact**: First non-OpenAI massive LLM

## Stability AI (Detailed)

### Stable Diffusion XL
- **Innovations**: Larger UNet backbone, two-stage pipeline
- **Key Changes**: Base model + refiner, better image quality
- **Impact**: 6B total parameters for image generation

### Stable Diffusion 3
- **Innovations**: Diffusion Transformer architecture (DiT)
- **Key Changes**: Multimodal Diffusion Transformer (MM-DiT), better text rendering
- **Impact**: Improved prompt following and text-in-image

## Domain-Specific BERT Models

### SciBERT
- **Innovations**: Scientific paper corpus pretraining (1.14M papers)
- **Key Changes**: Scientific vocabulary, better citation understanding
- **Impact**: Superior on biomedical/CS tasks

### ClinicalBERT
- **Innovations**: MIMIC-III clinical notes training
- **Key Changes**: Medical terminology, clinical abbreviations
- **Impact**: Better on ICU prediction tasks

### FinBERT
- **Innovations**: Financial news and reports pretraining
- **Key Changes**: Financial sentiment analysis optimization
- **Impact**: Superior stock movement prediction

### LegalBERT
- **Innovations**: Legal documents corpus (EU legislation, cases)
- **Key Changes**: Legal terminology and reasoning
- **Impact**: Better contract analysis and legal QA

## Sequence-to-Sequence Models

### ProphetNet
- **Innovations**: Future n-gram prediction
- **Key Changes**: Predicts next N tokens simultaneously
- **Impact**: Better for summarization and question generation

### MASS (Masked Sequence to Sequence)
- **Innovations**: Unified pretraining for encoder-decoder
- **Key Changes**: Masks continuous spans in encoder, predicts in decoder
- **Impact**: Improved translation and summarization

## Chinese Language Models (Detailed)

### ChatGLM-2
- **Innovations**: Multi-query attention, FlashAttention
- **Key Changes**: 32K context, better long-context understanding
- **Impact**: 6B model competitive with 13B models

### ChatGLM-3
- **Innovations**: Improved instruction following, tool use
- **Key Changes**: Function calling, code interpreter integration
- **Impact**: Better Chinese coding abilities

### Aquila (智源·悟道)
- **Innovations**: Chinese-English bilingual (7B, 33B)
- **Key Changes**: BMTrain for efficient training
- **Impact**: Open-source alternative to GPT-3

### ERNIE 3.0/4.0
- **Innovations**: Unified pretraining framework (knowledge graph integration)
- **Key Changes**: Auto-encoding + auto-regressive combined
- **Impact**: Baidu's flagship model, multimodal capabilities

## Japanese Models

### Rinna
- **Innovations**: Japanese GPT-2/GPT-NeoX variants
- **Key Changes**: Japanese Wikipedia and CC corpus
- **Impact**: 3.6B, 36B models for Japanese NLP

### Stockmark
- **Innovations**: Business Japanese optimization
- **Key Changes**: Financial news and business documents
- **Impact**: Superior for Japanese business applications

### Japanese StableLM
- **Innovations**: Stability AI's Japanese variant
- **Key Changes**: Japanese-English bilingual (7B)
- **Impact**: Open-source Japanese LLM

### CyberAgent OpenLM
- **Innovations**: Open Japanese LLM (7B, 13B)
- **Key Changes**: Optimized for Japanese social media
- **Impact**: Better Japanese informal language understanding

## Korean Models

### KoGPT (Kakao)
- **Innovations**: Korean GPT-2/GPT-3 style models
- **Key Changes**: Korean Wikipedia + news corpus
- **Impact**: 6B model, Korean language leader

### Polyglot-Ko
- **Innovations**: Korean-specific tokenizer and training
- **Key Changes**: 1.3B, 3.8B, 5.8B, 12.8B variants
- **Impact**: Open Korean models with strong performance

### KoAlpaca
- **Innovations**: Instruction-tuned Korean model
- **Key Changes**: Korean translation of Alpaca dataset
- **Impact**: Better Korean instruction following

## Open Source Models (Detailed)

### RedPajama
- **Innovations**: Open reproduction of LLaMA training data
- **Key Changes**: 1.2T tokens, fully open dataset
- **Impact**: Enables open research on data quality

### OpenLLaMA
- **Innovations**: Apache 2.0 licensed LLaMA reproduction
- **Key Changes**: Trained on RedPajama data
- **Impact**: Commercially usable LLaMA alternative

### Vicuna
- **Innovations**: GPT-4 distillation via ShareGPT conversations
- **Key Changes**: 70K user-shared ChatGPT conversations
- **Impact**: 13B model competitive with GPT-3.5

### Alpaca
- **Innovations**: Instruction tuning with self-instruct
- **Key Changes**: 52K instructions generated by GPT-3.5
- **Impact**: Showed effectiveness of synthetic instruction data

### Dolly (Databricks)
- **Innovations**: Human-annotated instruction dataset
- **Key Changes**: 15K human-generated instruction pairs
- **Impact**: First truly open instruction dataset

### OPT (Open Pre-trained Transformer)
- **Innovations**: Open replication of GPT-3 (125M to 175B)
- **Key Changes**: Released logbook documenting training challenges
- **Impact**: Democratized access to large models

## Specialized Coding Models (Additional)

### CodeGen (Salesforce)
- **Innovations**: Multi-turn program synthesis
- **Key Changes**: Conversational code generation
- **Impact**: 16B model, strong on HumanEval

### InCoder (Meta)
- **Innovations**: Causal masking with infilling
- **Key Changes**: Left-to-right and infilling combined
- **Impact**: 6.7B model for IDE integration

### SantaCoder
- **Innovations**: 1.1B model trained on The Stack
- **Key Changes**: Small but efficient code model
- **Impact**: Runs locally, good for code completion

### Replit Code
- **Innovations**: Fine-tuned for Replit IDE
- **Key Changes**: Optimized for web development
- **Impact**: 3B model, fast inference

### Phind-CodeLlama
- **Innovations**: Fine-tuned on high-quality programming data
- **Key Changes**: Better than GPT-4 on HumanEval (at release)
- **Impact**: 34B model, open weights

## Embedding Models (Additional)

### all-MiniLM-L12-v2
- **Innovations**: 12-layer MiniLM variant
- **Key Changes**: Better quality than L6, still efficient
- **Impact**: 33M params, good speed/quality tradeoff

### SGPT (Sentence-GPT)
- **Innovations**: GPT-style models for embeddings
- **Key Changes**: Asymmetric semantic search
- **Impact**: Better for query-document tasks

## Multimodal Models (Additional)

### InstructBLIP
- **Innovations**: Instruction-aware visual feature extraction
- **Key Changes**: Q-Former with instruction conditioning
- **Impact**: Better instruction following for vision tasks

### KOSMOS-1
- **Innovations**: Multimodal LLM with in-context learning
- **Key Changes**: Interleaved text-image training
- **Impact**: Few-shot multimodal tasks

### KOSMOS-2
- **Innovations**: Grounding to object-level understanding
- **Key Changes**: Bounding box annotations in training
- **Impact**: Can generate/understand spatial references

### Otter
- **Innovations**: In-context instruction tuning for multimodal
- **Key Changes**: MIMIC-IT dataset (multimodal instructions)
- **Impact**: Better multimodal in-context learning

### OpenFlamingo
- **Innovations**: Open-source Flamingo reproduction
- **Key Changes**: 3B/9B models, Apache 2.0 license
- **Impact**: Democratized multimodal few-shot learning

### Fuyu-8B (Adept)
- **Innovations**: Simplified architecture (no image encoder)
- **Key Changes**: Treats images as token sequences directly
- **Impact**: Faster, better for UI understanding

## Math & Reasoning Models (Additional)

### MathPrompter
- **Innovations**: Algebraic template-based reasoning
- **Key Changes**: Combines symbolic and neural approaches
- **Impact**: Better mathematical reasoning consistency

### MetaMath
- **Innovations**: Backward reasoning data augmentation
- **Key Changes**: Rephrasing problems from answers
- **Impact**: State-of-art on GSM8K (at release)

## Long Context Models (Additional)

### LongLLaMA
- **Innovations**: Focused Transformer (FoT) mechanism
- **Key Changes**: Contrastive learning for long context
- **Impact**: Extends LLaMA to 256K tokens

### Together Llama-2-7B-32K
- **Innovations**: Positional interpolation scaling
- **Key Changes**: Extended RoPE base frequency
- **Impact**: 32K context with minimal fine-tuning

## Instruction-Tuned Models (Additional)

### Flan-UL2
- **Innovations**: Instruction tuning on UL2
- **Key Changes**: Combines UL2 pretraining with Flan tasks
- **Impact**: Strong zero-shot performance

### Koala
- **Innovations**: Focus on dialogue coherence
- **Key Changes**: Curated web conversations + ShareGPT
- **Impact**: 13B model with good dialogue quality

### Guanaco
- **Innovations**: QLoRA fine-tuning demonstration
- **Key Changes**: 4-bit quantized training
- **Impact**: Shows effectiveness of parameter-efficient tuning

## Small/Efficient Models (Additional)

### MobileLLaMA
- **Innovations**: LLaMA optimized for mobile devices
- **Key Changes**: Structured pruning + quantization
- **Impact**: Runs on smartphones efficiently

### phi-1 / phi-1.5
- **Innovations**: Textbooks Are All You Need approach
- **Key Changes**: Synthetic textbook-quality data
- **Impact**: 1.3B model competitive with 7B models

## Medical/Healthcare Models (Additional)

### BioMedLM (Stanford)
- **Innovations**: 2.7B model trained on PubMed abstracts
- **Key Changes**: Medical domain-specific architecture
- **Impact**: Better than GPT-3 on biomedical tasks

## Legal Models (Additional)

### Pile of Law
- **Innovations**: 256GB legal text corpus
- **Key Changes**: Court opinions, statutes, regulations
- **Impact**: Enables legal LLM development

### Lawyer LLaMA
- **Innovations**: Legal instruction tuning
- **Key Changes**: Legal case analysis and reasoning
- **Impact**: Better legal question answering

## Multilingual Models (Additional)

### XLM (Cross-lingual Language Model)
- **Innovations**: Translation Language Modeling (TLM)
- **Key Changes**: Parallel sentences for cross-lingual training
- **Impact**: Better cross-lingual transfer

### XGLM
- **Innovations**: GPT-style multilingual model (7.5B)
- **Key Changes**: 30 languages, balanced corpus
- **Impact**: Open multilingual alternative to GPT-3

## Audio/Speech Models (Additional)

### HuBERT (Hidden Unit BERT)
- **Innovations**: Self-supervised speech via clustering
- **Key Changes**: Predicts cluster assignments
- **Impact**: Better than Wav2Vec2 on many tasks

### WavLM
- **Innovations**: Unified speech model for understanding + generation
- **Key Changes**: Denoising and dereverberation objectives
- **Impact**: Better robustness to noise

### SpeechT5
- **Innovations**: Unified-modal speech-text pretraining
- **Key Changes**: Shared encoder-decoder for speech/text
- **Impact**: Multitask speech processing

### MusicLM (Google)
- **Innovations**: Text-to-music generation
- **Key Changes**: Hierarchical audio tokenization
- **Impact**: High-quality music from text descriptions

### GPT-4 Realtime
- **Innovations**: Native audio input/output (not ASR+TTS)
- **Key Changes**: Direct audio token processing
- **Impact**: Lower latency, more natural conversations

## Retrieval-Augmented Models (Additional)

### REALM (Retrieval-Augmented LM)
- **Innovations**: End-to-end trainable retrieval
- **Key Changes**: Neural retriever + generator jointly trained
- **Impact**: Pioneered differentiable retrieval

### RAG (Retrieval-Augmented Generation)
- **Innovations**: Pre-trained retriever + generator
- **Key Changes**: DPR + BART combination
- **Impact**: Became standard for knowledge-intensive tasks

### FiD (Fusion-in-Decoder)
- **Innovations**: Encode passages independently, fuse in decoder
- **Key Changes**: Scales to 100+ retrieved passages
- **Impact**: State-of-art on open-domain QA

### Atlas (Meta)
- **Innovations**: Few-shot retrieval-augmented model
- **Key Changes**: Retriever fine-tuning with small data
- **Impact**: Strong few-shot performance

## Research/Experimental Models (Additional)

### MT-NLG (Microsoft-NVIDIA)
- **Innovations**: 530B dense model with 3D parallelism
- **Key Changes**: Megatron + DeepSpeed training
- **Impact**: Showed feasibility of 500B+ models

### PanGu-α (Huawei)
- **Innovations**: Chinese-focused 200B model
- **Key Changes**: Query-layer normalization
- **Impact**: Strong Chinese language performance

### CPM-Bee (OpenBMB)
- **Innovations**: 10B bilingual model with MoE
- **Key Changes**: Live training data updates
- **Impact**: Open Chinese-English bilingual model

## Open Assistant Models (Additional)

### OpenAssistant
- **Innovations**: Fully open RLHF stack
- **Key Changes**: Open conversation dataset (161K)
- **Impact**: First fully open ChatGPT alternative

### Nous-Hermes
- **Innovations**: Fine-tuned on curated instructions (300K)
- **Key Changes**: Focused on reasoning and creativity
- **Impact**: Strong performance on creative tasks

### Nous-Hermes 2
- **Innovations**: DPO training, multi-model variants
- **Key Changes**: Based on Llama-2, Mistral, Mixtral
- **Impact**: Improved instruction following

## StableLM Variants (Additional)

### StableLM Alpha
- **Innovations**: Early open 3B/7B models
- **Key Changes**: Trained on The Pile + RefinedWeb
- **Impact**: Stability AI's entry into LLMs

### StableLM Tuned
- **Innovations**: Instruction tuning on Alpha
- **Key Changes**: RLHF with human feedback
- **Impact**: Better chat capabilities

### StableLM Zephyr
- **Innovations**: DPO training approach
- **Key Changes**: Distillation from larger models
- **Impact**: Efficient alignment without RLHF

## MPT Variants (Additional)

### MPT-7B-Instruct
- **Innovations**: Instruction tuning with Dolly + Anthropic HH
- **Key Changes**: Commercial-friendly license
- **Impact**: Open alternative to Llama for commercial use

### MPT-7B-Chat
- **Innovations**: Conversational fine-tuning
- **Key Changes**: ShareGPT + HC3 datasets
- **Impact**: Better multi-turn conversations

### MPT-30B-Instruct/Chat
- **Innovations**: Scaled-up instruction tuning
- **Key Changes**: 30B parameters, ALiBi encoding
- **Impact**: Competitive with GPT-3

## Yi Models (Detailed)

### Yi-6B / Yi-34B
- **Innovations**: 4K/32K context variants
- **Key Changes**: High-quality Chinese-English corpus
- **Impact**: Strong multilingual performance

### Yi-200K variants
- **Innovations**: Extended to 200K context
- **Key Changes**: Depth-upscaled attention mechanism
- **Impact**: Longest context among open models (at release)

### Yi-VL (Vision-Language)
- **Innovations**: Vision adapter for Yi base models
- **Key Changes**: Dual-encoder architecture
- **Impact**: Strong multimodal Chinese understanding

## Baichuan Models (Detailed)

### Baichuan-7B / 13B
- **Innovations**: 2.6T high-quality Chinese tokens
- **Key Changes**: RoPE with ALiBi hybrid
- **Impact**: Top open Chinese model (at release)

### Baichuan2
- **Innovations**: Improved training data quality
- **Key Changes**: Better multilingual capabilities
- **Impact**: Commercial-friendly license

## InternLM Models (Detailed)

### InternLM-7B / 20B
- **Innovations**: Progressive pretraining strategy
- **Key Changes**: Multi-stage data curriculum
- **Impact**: Shanghai AI Lab's flagship model

### InternLM2
- **Innovations**: GQA, extended 200K context
- **Key Changes**: Better code and math reasoning
- **Impact**: Competitive with Llama-2 70B

## Other Notable Models (Detailed)

### Kimi (Moonshot AI)
- **Innovations**: 200K+ context window
- **Key Changes**: Novel long-context attention mechanism
- **Impact**: Chinese market leader for long documents

### Granite (IBM)
- **Innovations**: Enterprise-focused with governance
- **Key Changes**: Code models (3B-34B) with compliance
- **Impact**: Designed for regulated industries

### Megatron (NVIDIA)
- **Innovations**: 3D parallelism framework
- **Key Changes**: Tensor, pipeline, data parallelism
- **Impact**: Enables training of massive models

### NeMo (NVIDIA)
- **Innovations**: Toolkit for conversational AI
- **Key Changes**: Modular architecture for ASR/NLP/TTS
- **Impact**: Production-ready conversational AI

### Cerebras-GPT
- **Innovations**: Trained on Cerebras wafer-scale chips
- **Key Changes**: 111M to 13B models, all open
- **Impact**: Demonstrated custom hardware efficiency

### GLM-4 (Zhipu AI)
- **Innovations**: Improved GLM architecture
- **Key Changes**: Enhanced multimodal and tool use
- **Impact**: Powers Chinese AI applications

### Ernie Bot (百度/Baidu)
- **Innovations**: Knowledge-enhanced LLM
- **Key Changes**: Integration with Baidu search
- **Impact**: Major Chinese commercial LLM

### Spark (讯飞/iFlytek)
- **Innovations**: Speech-optimized LLM
- **Key Changes**: Native speech understanding
- **Impact**: Strong in voice assistants

### SenseChat (商汤/SenseTime)
- **Innovations**: Multimodal foundation model
- **Key Changes**: Computer vision integration
- **Impact**: Visual reasoning capabilities

### Tongyi Qianwen (通义千问/Alibaba)
- **Innovations**: Integrated with Alibaba ecosystem
- **Key Changes**: E-commerce and business optimization
- **Impact**: Powers Alibaba's AI services

## Additional Alternative Architectures

### Longformer
- **Innovations**: Combines local + global attention
- **Architecture**: Sliding window + global tokens
- **Key Advantage**: O(N) complexity for documents
- **Used in**: Document understanding, legal AI

### BigBird
- **Innovations**: Sparse attention with random, window, global
- **Architecture**: Three types of attention combined
- **Key Advantage**: Linear complexity, provably expressive
- **Impact**: 4096+ token sequences efficiently

### Reformer
- **Innovations**: Locality-sensitive hashing (LSH) attention
- **Architecture**: LSH bucketing for approximate attention
- **Key Advantage**: O(N log N) complexity, reversible layers
- **Impact**: Memory-efficient training

### Synthesizer
- **Innovations**: Learned synthetic attention (no dot product)
- **Architecture**: Fixed or learned attention patterns
- **Key Advantage**: Faster than standard attention
- **Impact**: Shows attention patterns can be learned

### AFT (Attention Free Transformer)
- **Innovations**: Replaces attention with learned positions
- **Architecture**: Position-weighted global context
- **Key Advantage**: O(N) complexity, simpler
- **Impact**: Competitive on vision tasks

### Linear Transformer
- **Innovations**: Kernelized attention with linear complexity
- **Architecture**: Associative matrix operations
- **Key Advantage**: O(N) forward and backward pass
- **Impact**: Enables very long sequences

### Transformer-XL
- **Innovations**: Segment-level recurrence with relative positions
- **Architecture**: Cache previous segment representations
- **Key Advantage**: Longer context without memory explosion
- **Impact**: Improved language modeling

### Universal Transformer
- **Innovations**: Recurrent depth (adaptive computation)
- **Architecture**: Shared weights across layers with ACT
- **Key Advantage**: Variable depth per token
- **Impact**: More expressive than fixed-depth

### Sparse Transformer
- **Innovations**: Factorized sparse attention patterns
- **Architecture**: Strided and fixed attention patterns
- **Key Advantage**: O(N√N) complexity
- **Impact**: Enabled GPT-3 style models at OpenAI

### Perceiver / Perceiver IO
- **Innovations**: Cross-attention to latent bottleneck
- **Architecture**: Fixed-size latent array cross-attends to inputs
- **Key Advantage**: O(M) complexity in latents, arbitrary input
- **Impact**: Handles multi-modal inputs naturally

### CoLT5 (Conditional Computation)
- **Innovations**: Token-level conditional computation
- **Architecture**: Light and heavy branches per token
- **Key Advantage**: Efficient for long contexts
- **Impact**: Better than T5 with less compute

### H-Transformer
- **Innovations**: Hierarchical attention structure
- **Architecture**: Block-level then token-level attention
- **Key Advantage**: O(N√N) complexity
- **Impact**: Efficient for long documents

### Swin Transformer
- **Innovations**: Shifted window attention (for vision)
- **Architecture**: Local windows that shift between layers
- **Key Advantage**: Linear complexity in image size
- **Impact**: State-of-art computer vision

### Vision Transformer (ViT)
- **Innovations**: Pure transformer for images
- **Architecture**: Image patches as tokens
- **Key Advantage**: Scalable, no convolutions needed
- **Impact**: Revolutionized computer vision

### MAE (Masked Autoencoders)
- **Innovations**: High masking ratio (75%) for vision
- **Architecture**: Asymmetric encoder-decoder
- **Key Advantage**: Efficient self-supervised learning
- **Impact**: Competitive with supervised pretraining

### BEiT (BERT for Images)
- **Innovations**: Visual token prediction
- **Architecture**: dVAE tokenizer + BERT-style training
- **Key Advantage**: Better transfer learning
- **Impact**: Strong on downstream vision tasks

### DeiT (Data-efficient ViT)
- **Innovations**: Distillation token for training efficiency
- **Architecture**: ViT with knowledge distillation
- **Key Advantage**: Trains without huge datasets
- **Impact**: Democratized vision transformers

### MLP-Mixer
- **Innovations**: Pure MLP architecture (no attention)
- **Architecture**: Token-mixing and channel-mixing MLPs
- **Key Advantage**: Simple, no attention mechanism
- **Impact**: Showed MLPs can compete with attention

### ResMLP / gMLP
- **Innovations**: MLP with gating mechanisms
- **Architecture**: Spatial gating units
- **Key Advantage**: Simpler than attention
- **Impact**: Competitive on vision benchmarks

### FeedForward Network (FFN) as Associative Memory
- **Innovations**: Interprets FFN as key-value memory
- **Theoretical**: FFN stores knowledge as key-value pairs
- **Impact**: Better understanding of where knowledge is stored

### Mixture of Experts (MoE) - General

### Base4 / Outrageously Large Neural Networks
- **Innovations**: Pioneered sparse MoE at Google
- **Architecture**: Thousands of experts, top-K routing
- **Impact**: Enabled trillion-parameter models

### Expert Choice Routing
- **Innovations**: Experts choose tokens (not tokens choose experts)
- **Architecture**: Reverse routing with capacity
- **Key Advantage**: Better load balancing
- **Used in**: Google's Switch Transformer successors

### Soft MoE
- **Innovations**: Weighted combination of all experts
- **Architecture**: Soft routing weights instead of discrete
- **Key Advantage**: Fully differentiable, no load balancing issues
- **Impact**: Simpler training dynamics
