# [Transformer](https://youtu.be/Ub3GoFaUcds)
Release date : Oct 18, 2025
## Idea
- training strategy : SFT, PEFT, FLAN, RL, RM, RLHF, PPO, DPO
- tasks : NER, PoS, MLM, NSP, MT, QA, NLG
- architecture  :LSTM, GRU, GloVe, BPE, CoT, ToT, SC, RAG
- datasets : MNLI, WNLI, C4, SQUAD, GLUE, MRPC
- metrics : F1, PPL, ROUGE, BLEU, METEOR, LaaJ, WER

## Details
### NLP overview
- Sentiment extraction, Intent detection, Language , Topic modeling
- Part of speech tagging, Named entity recognition, Dependency parsing, Constituency parsing
- Machine translation, Question answering, Summarization, Text generation
- Dataset for NER : annotated reuters newspaper
- For translation : WMT'14 ; eval : BLUE, ROuge, Perplexity

### tokenization
- converting lettrs into numbers
- sentence can be broken down into smaller parts which can be resued to represent them in a meaningful way
- just by space is not that useful
    - since one word might mean different and different words might mean almost the same
        - run == runs, bank!=bank
- subword : using ##, so that the words that mean, at the root level, the same are broken in a meaningful way
    -  cons : the sequence will be longer, increaing processing complexity
    - BPE : byte pair encoding  
    - Wordpiece 
    - Leverages common prefixes and suffixes, Learned from the data
    - lesser risk of out of vocabulary word

### Word/Token representations
- one hot will make things bad, as all the vectors will be orthogonal and it will be og very high dimensionality
    - also, it will lose the nuances
- learned embedding is better as it can use cosine similarity
    - it started with word2vec : cbow and skip gram (proxy task)

### RNN

## Resource


## misc
00:09:40 NLP overview
00:22:57 Tokenization
00:30:28 Word representation
00:53:23 Recurrent neural networks
01:06:47 Self-attention mechanism
01:13:53 Transformer architecture
01:29:53 Detailed example
---

