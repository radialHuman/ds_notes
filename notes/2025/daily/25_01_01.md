## [KL Divergence - CLEARLY EXPLAINED!](https://youtu.be/9_eZHt2qJs4)
Release date : Jan 26, 2021
### Idea #llm
- Quantifies difference between 2 prob dist
- for variational inference
- represent complex dist in a simple manner

### Details
- Doesn require teh 2 prob to be from the same distribution
- log likelhood is used instead of prob as it can diminish
- by finding the difference between the log likelihood similarity ca be found
    - which can be also written as log of the ratio of thier probabilty : log likelihood ratio
- To find the difference betweent he two distributions, on an average
    - either integrate or differentiate it
- this can be complutationally expensive if the limit is infinity
- This can be overcome by yusing Law of large numbers
- instead of summing in case of discreet and continuous distribution, it can be taken as arthemetic mean
- its asymetric and hence not a distance but a divergence
- first dist/secndond dist = forward kl
- second/first = reverse kl

### Resource


### misc

---

## [Fine Tune Qwen 2 VL With Your Data](https://youtu.be/tTxrPWJqSw4)
Release date : Sep 9, 2024
### Idea #finetune
- Fine tune vision llm using own data set and Swift

### Details
- swift is modelscope's library
- install pyav and qwen_vl_utils
- can be done via shell script too
- sft parameters
```sh
# sh examples/custom/sft.sh
CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --custom_register_path examples/custom/dataset.py \
                           examples/custom/model.py \
    --model AI-ModelScope/Nemotron-Mini-4B-Instruct \
    --train_type lora \
    --dataset swift/stsb \
    --num_train_epochs 3 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 16 \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4 \
    --max_length 2048 \
    --output_dir output
```

### Resource
https://github.com/modelscope/ms-swift
https://huggingface.co/datasets/linxy/LaTeX_OCR

### misc

---

## [Optimizing Vector Databases With Indexing Strategies](https://youtu.be/gM_7DbppaaI)
Release date : Jul 5, 2024  
### Idea #VDB
- Scaling will not be good if brute force is used for searchign embeddings
- Flat indexing is not sueful then

### Details
- Distributed architecture can be used
- But for throughput and latency this is not enough
- Moving from exact search to aprox search will give the speed and compromise on accuracy a bit
- Invert File
    - In the VDB, run k means to find mediods
    - when query comes, find distance between the mediods and query to find the nearest
    - then only search in that cluster
    - this gives speed as useless ones are ignored
- PQ : Product quantization also can be done
- HNSW is the most commonly used one
    - using skipped linked list and graph like connecttion between the vectors in layers
- Tree based and hashing base dare also there for searching
    - LSH ???
- its all a trade off

### Resource


### misc

---

## Learnable parts of a transformer #llm

When pretraining a transformer model, several components are learnable, meaning their parameters are adjusted during the training process. These learnable parts enable the transformer to capture intricate language patterns and generate coherent text. Here's an exhaustive list of techniques and components involved:

**1. Token Embeddings:**

These are vector representations of individual tokens in the model's vocabulary. During pretraining, the model learns to associate each token with a meaningful vector that captures its semantic and syntactic properties. The sources explain that this is done using an embedding layer, which acts as a lookup table that maps each token ID to its corresponding embedding vector. 

**2. Positional Embeddings:**

Transformers process text sequences in parallel, lacking inherent awareness of token order. Positional embeddings are added to token embeddings to provide information about the position of each token in the sequence. This allows the model to understand the sequential nature of language. The sources mention that positional embeddings can be either learned during training or generated using predefined functions.

**3. Multi-Head Attention Mechanism:**

This mechanism allows the model to attend to different parts of the input sequence and weigh their importance in relation to each other when generating the output. Each attention head focuses on capturing different aspects of relationships between tokens, enabling the model to process information from multiple perspectives simultaneously. All the parameters within the attention mechanism, including the query, key, and value matrices, are learned during pretraining.

**4. Feedforward Networks:**

These networks, present in each transformer block, apply non-linear transformations to the output of the attention mechanism, further processing the information and enabling the model to capture complex relationships. The weights and biases of these feedforward networks are adjusted during pretraining.

**5. Layer Normalization:**

This technique is applied to stabilize training by normalizing the activations within each layer. The parameters of the normalization layers are also learned.

**6. Output Layer:**

In a language modeling scenario, the output layer is typically a linear layer that predicts the probability of the next token in the sequence, given the processed input. This layer's weights are adjusted to minimize the prediction error during pretraining. 

### **Additional Considerations for Pretraining:**

*   **Choice of Tokenization Scheme:**  The way text is tokenized significantly impacts the model's ability to learn and represent language. Techniques like Byte Pair Encoding (BPE) are commonly used to handle unknown words and create a vocabulary that efficiently captures language patterns. The sources emphasize that BPE can break down unknown words into subwords or individual characters, enabling the tokenizer and the LLM to process any text.
*   **Data Sampling Strategy:** The process of creating input-output pairs from the training data influences what the model learns. The sliding window approach, where segments of text are used to predict the following token, is commonly employed.
*   **Optimization Algorithm:** The algorithm used to update the model's parameters during training, such as Adam or SGD, also plays a crucial role in the effectiveness of pretraining.
*   **Hyperparameter Tuning:** Various hyperparameters, such as the learning rate, batch size, and number of training epochs, need to be carefully tuned to optimize the pretraining process. For instance, the sources note that a learning rate warmup and cosine annealing can be used to further optimize the training loop.

By effectively learning these components and fine-tuning the training process, transformers acquire a robust understanding of language, laying the foundation for various downstream tasks like text generation, translation, and question answering. 
