## [TabPFN: Deep Learning for Tabular Data is Here! (Prof. Frank Hutter explains)](https://youtu.be/MKu09syGgEk)
Release date : Feb 19, 2025
### Idea #DL
- TabPFN : tabular prior data fitted netwrok
- table already has feature but NN tries to understand the features

### Details
- tries to understand the column names and realtions
- does ICL using transformers
- 

### Resource
https://github.com/PriorLabs/TabPFN

### misc

---

## [COCONUT: Chain of Continuous Thought](https://youtu.be/BaTjJJsz0rY)
Release date : 20/02/25
### Idea #TTC
- Resoning in latent space like test time compute
- scaling power of llm during inference
    - continuous cot
    - recurrent depth model
- During the thinking process, it need not think in tokens. It might discover a scaling axis
- In LRM : large resoning models

### Details
- thinking logically in the embedding space with numbers, can find hidden ways fo thinking
- unlike resoning in the space of tokens and words 
- since tokens is for us to understand and model needs just numbers, it can be tokenless resoning
    - This might allow us to scale
- i.e. thinkgin before we generate the tokens might get us better answers
- Scaling history
    - large models : not focusing on this now
    - more pre-trainign data : only so much data in the net
    - Post trainign : RL and RLHF : this is in focus and growing
- COCONUT
    - instead of generating tokens, it loops back witht he results to the starting and proecess the embeddings again.
    - this is sued as thinking process with chain of thoughts. 
- Recurrent Depth approach
    - Is a translformer architecture is built upon a latent dept recurrent block
    - that randomly samples # of iterations while training
    - Each block consists of a number of sub-layers.
        - The blue prelude block embeds the inputs into latent space
        - where the green shared recurrent blocks is a block of that is repeated to compute the final latent state,
        - which is decoded by the layers of the red coda block.
    - This setup is repeated multiple times
    - It becomes more efficient when it know so a given input it doesn have to spend more time than required to think
- Difference between these researches:
    - RD is architecture change : budget forcing ???
        - fc=ocuses on what to think and how to think
    - Coconut is adappting to the existing architecture by adding somehting :cot inside mode
        - doesn allow token generation but keeps looping the embeddinggs to think
### Resource
- https://github.com/facebookresearch/coconut
    - hack normal transformer from preventing it to generate directly
- https://github.com/seal-rg/recurrent-pretraining
    - https://huggingface.co/tomg-group-umd/huginn-0125
    - https://www.arxiv.org/abs/2502.05171
    - https://colab.research.google.com/drive/14gccwzJsr7yoHjV3aTp7efN4aNbR6thy?usp=sharing

### misc

---

## []()
Release date : 
### Idea
- 

### Details
- 

### Resource


### misc

---

