https://arxiv.org/pdf/2411.17116 - Star attention
https://arxiv.org/abs/2411.15594 - Survey on LLM-as-a-Judge 
https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space - Large concept model
https://arxiv.org/abs/2412.13663 - modernbert
https://arxiv.org/abs/2412.14352 - https://github.com/dongxiangjue/Awesome-LLM-Self-Improvement 
https://arxiv.org/pdf/2412.17498 - Optimized Deep Reasoning Translation via Long Chain-of-Thought
https://arxiv.org/pdf/2412.10136 - Can LLMs Convert Graphs to Text-Attributed Graphs?
https://arxiv.org/pdf/2412.06769 - Training Large Language Models to Reason in a Continuous Latent Space
https://arxiv.org/pdf/2412.07017 - ASYNCHRONOUS LLM FUNCTION CALLING
https://arxiv.org/pdf/2412.05579 - LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods
https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/ - byte latent Transformer: Patches Scale Better Than Tokens 
https://arxiv.org/pdf/2411.19865 - Reverse Thinking Makes LLMs Stronger Reasoners
https://arxiv.org/pdf/2412.00722 - Towards Adaptive Mechanism Activation in Language Agent
https://arxiv.org/pdf/2411.19443 - AutoRag
https://arxiv.org/pdf/2412.02830 - RARE (RetrievalAugmented Reasoning Enhancement)
https://arxiv.org/pdf/2411.00640 - Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations
https://arxiv.org/pdf/2411.14405 - Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions
https://arxiv.org/html/2411.10541v1 - Does Prompt Formatting Have Any Impact on LLM Performance?
https://arxiv.org/pdf/2411.11843 - Bi-Mamba: Towards Accurate 1-Bit State Space Models
https://arxiv.org/pdf/2411.04905 - OpenCoder
https://arxiv.org/pdf/2411.07396 - Toward Optimal Search and Retrieval for RAG
https://arxiv.org/pdf/2411.07494 - RAPID RESPONSE: MITIGATING LLM JAILBREAKS WITH A FEW EXAMPLES
https://arxiv.org/pdf/2411.04996 - Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models
https://arxiv.org/pdf/2411.02830 - Mixtures of In-Context Learners
https://arxiv.org/pdf/2501.00332 - Main-Rag
https://arxiv.org/pdf/2501.06252v2 - transx2
https://arxiv.org/pdf/2501.06425v1 - tensor is all u need
https://arxiv.org/pdf/2501.06713v2 - Mini Rag