## [The True Power of "ChainMap" in Python](https://youtu.be/37SpumiGHgE)
Release date : Dec 21, 2024
### Idea
- from collections import ChainMap

### Details
- combines multiple dictionaries allowing duplicates
- while | operator doesn allow duplicate keys
- but operates only on the first dict
- useful in case of user preference and default is combines

### Resource
- 

### misc
 
---

## [The BEST Way to Chunk Text for RAG](https://youtu.be/Pk2BeaGbcTE)
Release date :  Dec 9, 2024 
### Idea
03:47 - Character Text Splitting
06:28 - Token Text Splitting
10:26 - Recursive Character/Token Splitting
16:07 - Kamradt & Modified Semantic Chunking
20:43 - Cluster Semantic Chunking
24:46 - LLM Semantic Chunking

### Details
- Character : simple length, can be with overlap
- Token based : same thing but split at token/sub word level
- Recurssive character : since we write in a way that has some sort of seperation like paragraphs, 
    - first it splits the doc into paras
    - then if the para is longer than the window, it will use diffrent ways to break it down on technique at a time like
    - line break > . , ? ! > space
    - langchain doesn use them all, while chroma uses more such splitters
    - using this actual paras remain intact unless its too large then it gets broken down
    - might violate the limit of chunk size if required depending on the para lenght
    - can have overlap
- Recurssive token : same but on token splitting  
- Kamradt SemanticChunker : using embedding models to compare various chunks, better semantic boundries can be found
    - first chunker splits in fixed length
    - then surrounding context of each chunks using the segemnts before and after
    - then cosime similarity is calculated for these segments
    - if the distance is too much between them then it s different topic
    - uses sliding window to find near by segments
    - in chromadb : it also involves binary search,to control the chnck sizes
- Cluster semantic chuker : the previous one uses local segemnst to understand the context
    - used in chromadb
    - compares all the chunks to be compared and finds related ones not just near ones
    - something like raptor without repetition
- LLM for chunking
    - in chroma
    - first recurssive split with a wrapper which has a label
    - llm using prompt will tell where to split
```llm
"You are an assistant specialized in splitting text into thematically consistent sections. "
"The text has been divided into chunks, each ma with <|start_chunk_X|> and <|jend_chunk_X|> tags, where X is the chunk number. "
"Your task is to identify the points where splits should occur, such that consecutive chunks of similar themes stay together. "
"Respond with a list of chunk IDs where you believe a split should be made. For example, if chunks 1 and 2 belong together but chunk 3 starts a new topic, you would suggest a split after chunk 2. THE CHUNKS MUST BE IN ASCENDING ORDER."
"Your response should be in the form: 'split_after: 3, 5°."
```
- Comparision
    - Recurssive character 200-400 size

### Resource
- https://github.com/ALucek/chunking-strategies
- https://research.trychroma.com/evaluating-chunking
- https://github.com/brandonstarxel/chunking_evaluation
- https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb

### misc
 
---

## [Navigating the Vector Database Landscape](https://youtu.be/SQvABMAgGHo)
Release date : Mar 28, 2024 
### Idea
- 

### Details
- latency, cost, operational complexity and over head, throughput, ease of use
- serverless option in pinecone now
- more strategy and startup details than technical

### Resource
- 

### misc
 
---

## [Anthropic’s STUNNING New Jailbreak - Cracks EVERY Frontier Model](https://youtu.be/LGHaMcP_flA)
Release date :  Dec 20, 2024
### Idea
- making llm do somethign they are not supposed to by reptedly asking them using slight modification to the prompt
- BoN/short gunning

### Details
- best-of-N jail breaking
- works on audio and video input too
- case change, text in image, speed pitch variotion
- ASR : attack success rate follows power-law-like-scale

### Resource
- https://arxiv.org/html/2412.03556v1
- https://github.com/jplhughes/bon-jailbreaking

### misc
 
---

## [Practical RAG - Choosing the Right Embedding Model, Chunking Strategy, and More](https://youtu.be/j1XRLh7yzzY)
Release date :  Nov 10, 2023 
### Idea
- 

### Details
- VDBs
    - for unstrctured data, no data model
    - image videos text 
    - indexing and search
    - convert data into meaning ful vectors
    - closer vectors to the input comparision
    - zilliz VDB
- Rag
    - embeddings
        - use a fine tuned ones not the default one
        - recommendation :  gte and E5
    - simialrity search
    - vdb
        - Indexing 
            - hash based : Falconn, 
            - Tree based : ANNOY
            - Inverted file based : FAISS
            - Graph based : HNSW 
            - RAFT ???
        - 100% recall/accuracy Brute-force index (FLAT)
        - Approximate search:
            - 10MB - 2GB Inverted-file index (IVF)
            - 2GB - 20GB Graph-based index (HNSW)
            - 20GB - 200GB Hybrid index (HNSW_SQ, IVF_PQ)
            - 200GB+ Disk index (DiskANN)
    - llm
        - goal of the model, task
        - latency
        - cost
        - quantization
        - flash attentions
    - Chunking
        - context in important
        - AutomergingRetriver - llama index
    - Beyond RAG for VDB
        - Image/video simialrity
        - Audio smilarity
        - molecular similarity
        - text search engine
        - dna seq classification
        - QNA
        - Recommender
        - Anomaly 

### Resource
- 

### misc
 
---

## [NEW Transformer for RAG: ModernBERT](https://youtu.be/Z1Dl3juwtSU)
Release date : Dec 20, 2024  
### Idea
- bert is masked bi directional
- gpt : causal uni directional

### Details
- ModernBert
    - RoPE
    - GeGLE activation function
    - local-global attention layers (also in bi latent transformer by meta)
    - only english
    - hardware optimization
    - flash attention 2/3

### Resource
- https://arxiv.org/pdf/2412.13663 paper
- https://github.com/AnswerDotAI/ModernBERT
- https://arxiv.org/pdf/2402.00838 embedding technique paper
- https://huggingface.co/blog/modernbert

### misc
 
---

## [Deep Dive: Optimizing LLM inference](https://youtu.be/hMs8VNRy5Ys)
Release date : Mar 11, 2024  
### Idea
- KV Cache
- continuous batching for better throughput
- speculative decoding

### Details
- #TODO

### Resource
- 

### misc
 
---

 