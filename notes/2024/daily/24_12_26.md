## [Llama-Chunk: RAG with Llama](https://youtu.be/S2BKn0fAfPA)
Release date : Dec 25, 2024  
### Idea #chunking
- using llm to create good chunks using spl character 

### Details
- for a large amount fo document
- prompt based chunck
- based on log probability
- finds this using likelihood of finding chuncks using 70B llama

### Resource
- 

### misc
 
---

## [How-To Fine-Tune Any Vision Language Model on Your Own Custom Dataset Locally](https://youtu.be/Ctb29UxzIS8)
Release date : Dec 25, 2024
### Idea #finetune
- HF released this and has a notebook 

### Details
- using qlora

### Resource
- https://colab.research.google.com/github/huggingface/cookbook/blob/main/notebooks/en/fine_tuning_vlm_trl.ipynb
- https://huggingface.co/datasets/HuggingFaceM4/ChartQA

### misc
 
---

## [Why AI Agents Don't Really Automate Much of Anything: Your Shortcut to Managing a Machine](https://youtu.be/lboawlmdnFk)
Release date : Dec 21, 2024
### Idea #agentcheck
- why agents might be more about managing the machines 

### Details
1. Automation with trigger : does some basic helpful task
2. capable bot : same but with a tool/capability to use to get some task done
3. Step complition : breaksdown the task into multiple steps | uses llm/task and then combines the results and another llm sums it up (might involve CoT, in paralle of sequence)
4. Agents instead of llms , in the above step if llms are replaced with agents like a manager, specialized agents and organizer
5. Looped : if there is loop between the task splitting manager and result collecting organizer to evaluuate the output and improve
- things like these can have a snow ball effect for error
- needs perfect agents to make it more accurate
- There must be some sort of reality check at each point to see if we are moving towards the common end goal
- may be add a checker at each point
- they are good at being creative if not controlled
- needs human in the loop for maintaining sanity
- best outcome as per the author

### Resource
- https://seanblanchfield.com/2024/08/many-meanings-of-agent

### misc
00:00 Introduction to AI Agents
00:41 Understanding Basic AI Automation
01:35 Exploring Capable Bots
02:30 Step Completion and Chain of Thought
03:58 The Role of AI Managers
05:49 Accuracy Challenges in AI Systems
12:33 Human-AI Collaboration
14:40 Demo: Monster Creator
---

## [ImageBind paper explained: One Embedding Space To Bind Them All (from Meta AI)](https://youtu.be/cfU_QPNcl2U)
Release date : May 25, 2023
### Idea #vision #multimodal
- human mind combines image text video audio touch to think of something 
- such aligned combination fo data is difficult to find online
- Goal : emergence of alignement between modality

### Details
- Clip algns images and corresponsing text using 2 encoders to learn the semenatic embeddings
    - this used positive and negative paring
    - much better than predictive way of doing
    - using info mc loss
- similarly to align audio and image. there was AudioCLip
- for depth it was contrastive multiwave coding
- Audiovisual instance discrimination with cross-modal agreement (audio and video)-
- The main issue is the embeddings are not interchangeable
- because they are individual paring and not all in one
- ImageBind
    - combines A,V,T,I, Depth, Thermal, IMU (accelerometer, gyroscope data)
    - single mebdding for all with image being the one that binds them all together
    - first for each modality encoders are used to get embeddings
    - Vision Transformer (ViT) as base
        ViT-H --> Images/Videos
        OpenCLIP --> Text
        ViT-B --> Audio
        ViT-S --> Thermal
    - before encoding them, they are prerpocessed
        - video :2frames/2seconds
        - audio : 2 sec audio to mel log spectograms
        - Thermal and depth : 1 channel images
        - for IMu : 5 sec clip converts 3d to 1 d convolution
    - Image, text and video encoder (pre-trained) weights are fixed and thes rest are updated based on loss function
    - once they are passed thorugh respective econders, they are passed through a liner layer to make sure the lenght of each embedding is the same
    - loss fucntion sued would be InfoNCE Loss, modified cross entropy loss
        - it extends the idea of contrastive elanring to multimodalities
        - where nuemerator is similarity 
        - denominator is dot product of negative cases
        - and a negative log of this ratio is the overall loss funciton
    - dataset for trainign
        - Video + Audio --> Audioset dataset
        - Image + Depth --> SUN RGB-D
        - dataset Image + IMU --> Ego4D dataset
        - Image + Thermal --> LLVIP dataset
    - embedding space arthematic across modals can be done (king - queen = man like)
    - can be used for object detection using prompts

### Resource
- https://arxiv.org/pdf/2305.05665.pdf

### misc
 0:00 - Intro 
1:39 - CLIP and motivation for ImageBind (Linking Modalities)
3:04 - AudioClip and similar works 
3:52 - ImageBind and Multiple Modalities
5:58 - Preprocessing 
6:48 - InfoNCE loss 
7:30 - InfoNCE Loss explained
9:46 - Results 
---

## [2 Methods For Improving Retrieval in RAG](https://youtu.be/smGbeghV1JE)
Release date : Dec 19, 2024 
### Idea #RAG
- recall is improtant for retrival
- improving techniques

### Details
- Vector search based on similarity might not work in case of exact matches so the output might be incorrect
- BM25 instead is not much better as it is based on frequency based key word search
    - lacks context, if the key word apperas multiple times but in someother context it will still be selected
    - language related conjugation issues where one work can be spelled differently in different context, but still might be relevant
- metadata was added which increased the performance
    - it was extracted using llm so a bit more expensive else do it manually
    - also one extra llm call so extra latency
- corner case was still left
- Look for actuall issues particular to the use case and type of search and work on it rather than use tools which are in fashion

### Resource
- 

### misc
 
---

## [RAG to Riches: Building Enterprise-Grade AI Systems with Elasticsearch by Hamza Tahir, ZenML](https://youtu.be/fhWKZaTL_w8)
Release date : Dec 19, 2024  
### Idea #RAG
- Rag for enterprise

### Details
- Challenges
    - Vendor Lock-in Risk
    - Regulatory Compliance
    - Security & Governance
    - Quality & Reliability : hallucination + data leakage
    - Iteration Speed Key Challenge
- Data Flywheel
    - rag is a data pipeline at the core
    - betetr indexing based on type fo data
    - better chunking
    - reranking
    - metadata enchancement
    - multi doc handelling
    - quantitative and qualitative metrics
    - semantic+ keyword = hybrid
    - Fine tune mebeddings #doubt
    - Fine tune model : domain specific, distillation,
- add a feedback loop based on feedback
- 

### Resource
- 

### misc
 
---

## [3 Ways to Quantize Llama 3.1 With Minimal Accuracy Loss](https://youtu.be/xpdWFMsJzVY)
Release date : Sep 6, 2024
### Idea #finetuning
- llama 3.1 quatizing using 4 methods
- to reduce the size of model by reducing decimal values of parameters

### Details
- loss of accuracy can be a side effect
- needs infra for this
- finetuning needs data
- AutoRound
    - auto-round
    - 16 to 4 bit
    - (sym) there is symmetrci and assymetric quantization #doubt
- GPTQ
    - auto-gptq
- Bitsandbytes
    - bitsandbytesconfig
    - bitsandbytes
- AWQ
    - autoawq optimum
- mesure the performance of each type

### Resource
- https://colab.research.google.com/drive/1WoKGDXf5LKDzudiKJebubDYo2f6qBemv?usp=sharing 

### misc
 
---

## [LLM Chronicles #6.3: Multi-Modal LLMs for Image, Sound and Video](https://youtu.be/_sGwL6RAsUc)
Release date : Jul 1, 2024
### Idea
-  #multimodal


### Details
- #TODO

### Resource
- Vision transformer: https://arxiv.org/pdf/2010.11929
- Survey of multi modal LLMs: https://arxiv.org/pdf/2306.13549 
- Microsoft's CLAP: https://arxiv.org/pdf/2206.04769 
- SigLip: https://arxiv.org/pdf/2303.15343

### misc
 01:32 - MLLM Architecture
03:49 - Training MLLMs
07:02 - Vision Transformer
09:24 - Contrastive Learning (CLIP, SigLIP)
12:35 - Lab: PaliGemma
22:53 - Summary

---

## [Coding a Multimodal (Vision) Language Model from scratch in PyTorch with full explanation](https://youtu.be/vAmKB7iPkWw)
Release date : Aug 7, 2024
### Idea
- #multimodal
- We will be coding the PaliGemma Vision Language Model from scratch while explaining all the concepts behind it:
    - Transformer model (Embeddings, Positional Encoding, Multi-Head Attention, Feed Forward Layer, Logits, Softmax)
    - Vision Transformer model
    - Contrastive learning (CLIP, SigLip)
    - Numerical stability of the Softmax and the Cross Entropy Loss
    - Rotary Positional Embedding
    - Multi-Head Attention
    - Grouped Query Attention
    - Normalization layers (Batch, Layer and RMS)
    - KV-Cache (prefilling and token generation)
    - Attention masks (causal and non-causal)
    - Weight tying
    - Top-P Sampling and Temperature

### Details
- Parts of poli gemma
    - Contrastive vision encoder
    - linear projecttion
    - Decoder LLM 
    - combine image and text embeddings
- #TODO

### Resource
- https://github.com/hkproj/pytorch-paligemma
- https://www.youtube.com/watch?v=bCz4OMemCcA (attention for trx)

### misc
00:00:00 - Introduction
00:05:52 - Contrastive Learning and CLIP
00:16:50 - Numerical stability of the Softmax
00:23:00 - SigLip
00:26:30 - Why a Contrastive Vision Encoder?
00:29:13 - Vision Transformer
00:35:38 - Coding SigLip
00:54:25 - Batch Normalization, Layer Normalization
01:05:28 - Coding SigLip (Encoder)
01:16:12 - Coding SigLip (FFN)
01:20:45 - Multi-Head Attention (Coding + Explanation)
02:15:40 - Coding SigLip
02:18:30 - PaliGemma Architecture review
02:21:19 - PaliGemma input processor
02:40:56 - Coding Gemma
02:43:44 - Weight tying
02:46:20 - Coding Gemma
03:08:54 - KV-Cache (Explanation)
03:33:35 - Coding Gemma
03:52:05 - Image features projection
03:53:17 - Coding Gemma
04:02:45 - RMS Normalization
04:09:50 - Gemma Decoder Layer
04:12:44 - Gemma FFN (MLP)
04:16:02 - Multi-Head Attention (Coding)
04:18:30 - Grouped Query Attention
04:38:35 - Multi-Head Attention (Coding)
04:43:26 - KV-Cache (Coding)
04:47:44 - Multi-Head Attention (Coding)
04:56:00 - Rotary Positional Embedding
05:23:40 - Inference code
05:32:50 - Top-P Sampling
05:40:40 - Inference code
05:43:40 - Conclusion
---

