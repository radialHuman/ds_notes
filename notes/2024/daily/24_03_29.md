## [Jamba - A Groundbreaking SSM + Transformer Open Model](https://youtu.be/5Ovt6lRXMWA)
Release date : 
### Idea
- State space models + Transformers
- Prod grade mamba model with MoE transformers

### Details
- Open like mistral
- 52B with 12 B active prama
- 16 experts with 2 active
- 256K context length
```python
!pip install -qqq transformers>=4.39.0 mamba-ssm causal-conv1d>+1.2.0 accelerate bitsandbytes --progress-bar off
!pip install flash-attn --no-build-isolation

import torch from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
# Load model in 8-bit precision

quantization_config = BitsAndBytesConfig( load_in_8bit=True, llm_int8_skip_modules=["mamba" ]
) 
model = AutoModelForCausalLM.from_pretrained(
    "ai21labs/Jamba-ve.1"
    trust_remote_code=True
    torch_dtype=torch.bfloati6
    attn_implementation="flash_attention_2"
    quantization_config=quantization_config
) 
tokenizer = AutoTokenizer.from_pretrained("ai21labs/Jamba-v0.1")


# Tokenize input
prompt = """George wants to warm his hands quickly"""
input_ids = tokenizer(
prompt, 
return_tensors='pt'
).to(model.device)["input_ids"]
# Generate answer
outputs = model.generate(input_ids, max_new_tokens=216)
# Print
    output print(tokenizer.batch_decode(outputs))
```

### Resource
- [site](https://www.ai21.com/blog/announcing-jamba)
- [HF](https://huggingface.co/ai21labs/Jamba-v0.1)

### misc
 
---
