## [Karan Dalal - Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://youtu.be/FsflifJAWdc)
Release date : Aug 20, 2024
### Idea #TTT #RNN
- Need for long context for a real life situation where text images are too much to handle
- Transformers quadratic complexity : cost of genrating next token depedns on on far it is from the first token generated 
- RNNS : are linear complexity and irrespective of the token lcoation, the cost is constant
- LSTM lost because it was not able to improve loss after 100 tokens compared to trx (2020 scaling laws of Neural LM)
    - doesn scale well in terms of model size and taking advantage of long context
### Details
- RNNs are bad when it comes to long context because
    - They have hidden state between input and output layer
    - The hidden state are of fixed size
    - information needs to be compressed into the fixed size hidden state
- TRX also does the same thing but uses KV caching which can grow with seq length and is not fixed
    - acts like a lookup table
- How does RNN get over the issue of ocmpressing millions of token in fixed lenght
    - compress the tokens into hidden states using model weights (like in a NN)
    - context is the datset and NN gets trained on it and replaces the hidden state
    - hidden states are used as ML models
    - This ML model in the paper is MLP and a linear layer
    - Instead of using hidden state as just memoery it now uses learning using ML model and updating the weights
- This TTT is a replacement for attention layer in the architecture
    - In this TTT layer, 
        - Inner loop : there is a seq-to-seq process which does GD on context, using learned reconstruction loss 
        - This happens all the time vene during inference
        - Hence learning at test time
    - During pre-training (outer loop)
        - Uses std cross entropy loss or thats generaly used
        - This has its own gradient
    - since there is grad of grad, there is an element of learning to learn going on
- For this we need self-supervised in the hidden state to learn the context
    - Navie reconsturction loss :  
        - A token is taken and nosie is added to it
        - this has only key , not query or value
        - this is learnt in the outer loop
        - then using label of V we compare the noisy token 
        - the token is recontructed in the hidden state
        - then the loss is taken on the label
        - This is claled multi-view resconruction
        - after this the Q is used in the test view to reforward to the model
    - The inner loop learns and the outer loop controls the learning as per the downstream tasks
- Conparing perpexity vs flops TTT-with linear in hidden state scales better than TRX
- But while comparing inference time speed, TTT out performs TRX
    - due to linear completixity and the model size remains same unlike KV caching
    - TTT has paralel mini batch
    - GD's matrix multipleication is done in a way thats faster on GPU when compared to mamba
- Aim : use ML for human learning
    - TRX learns from a bunch of images, while humans learn from constant input of multimodal inputs
    - Human learning calidation data set is very different from ML, as its general knowledge

### Resource


### misc

---

## [Inference Time Compute](https://youtu.be/_Bw5o55SRL8)
Release date : Dec 3, 2024
### Idea #ITC #important
- O1 has implemented this 
- to have llm thinking slow before answering
- It thinks for a long time before giving the final answer
    - uses a lot of tokens doing that
- Does step-by-step reasoning using process reward model
- Does self-reflection in a trail and error loop
- STAR : scaling thinkgin and bootstraping (trainign time)

### Details
1. Make LLm think
    - o1 uses long internal COT 
    - COT : give examples of how to break down a problem into sub taks and how to execute them
        ```prompt
        Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?
        A: ROger started with  5 balls. 2 cans of 3 tennis balls and each is 6 6 tennis ball. 5+6=11.  The answer is 11.
        Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?
        ```
        - immitating human reasoning process
        - Theoretically, it adds "recurrence" computation back to transformer, extending expressivity of Transformer to solve inherently serial problems.
        - Good for maths, code eval, simulation etc
        - Not good for Linear programming, grpah connectivity and more
        - Trainig for COT is done by asking llm for reasnoning and comrping it with actual way of arrving at the answer
        - STaR : Bootstrapping reasining with reasoninig
2. Step by step reasioning
    - We should provide procedural supervision, and this can be implemented by using a process reward model (PRM).
    1. OpenAI : high quality manual Anotation PRM800K
        - Through reinforcement learning, 01 learns to hone its chain of thought and refine the strategies it uses.
        - It learns to recognize and correct its mistakes.
        - It learns to break down tricky steps into simpler ones.
        - It learns to try a different approach when the current one isn't working.
        - This process dramatically improves the model' ability to reason 
        - Procedure : 
            - GPT4 is gone through math trianing for foloowing format tuning
            - PRM data set is collected in a certain format which has feedback for reasoning steps
            - label by human for each step is (+ 0 -)
            - PRM is done #doubt
            - The it provides resoning trajectories which is then human evaluated 
    2. Math Shepard , Deepseek and others : Automatic annotation
        - Since PRM method above is relatively expensive and labourous
        - hueristic is used to annontate
        - Multiple reaosning trajectories are taken and the right one is rewarded #doubt
        - then trianign si done using varied cross entropy
3. Self reflect and backtracking
    - Through reinforcement learning, 01 learns to hone its chain of thought and refine the strategies it uses
    - It learns to recognize and correct its mistakes.
    - It learns to break down tricky steps into simpler ones.
    - It learns to try a different approach when the current one isn't working.
    - This process dramatically improves the model's ability to reason.
    - Process 
        - Three types of tasks :  decision making, programming, reasoning
        - tracked across : task -> trajectory -> eval -> reflection -> next trajectory
            - eavl bith internal and external using  : rule/heuristic or self-generated unit test functions and binary reward
    - Limitation
        - Cant do it withotu external feedback
    - Teach LLM to self-reflect/self-correct
        - incentive should be right for trainign
        - Problem 1 (Distribution Shift): Actual mistakes may deviate from example (SFT -> Online RL)
        - Problem 2 (Behavior Collapse): Learns to produce best first-attempt response + superficial edits
    - Teach backtracking : Stream-of-search
        - By having multiple trajectory and linearizing them to backtrack
        - From searhc tree (DFS/BFS)  to Stream #doubt
    - Teach backtracking : Searchformers #doubt
        - DFS bFS are elss efficient
4. Inference time scaling laws
    - o1 performance smoothly improves with both train-time and test-time compute
    - Strategies that can be scaled     
        1. Best-of-N : sample N solution for LLM, selecting the best one with verifier 
        2. Beam search with PRM :  select top n samples at each step using PRM
        3. Lookahead search with PRM : at each step of Beam Search, rollout K-steps in advance, using PRM for these K-step rollout to represent the value for the current step.  
        -  
    - Underestimated Inference-Time Scaling Power
- Inference-Time Scaling unlocks much potential for the model, and can be an effective complement to Training-Time Scaling.
### Resource
- Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective
- THE EXPRESSIVE POWER OF TRANSFORMERS WITH CHAIN OF THOUGHT
- CHAIN OF THOUGHT EMPOWERS TRANSFORMERS TO SOLVE INHERENTLY SERIAL PROBLEMS
- STaR: Bootstrapping Reasoning With Reasoning
- [Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters](https://arxiv.org/abs/2212.10001)
- [Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](https://arxiv.org/abs/2305.04388)
- [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050)
- [Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://arxiv.org/abs/2312.08935)
- [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)
- [Training Language Models to Self-Correct via Reinforcement Learning](https://arxiv.org/abs/2409.12917)
- [Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping](https://arxiv.org/abs/2402.14083)
- [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)
### misc

---

