## [How to discover, fine-tune, and deploy Llama 3.1 models with SageMaker JumpStart | AWS OnAir S05](https://youtu.be/_cR5uPUEUZ8)
Release date :  Oct 5, 2024   
### Idea
- use jumpstart to fine tune models using llama
- no code, ui, can use sdk and boto if wanna code

### Details
- 

### Resource
- 

### misc
 
---

## [Build high-performance RAG using just PostgreSQL (Full Tutorial)](https://youtu.be/hAdEuDBN57g)
Release date : Oct 1, 2024
### Idea
- RAG using pgvector, code walkthrough

### Details
- 

### Resource
- https://github.com/daveebbelaar/pgvectorscale-rag-solution/tree/setup

### misc
 
---
## [How Contextual Retrieval Elevates Your RAG to the Next Level](https://youtu.be/jQlxAozlJi4)
Release date : Oct 3, 2024 
### Idea
- for a very niche topic, rag can be noisy in geenratting and fetching relevant chuncks
- this trick can help reduce noise - by anthropic

### Details
- in traditional we can use M25 tfidf index for key word search
- When there is a pronoun like the company or it or they them etc, it might not be able to connect as the Noun is in another chuck
- this can lead to hallucination or noisy retrival
- to prevent this
    - enrich the chunk with extr information
    - add more context so that if read in isolation, provides clarity
    - liek appending extra context before the chunk
    - using contextual embedding or contextual BM25 (tfidf index)
- better than sumamry fo the page added to chuck as that might be too much of a context or irrlevant
- implementation
    - it is done using a smaller llm with a prmpt like
    ```llm
    <document>
    {{WHOLE_DOCUMENT}}
    </document> Here is the chunk we want to situate within the whole document
    <chunk>
    </chunk>
    Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.
    ```
- Costly affair
    - for chuck contextual chuck, whoel doc needs to be passed
    - context length needs to be large and llm should not suffer from lost in haystack situation
- Solution
    - prompt chaching ($1.02 for million doc tokens for 8k token doc) 

- Consideration
    - chucking tech exp
    - embedding exp
    - custom contuextual promts
    - number of chunkc exp
    - run eval
    - add reranker

- embedding + bm25 :  hybrid embedding

### Resource
- https://www.anthropic.com/news/contextual-retrieval

### misc
 
---
TODO L document understanding LLMs
LayoutLM
DocOwn
TinyChart