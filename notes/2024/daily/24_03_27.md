## [LLM Agent Operating System](https://youtu.be/JMU5622vCNw)
Release date : 27/03/24
### Idea
- 

### Details
- 

### Resource
- [paper](https://huggingface.co/papers/2403.16971)

### misc
 
---
## [DBRX: MOST POWERFUL Open Source LLM ](https://youtu.be/5rtGvKuEnuQ)
Release date : 27/03/24
### Idea
- new LLM from databricks
- mixture of expert model

### Details
- 136B prams with 36B live
- 16 experts with 4 active during trianing and inference
- models
    - base : pre trained
    - instruct : fine tuned for instruction following
- DBRX uses rotary position encodings (RoPE), gated linear units (GLU), and grouped query attention (GQA). It uses the GPT-4 tokenizer as provided in the tiktoken repository.
- DBRX has 16 experts and chooses 4,  while Mixtral and Grok-1 have 8 experts and choose 2.
- 

### Resource
- [github](https://github.com/databricks/dbrx)
- [HF](https://huggingface.co/databricks)

### misc
- llm foundry https://github.com/mosaicml/llm-foundry
---
