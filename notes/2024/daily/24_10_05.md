## [Fine-Tune BERT for Multi-Class Sentiment Classification with Twitter Data | Python & Streamlit](https://www.youtube.com/watch?v=NjndMvmIiec)
Release date :  Oct 4, 2024 
### Idea
- bert based classifiers

### Details
- 

### Resource
- 

### misc
 
---

## [Were RNNs All We Needed?](https://youtu.be/cK4e58Jr6-k)
Release date :  Oct 5, 2024
### Idea
- minimal varsion of lstm and gru to make them faster and parallel

### Details
- 

### Resource
- https://arxiv.org/abs//2410.01201

### misc
 
---
## [Topical Chunking - Advanced Chunking Strategy for RAG](https://youtu.be/lW2KxJIwhUY)
Release date : Oct 3, 2024
### Idea
- Chunck strategy based on topic they cover

### Details
- Steps
    1. take first chunk , metadata it with 
        - id
        - summary
        - topic
        - sentences
    2. Take second chunk , pass it to llm and ask if it belongs to previous metadata or not
        - if not repeat step 1 for 2 and create a new topic
        - if yes then update the topic which it matches to and update its metadata with
            - sumamry, sentences and topic
    3. repate for all the chunks and form a bunch of topics
    4. take the topics embed them and store in VDB

### Resource
- 

### misc
 
---
## [ell: A Powerful, Robust Framework for Prompt Engineering](https://youtu.be/IQI5BZlVI3Y)
Release date : Sep 20, 2024 
### Idea
- using prompts generation to make it better than lagchain

### Details
- uses llm to generate prompts
- less boilerplate code 
    ```python
    from dotenv import load_dotenv load_dotenv()
    import ell

    # to store this particular config of prompt, has function and llm based version control too
    ell.init(store="./logdir", autocommit=True)

    @ell.simple (model="gpt—4o-mini")
    def write_a_poem(name : str, temperature=@.3): 
        """You are a helpful assistant.""" # System Message
        return f"Write a short poem for a developer named {name}." # User message
    print(write_a_poem("MYNAMEHERE"))

    # another way 
    def write_a_poem(name : str, temperature=@.3):
        return [ell.system ("You are a helpful assistant."),
        ell.user(f"Write a short poem for a developer named {name}.")
        ]
    ```
- also has a web ui for interacting like drag and rop
    > ell ell-studio —storage ./logdir
- 

### Resource
- https://github.com/MadcowD/ell

### misc
 
---
## [Best small LLMs for local system](https://youtu.be/obSiT-y-uKU)
Release date : Sep 30, 2024
### Idea
- small llms for local system without gpus

### Details
- llama 3.2 3 and 1 B
- Gemma 2 2bit Instruction tuned
- phi 3.5 vission instrcut (Multi modal)
- minicpm - 4B
- Qwen 2.5 - 0.5 B

### Resource
- 

### misc
 
---
## [Ep 41. Your RAG Demo Is a Waste of Time](https://youtu.be/lY2EwRLbk_k)
Release date : Sep 19, 2024
### Idea
- Dont show rag as demo, thats easy instead use a custom created Q&A by human and then compare to set bench mark

### Details
- 

### Resource
- 

### misc
 
---

## About Vector DBs (Leo)
###  These algorithms and techniques help vector databases to efficiently store, index, and search vectors based on their similarity.

1. Flat indexing: This is the simplest method, where vectors are stored in a linear array without any specific optimization.
2. Ball trees: A ball tree is a data structure that partitions the vector space into a set of balls (spheres) with a specified radius. Each ball contains a subset of the vectors.
3. KD-trees: A KD-tree is a binary search tree where each node represents a hyperplane that divides the vector space into two subspaces.
4. Cover trees: A cover tree is a hierarchical data structure that recursively partitions the vector space into a set of overlapping subsets (covers).
5. HNSW (Hierarchical Navigable Small World) graphs: HNSW is a graph-based data structure that approximates the vector space using a series of interconnected nodes.
6. Quantization techniques (e.g., Product Quantization, Additive Quantization): These techniques reduce the dimensionality of vectors by dividing them into subvectors and approximating each subvector using a codebook.
7. Dimensionality reduction techniques (e.g., PCA, t-SNE): These techniques transform the original high-dimensional vectors into lower-dimensional vectors while preserving their essential properties.

###  Pros of VDBs

1. Efficient similarity search: Vector databases are optimized for performing complex similarity searches, such as nearest neighbor queries, which are crucial in machine learning applications.
2. High performance: Vector databases can handle large-scale vector data and perform searches faster than traditional databases.
3. Dimensionality reduction: Vector databases often support techniques like PCA or t-SNE to reduce dimensionality, improving search efficiency and reducing storage requirements.
4. Scalability: Vector databases are designed to scale horizontally, allowing them to handle increasing data sizes and search requests.
5. Data compression: Vector databases can compress vector data efficiently, minimizing storage requirements without compromising search quality.
6. Integration with ML frameworks: Vector databases often provide seamless integration with popular machine learning frameworks like TensorFlow, PyTorch, and Scikit-learn.
7. Flexible indexing: Vector databases offer flexible indexing options, allowing users to balance search accuracy and performance based on their specific requirements.

### By following these steps, you can ensure that your RAG model stays up-to-date with the evolving document source while maintaining a high level of performance.
- Periodic updates: Schedule periodic updates to the retriever component to ensure it has access to the latest information from the document source.
- Incremental updates: Implement incremental updates, where only the new or modified parts of the document source are processed and added to the retriever's index.
- Caching: Use caching techniques to store previously retrieved documents and their corresponding embeddings, reducing the need to recompute them during each update.
- Versioning: Maintain different versions of the retriever index to support various stages of the update process, allowing for seamless transitions between versions.
- Monitoring: Monitor the performance of the RAG model after each update to ensure the quality of the generated answers remains high.
- Evaluation: Regularly evaluate the model's performance using appropriate metrics and adjust the update frequency or techniques as needed.

#### if you're using a Faiss retriever, you can follow these steps to perform incremental updates:

1. Train the retriever index with the initial dataset.
2. Save the index and the embedding model.
3. When new data arrives, encode the new documents using the saved embedding model.
4. Perform incremental addition to the Faiss index using the add_with_ids method.

```python
import faiss
import torch
from transformers import AutoModel, AutoTokenizer

# Load the saved embedding model and index
model = AutoModel.from_pretrained("path/to/embedding/model")
index = faiss.read_index("path/to/faiss/index")

# New documents
new_documents = ["document1", "document2"]

# Tokenize and encode the new documents
tokenizer = AutoTokenizer.from_pretrained("path/to/tokenizer")
inputs = tokenizer(new_documents, return_tensors="pt", padding=True, truncation=True)
embeddings = model(inputs.input_ids)[0]

# Perform incremental update
index.add_with_ids(embeddings.detach().numpy(), np.arange(embeddings.size(0)))

# Save the updated index
faiss.write_index(index, "path/to/updated/faiss/index")
```
--- 
## [Why vector search is not enough and we need BM25](https://youtu.be/3FbJOKhLv9M)
Release date : Sep 26, 2024
### Idea
- BM25 creates sparse vector 
- which gets numbers better than vector searching as they dont understand the relation between them

### Details
- 

### Resource
- 

### misc
 
---
## [Liquid LFM 40B: The Next Frontier in Transformer AI Architecture](https://youtu.be/HFZCs_t51xQ)
Release date : Oct 2, 2024
### Idea
- Different architecture than TRx

### Details
- smaller footprint and scaleable
- for edge
- memory efficient
- KV cache in TRx are linearly relation
- in LFM, its much better
- context length is less but is effective
    - no lost in middle
- Knowledge capacity
- Multi-step reasoning
- Long context recall
- Inference efficiency
- Trainign efficiency
- Architecture
    - composed of structured operators
    - edge deployment doesn need much change
- Not good at
    - coding
    - 


### Resource
- https://www.liquid.ai/liquid-foundation-models#reimagining-model-architectures

### misc
 
---
## []()
Release date : 
### Idea
- 

### Details
- 

### Resource
- 

### misc
 
---
