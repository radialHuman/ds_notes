## [WebLLM: A High-Performance In-Browser LLM Inference Engine](https://youtu.be/yrgrhUyUyDc)
Release date : Dec 23, 2024
### Idea #ondevice
- Web llm for on device 

### Details
- WebLLM is an open-source JavaScript framework enabling high-performance large language model inference in web browsers, utilizing WebGPU and WebAssembly for efficient local computation and seamless integration.

### Resource
- https://arxiv.org/abs//2412.15803

### misc
 
---

## [Large Language Models Can Help Make Better Image Classifiers. Here's How.](https://youtu.be/S0qo4sRb9lU)
Release date :  Jun 11, 2024
### Idea #vision
- Adapting the language generation process to the downstream 
task and mode! to understand and improve vision-language models.

### Details
- If we use clip siglip or open ai clip, it will be helpful if we make the training as per the task than just using the techqnies directly
- Contrastive Vision lnaagueg model
        - has two big txt encoder and image encoder
        - they are trained to encode text and images such that related text and images get higher similarity score
        - This ability gives llm to do 0-shot classification
        - if the text is more detailed, we can improve the performance
        - we can also use the the model by asking it what kind of descriptions it has to generate an image
        - there can be some ambiguity in the description and it might not be the best fit for our downstream task
        - to eliminate this we have two ways
1. Follow-up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification (ICLR 2024)
    - Reza Esfandiarpoor, Stephen H. Bach
    - first detect ambiguity and use an LLM to to generate information we need to resolve the ambiguity
    1. just give the image and find which classes the VLM thinks it similar to
    2. next ask a different llms to find difference between these classes
        - and how to write a prompt to seperate images
        - use this imfo to create prompt accordingly to get the right output
- If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions (under review)
    - Reza Esfandiarpoor, Cristina Menghini, Stephen H. Bach
    - representation of concepts in vision llms
    - not the way we see images and our assumptions on shape and color might not always be accurate
    - Extract and Explore (EX2)
    - We prompt our classes/understanding of them to prompt and ask the promtp what it thinks of it
        - Ex : describe class 1 ...
    - This can be done using RLHF and PPO
    - reward it in alignement with your downstream task
    - Adapting llm to vlm's preference
        - VLM is frozen and LLM is updated with refinorment training
        - in the reward function you have cos of each description and image
        - it has KL divergence involved without which the difference between the model we trained and the original will be a lot
        - KL Divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. In VAEs, it regularizes the learned latent distribution to be close to a standard Gaussian, preventing overfitting and ensuring that the model generalizes well when generating new data.
        - to control the training and not make it generate nonsense since the original model was not that bad
        - This way we can udnerstand by prmoting wht kind of thing it understands about the image
    - Suprious description : not related to the image at all
    - understand non visual concept description
    - this can be used to do hypothesis testing, debugging generations
- Role of spurious descriptions in VLM representations
- Role of non-visual descriptions in VLM representations
- Different VLMs prioritize different attributes (represent concepts differently)
- VLMs prioritize different attributes across datasets

### Resource
- https://snorkel.ai/blog/improving-vision-language-models-two-studies-on-vlm-llm-cooperation/

### misc
 
00:00 Introduction
01:00 Using LLMs to Improve Image Classification
02:01 Adapting Language Generation Process
02:20 Contrastive Vision Language Models
03:51 Class Description and Ambiguities
05:00 Follow-Up Differential Descriptions (FuDD)
06:20 Experiments and Pipeline Analysis
09:14 Analytical Experiments on Descriptions
10:07 Ambiguous Classes Importance
11:05 Fine-Tuning Publicly Available LLMs
13:54 Second Part Introduction
14:03 Understanding VM Representations
15:15 Extract and Explore Methodology
17:09 Reinforcement Learning with Human Feedback
22:21 Analysis of Generated Descriptions
25:31 Role of Spurious and Non-Visual Descriptions
26:20 Fine-Grain Attribute Analysis
30:55 Use Case of X2 in Bias Analysis
33:49 Summary of Findings
---

## [What CLIP models are (Contrastive Language-Image Pre-training)](https://youtu.be/8sVgLz3-W_s)
Release date : Jun 17, 2023
### Idea #vision
- whats clip in dalle2 

### Details
- uses contrastive learnign to pair text and images based on simialrity using labelled trained dataset
- two transformrs are involed 
- one to encode the text into vector
- one for image 
- then takes bothe vecotrs and calculate the cosine similarity between them
- the outptu of the training shiuld be that the pairs have high simialrity
- and mismatched to have low
- this results in identity matrix so the matched ones will have high score
- like a recooendation engine
- labels are NLp instead of labels like how it used to be
1. take inout text embedding
2. (prior) predict corresponsing image would look like
3. using the image embdding uses diffusion to generate
- 

### Resource
- 

### misc
 
---

## [Papers in Public #11: SigLip](https://youtu.be/93yLu0S7ie0)
Release date : Aug 5, 2024
### Idea #vision
- More effecient Clip (softmax based), from Google 

### Details
- Based on LiT
    - goes over the batch twice 
- Using Sigmoid loss funcntion (text to image and image to text)
- to make things cheaper and do things twice sigmoid is used in the loss function
- Also optimizes using chucks in gpus
- siglip : image and text encoder is trained from scratch
- siglit : only trains text while keeping image encoder frozend
- scales well

### Resource
- 

### misc
 
---

## [Papers in Pubic #10: LIT ðŸ”¥ Locked Image Tuning](https://youtu.be/JxMEwB7KcOg)
Release date :  Aug 3, 2024 
### Idea #vision
- cheaper and better way of doing 
- Locked image tuning

### Details
- instead of rnadomly initializing image and text encoders and training them
1. take pretrained image model like image net
2. lock it and not fiene tune it
3. The text modle is tuned based on the image model
- Hypothesis because the data in contrastive learnign is less clean as it from scrapped internet sites

### Resource
- 

### misc
 
---

## [Papers in Public #8: ALIGN](https://youtu.be/dabw9VxpuTQ)
Release date : May 21, 2024
### Idea #vision
- Almost like clip (from openai) , but from google 

### Details
- models are traine don scratch on
- datasets are pairs of image and coresponding text
- uses contrastive learnign to get smiliarity score fo related text and images
- bert for test embedding
- efficient net for image embedding
- clip used plain trnxs for text and vision trnx for image embddings
- dataset was different
- aling's is noiser and bigger
- 400 M pairs from internet

### Resource
- 

### misc
 
---

## [Papers in Public #7: CLIP](https://youtu.be/-9NQTy840i0)
Release date : May 20, 2024
### Idea #vision
- by openai in 2021

### Details
- contrastive laguage image pre-training
- basis for visiona models
- by training a model on which text description is closer to which image it can be generalized unlike traditional CNN which goes for label
- scrapedinternet and got images with captions
- caption prediciton 
- has image and text encoder to convert it into embedding
- these encoders are trained to make sure the embedding of text and corresponsing image is high in cosine simialirty
- but not with any other description

### Resource
- 

### misc
 
---

## [OpenAI Whisper: Robust Speech Recognition via Large-Scale Weak Supervision | Paper and Code](https://youtu.be/AwJf8aQfChE)
Release date : Sep 24, 2022 
### Idea
- #TODO

### Details
- 

### Resource
- 

### misc
 
---

