## [How to Translate from Any Language to Any Language Locally Using Hugging Face](https://youtu.be/oMeNMDiuVNc)
Release date : 
### Idea
- Translator and summarization on local 16gb ram

### Details
- Huggin face piepeline
```python
from transformers import pipeline
import torch

# this will donwlaod the model and will tkae some time as tis ~3gb
translator = pipeline(
    task="translation",
    model="facebook/nllb-200-distilled-600M", # smaller version of no language left behind
    torch_dtype=torch.bfloat16 # better memory usage
    )
# this mdoel uses flores 200 languages in github


text  = """ANYTHIGN TO TRANSLET HERE """
# code from flores 200 for src and tgt_lang
text_translated = translator(text, src_lang="eng Latn", tgt_lang="tur_Latn")


# summarizaion downlaod
summarizer = pipeline (task="summarization", model="facebook/bart-large-cnn", torch_dtype=torch.bfloat16)
text  = """ANYTHIGN TO SUMMARIZE HERE """
summary = summarizer (text,
min_length=10,
max_length=100)

```
### Resource
- 

### misc
 
---
## [LlamaParse: super-charging parsing of complex documents](https://youtu.be/dTVjx1kEJrc )
Release date : 
### Idea
- How normal parser will not help in retriving proer info while llmaaparser with instruction will get whats reqruired from a pdf which havs info scattered around

### Details
#### Vanialla way of parsing
```python
!pip install llama-index llama-parse
!wget "https://policyholder. gov. in/documents/37343/931203/NBHTGBP22011V@12223. pdf", # the docuement to parse

# llama-parse is async-first, running the sync code in a notebook requires the use of nest_asyncio
import nest_asyncio
nest_asyncio.apply()

import os
from google.colab import userdata # for colab , can be removed on local
os.environ["LLAMA_CLOUD_API_KEY"] = userdata.get('llamaâ€”cloud-key' ) # for llama parser
os.environ["OPENAI_API_KEY"] = userdata.get( 'openai-key') # for embeddinga

from Llama_index.llms.openai import OpenAI
from Llama_index.embeddings.openai import OpenAlEmbedding
from llama_index.core import VectorStoreIndex
from llama_index.core import Settings
# for the purpose of this example, we will use the small model embedding and gpt3.5
embed_mode1=OpenAIEmbedding(model="text-embedding-3-small")
Llm = OpenAI(model="gpt-3.5-turbo-0125")
Settings.11m = Llm

# convert pdf tinto markdown to make it easily readable by parser
from llama_parse import LlamaParse 
documents = LlamaParse(result_type="markdown"). load_data("./policy.pdf")


# Our markdown element node parser works well for parsing the markdown output of LlamaParse into a set of table and text nodes. .
from llama_index.core.node_parser import MarkdownElementNodeParser
node_parser = MarkdownELementNodeParser (llm=OpenAI(model="gpt-3.5-turbo-0125"), num_workers=8)
nodes = node_parser.get_nodes_from_documents (documents)

base_nodes, objects = node_parser.get_nodes_and_objects(nodes)
query_engine = recursive_index.as_query_engine(similarity_top_k=25)

query_1 = "My trip was delayed and I paid 45, how much am I covered for?"
response_1 = query_engine.query(query_1)
print(str(response_1))

```
- The output is not desireable and useful
- so with instruction to extract data use this
#### llamaparser way of parsing
```python
documents_with_instruction = LlamaParse(result_type="markdown", parsing_instruction="""
This document is an insurance policy.
When a benefit/coverage/exlusion is described in the document append a line of the following benefits string forn For {nameofrisk} and in this condition {whenDoesThecoverageApply} the coverage is {coverageDescription}.
If the document contains a benefits TABLE that describe coverage amounts, do not ouput it as a tabl c
""").load_data("./policy.pdf")

# this will turn the docuemnt into sub document loaded with info thats required
# this one will give a bunch of sentenses with required data for each page

# put the newly parsed list into query engine
node_parser_instruction = MarkdownElementNodeParser (llm=OpenAI (model="gpt-3.5-turbo-0125"), num_workers=8)
nodes_instruction = node_parser.get_nodes_from_documents(documents_with_instruction)
base_nodes_instruction, objects_instruction = node_parser_instruction.get_nodes_and_objects(nodes_instruction)
recursive_index_instruction = VectorStoreIndex(nodes=base_nodes_instruction+objects_instruction)
query_engine_instruction = recursive_index_instruction.as_query_engine(similarity_top_k=25)

response_1_i = query_engine_instruction.query(query_1)
print(response_1_i)
```
- The reponse will be much better

### Resource
- 

### misc
 
---

## [Install LocalAI Locally - Free Open Source OpenAI Alternative](https://youtu.be/wczttRH-huY)
Release date : 
### Idea
- Alternate to OAI's rest API
- For local inferencing for APIs based on open ai
- Needs docker and podman
- any gpus can be used
- container image for all the models with pip, libraies etc

### Details
```bash
# depending on the cuda version or cpu the commands will vary
! docker run -ti -p 8080:8080 --gpus all localai/localai:v2.11.0-cublas-cudal2-core phi-2
# phi is a model from MS
# To use this model, interact with the API (in another terminal) with curl for instance: curl http://localhost:8080/vl1/chat/completions -H "Content-Type: application/json" -d '{ "model": "phi-2", "messages": [{"role": "user", "content": "How are you doing?", "temperature": 0.1}] }'
```
- Function calling if supported by the model the cna be done

### Resource
- [github](https://github.com/mudler/LocalAI)

### misc
 
---
## [1- Lets Learn About Langchain-What We Will Learn And Demo Projects](https://youtu.be/AOI7IVE3CMw)
Release date : 
### Idea
This framework consists of several parts.
    - LangChain Libraries: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents.
    - LangChain Templates: A collection of easily deployable reference architectures for a wide variety of tasks.
    - LangServe: A library for deploying LangChain chains as a REST API.
    - LangSmith: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.

### Details
- using ollama
- starts soon

### Resource
- 

### misc
 
---
## [Fireside Interview with Maxime Labonne - ML Scientist and LLM Creator](https://youtu.be/kdh_zBbJYoo)
Release date : 
### Idea
- The one who explains Jamba
- created several popular LLMs such as AlpahMonarch-7B, Beyonder-4x7B, Phixtral, and NeuralBeagle14, along with releasing essential LLM tools like LLM AutoEval, LazyMergekit, LazyZxolotl, AutoQuant and AutoGGUF. 

### Details
- 

### Resource
- [blog](https://mlabonne.github.io/blog/)
https://github.com/mlabonne/llm-course
https://github.com/mlabonne/llm-course?tab=readme-ov-file#tools

### misc
 
---
## [Reflection Agents](https://youtu.be/v5ymBTXNqtk)
Release date : 21/02/24
### Idea
- 

### Details
- 

### Resource
- 

### misc
 
---
