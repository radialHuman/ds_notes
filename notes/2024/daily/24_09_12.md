## [Autoregressive decoding of sentence vectors as opposed to tokens](https://youtu.be/6VE6Fc1zjkk)
Release date : 11/08/24
### Idea
- generating one token is right now costly
- comparing one with all the other token is quadratic
- this paper tries to reduce the computation and time taken

### Details
- encoder condense the info in sentence into one token, i.e. one vector
- decoder will take compressed vector and recontructs it
- these become input and output layer while llm just works on compressed vecotrs
- hence reducing the load on ram to keep all the tokens in memory
- encoding done using layer norm
- decoding uses self and cross attention
- uses cross entropy and focal loss

### Resource
- https://arxiv.org/pdf/2408.00655

### misc
 
---

## [LLM Visualization Tool to Understand Inference](https://youtu.be/z4Nf0TaMmt0)
Release date : 10/09/24
### Idea
- Visualize how transformer works with calculations and layer details

### Details
- 

### Resource
- https://bbycroft.net/llm

### misc
 
---
## [Going beyond RAG: Extended Mind Transformers - Phoebe Klett](https://youtu.be/cS6M6Jec0lU)
Release date : 12/09/24
### Idea
- Rag and long context has disadvantages
- FIne tuning is diffucult
- proposing a method to tweak attenion and layers in llm to imbed new data

### Details
- Have to use thier models and pass memories /documents to get ebtter answers and reduce hallucination
- Not sure how to convert a model into extended one at the moment

### Resource
- https://arxiv.org/abs/2406.02332
- https://huggingface.co/normalcomputing/extended-mind-mpt-7b

### misc
- This model is part of ongoing research at Normal Computing.


---
## [Auto-Retrieval with LlamaCloud - Advanced RAG - Step-by-Step Tutorials](https://youtu.be/RK0MN_d6mzk)
Release date : 10/12/24 
### Idea
- 

### Details
- 

### Resource
- https://github.com/run-llama/llamacloud-demo/blob/main/examples/advanced_rag/auto_retrieval.ipynb

### misc
 
---



https://youtu.be/g-G6ZeCZrxE
https://youtu.be/AuVJ_GgF5jc
https://www.youtube.com/watch?v=es_8_iT-oQk
https://www.youtube.com/watch?v=PRlzBl0NLgc