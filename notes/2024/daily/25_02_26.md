## [Vision Transformer from Scratch Tutorial](https://youtu.be/4XgDdxpXHEQ)
Release date : Feb 25, 2025
### Idea #Vision
- instead og subwords or token,s they use transfoers for image patches
- then its positionally embeded to know the locaiton of the patch in the whole image
- then simialr to text, multi head attention understand the relations between patches
- this will be about google's 400m siglip
    - clip was using softmax function which is computationally expensive as it compares each image with all the text 
    - siglip replaces softmax loss with sigmoid loss so it concentrates only on idividual text image pair
        - makes it fatser and efficient while trainig by avoiding comparing with unrelated texts

### Details
#### CLip
- make omage and lable pair
- pass text to text encoder and image to image's
- get the embedding for both and performa dot product
- this will create a big square matrix which will be trained such that the diagonal has the highest value as it is the combination of right matches and low for the rest
- this way it learns the relation between the text and image
- this is done via a loss function : cross entorpy loss
- This uses softmax which will consider every image for each text which takes alot of time
- CLIP : divided smmation of text->image osftmax and image to text softmax
- SIGLIP : doesn look for all as it knows irrelavant ones will be 0. it divides the square matrix in to sub matrix and then palarellizes them
- REST IS CODE RUN SKIPPED

### Resource
https://colab.research.google.com/drive/1Q6bfCG5UZ7ypBWft9auptcD4Pz5zQQQb?usp=sharing#scrollTo=1EaWO-aNOk3v

### misc
⭐️ Contents ⭐️
(0:00:00) Intro to Vision Transformer
(0:03:48) CLIP Model
(0:08:16) SigLIP vs CLIP
(0:12:09) Image Preprocessing
(0:15:32) Patch Embeddings
(0:20:48) Position Embeddings
(0:23:51) Embeddings Visualization
(0:26:11) Embeddings Implementation
(0:32:03) Multi-Head Attention
(0:46:19) MLP Layers
(0:49:18) Assembling the Full Vision Transformer
(0:59:36) Recap

---

