## [MLOps for GenAI Applications // Harcharan Kabbay // MLOps Podcast #256](https://youtu.be/SIZdOMIv6HA)
Release date : 27/08/24
### Idea
- How to productionalize

### Details
- Local is bad habit like jupyter and ollama
- Must be API based
- operationalizing CICD RAG 
    - testing 
    - microservice
        - vdb
        - llm
        - embeddings
    - query
    - retrival
    - all are single point faliuer check
    - kubernetes stateless apps
    - argo cd
- code containerization, to keep track of all libraries
    - image version
    - no docker compose in prod
    - have jenkins for automaiton
    - have dev and stage
    - api keys must be in secret
- api endpoints with parameters tweaking
- security 
    - db password sharing control
    - configs files in kubeflow
    - label matching with pod
- Templatize PR terraform
- Monitoring
    - run time, rows processed, evn, metrics
    - set alert, dashboard like grafana
- Robustness
    - replicas of VBD
    - secrets not in github
    - llm proxies
    - resources based on human needs check
    - sandbox things before prod
    - qa for each library
    - failure response plan
    - license check

### Resource
- https://harcharan-kabbay.medium.com/

### misc
 
---
## [813: Solving Business Problems Optimally with Data â€” with Jerry Yurchisin](https://youtu.be/00VDmw8Eks4)
Release date : 27/08/24
### Idea
- optimization

### Details
- SKIPPED

### Resource
- burrito gourobi optimizer games

### misc
 
---
## [Stanford CS229 I Machine Learning I Building Large Language Models (LLMs)](https://youtu.be/9vM4p9NN0Ts)
Release date : 28/08/24
### Idea
- 

### Details
- Thinks to keep in mind for llms
    - Architecture : 
    - Training algorithm/loss : 
    - Data : 
    - Evolution : 
    - systems : 
- first two academia, last 3 industry
- pretrainig : gpt
    - autoregressive : distribution words using chain fo prob, downside is looped and takes ling time, there are toher ways too
    - steps
        - tokenize
        - forward
        - predict probability of next token
        - sample
        - detokenize
    - words -> tokens -> embeddings -> NN for vector representation -> lniear layer for fized size -> softmax for porb distribution of next words
    - cross entropy loss (maximizes text log likelihood)
    - tokenizer : to avoid typo in a word being considered as non match if word is a token
        - in case fo other languages word as token cant be used
        - cant have char as token as conolexity increases expoenentially with increase in seq length
        - byte pair encoding commonly used
        - Take large corpus of text
        - Start with one token per character
        - Merge common pairs of tokens into a token
        - Repeat until desired vocab size or all merged
        - very deep, lot of things
    - eval : preplexity
        - less is betetr
        - aggregate all nlp benchmarks is also a way : helm, MMLU
    - 

### Resource
- 

### misc
 
---
## [SelectLLM - Efficient LLM Selection](https://youtu.be/m5EpOUSzWGw)
Release date : 27/08/24
### Idea
- ensemble way to optimize which query goes to which llm

### Details
- 

### Resource
- https://arxiv.org/pdf/2408.08545

### misc
 
---
