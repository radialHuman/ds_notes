## [Titans: Learning to Memorize at Test Time](https://youtu.be/A6kPQVejN4o)
Release date : Jan 27, 2025
### Idea #TTT 
- by google
- Different sequence modelling techniques
    - TRX : input seq is used to predict the next token based on past probability (auto regressive, parallel using GPU) 
        - ALl the rpevious inputs are stored in KV cache
        - Since it has access to all the input at the same time it can run ||ly
    - RNN : Not only produces the next token but also has the state (hidden, memory) of all the past input. Not always parallel. 
        - This hidden state doesn need the previous token but just the hidden state and input
        - Since its one token at a time with the hidden state from previous, its can be run ||ly
        - Issue with RNN
            - cant be ||
            - hidden state is fixed in size so leads to missing information unlinke KV cache that can expand but then lead to quadratic complexity 
    - Combingin them both :  compress suing RNN hidden state and attention can use that to reduce KV cache
        - but if there is out of distribution data in RNN it will not be able to compress it so, practically this fails
        - While training ti works but while testing it fails

### Details
1. How LLM works:
- Input -> tokenized -> embedded -> layers .... -> logits
- The past memeory is stored in KV cache which keeps growing and also after some point lead to lost in haystack, as it compresses information
2. Paper
- Due to inference cost we cant have large context window as this will increase KV cache storage
- For each token that needs to be predicted, KV cahce at every layer needs to be kept
- To solve this we can compress the memory but this will only work on training time
- **Idea is to have a memory model able to retrive information exatcly, that trains at test time**
- **Idea is to have a "neural memoery" module in the architecture**
    - This the **inner loop** trained to learn everything about the input data (right after the embedding stage, on the fly)
    - so that it can be used to retrive when required
    - The output of this is information thats required by the attention layer while training
    - Which is sent to the attention layers
    - This will happen in both trianing and test time
- Training the neural memory at training time is easy but during the test time
    - This NMM is supposed to extract information when required. 
    - To update the information in the NMM
        - It has to recreate the data that it has learnt
        - This can be done by adding noise and asking it to reconstruct the actual information
        - This is done by learning to map projections of key and value 
        - This training happens using gradient descent on memory by comparing the actual data and the reconstruction of it w.r.t to the loss fucntion. **Surprise**
        - Monentum + surprise = moemntary surprise
        - This, in the titans paper is a linear layer but can be experimented around with
        - Done over minibatch GD
- Inference/ test time NMM
    - updating the weights depending on the surpise factor 
- Limitations
    - Since for every new token the GD occurs to update the NMM it can take some time (thoguh this can be ||lized, list fo toekns chunk by chunk)
- Use of NMM
- as Context memory
    - compress informaiton, add to the normal architecture to provide better result
- as Gate
    - no attention layers just the NMM as a lyer instead

### Resource
https://arxiv.org/abs/2501.00663

### misc

---

## [LlamaIndex Sessions: 12 RAG Pain Points and Solutions](https://youtu.be/EBpT_cscTis)
Release date :  Feb 25, 2024
### Idea #RAG #llamaindex
- Production issue with solutions in llama index

### Details
1. Missing content
- leads to hallucination
- Solution
    - clean the data, no conflicts
    - Unstructed.io cleaning library
    - better prompting : tell me dont know if not sure
2. Missing tok ranked document
- Solution : Hypertunign top_k and chunk size using semantic simialirty evaluator
- have reranker as default in the architecture
3. Not in context
- Even after reranking 
- Solution
    1. Change retriving strategy
    - Basic retrieval from each index
    - Advanced retrieval and search
    - Auto-retrival
    - Knowledge graph 
    - Composed or Hierarchical retriving
    2. Fine tune embedding model used to convert token to vectors
4. Not extracted the real data
- may be due to noise and conflicts in data
- Solution
    1. Clean data
    2. prompt compression :  longlllmlingua, hyperparameters, node post processor
    3. long context reorder : to prevent lost in the middle, 
5. Wrong outptu format
- better prompting to get right format : few shot
- output parsing : guardrails, StructredOutputParser
- Pydantic function calling 
- Openai json mode
6. Incorrect specifity
- to vague or general output
- Solution
    1. Advanced retrival strategy : small-to-big, snetence window, recursive, node reference, auto-merging, query rewriting + fusion, hyde, 
7. Incomplete output
- Query transformation
    1. Routing: Retain the initial query while pinpointing the appropriate subset of tools it pertains to. Then, designate these tools as the suitable options. 
    2. Query-Rewriting: Maintain the selected tools, but reformulate the query in multiple ways to apply it across the same set of tools.
    3. Sub-Questions: Break down the query into several smaller questions, each targeting different tools as determined by their metadata.
    4. ReAct Agent Tool Selection: Based on the original query, determine which tool to use and formulate the specific query to run on that tool.
8. Data ingestion capability
- Paralell ingestionPipeline with num_workers
9. Structured Data QA
- chain-of-table and mix-self-consistency pack
10. Data extraction from complex PDF
- EMbedded table retrival
11. Fallback models
- Neutrino router
- Open Router
12. LLM security
- NeMO guardrails
    - input
    - output
    - dailog
    - retrival
    - execution
- LLamaguard by FB
    - 

### Resource
https://towardsdatascience.com/12-rag-pain-points-and-proposed-solutions-43709939a28c
[Seven Failure Points When Engineering a Retrieval Augmented Generation System](https://arxiv.org/abs/2401.05856)
### misc

---

## []()
Release date : 
### Idea
- 

### Details
- 

### Resource


### misc

---

