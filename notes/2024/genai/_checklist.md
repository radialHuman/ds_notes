- Before starting an LLM project, conduct impact & risk assessment. After POC, conduct maturity assessment.
    - identify the problem if llm is req
    - identify profit , not tech for the sake of tech
    - cost
    - define time line
    - define impact
- Risks
    - Customer facing 
    - Preventing facing hallucinations 
    - privacy concern
    - ai bias
    - data sec breach
    - downtime
    - human oversight and feeedback
    - misuse
- Questions to ask
    - which llm, verison, token useage cost, latency, 
    - embeddings : which model, how parsing occurs, how chunking occurs, computational expense
    - store embedding : how to udpate, how to store chuncks, metadata storage, chunking strategy
    - retrieving : How many chunks are retrieved, combining chunks, relevancy ranking, Metadata filtering, Which metadata was retrieved with the embeddings, Which similarity algorithm is used
    - prompt strategy : ICL, query enrichment before sending
- Fine tuning
    - Corresponding code commit. Infrastructure used for fine-tuning & serving.
    - What model artifact is produced.
    - What training data is used. retraining strategy & frequency. Methodology (supervised, self-supervised, RLHF).
- Tools : 
    - version control : github, hitlab
    - cicd : jenkins, gitlab cicd
    - orchestration :L airflow, databricks, aws
    - mdoel regustry and tracking :  mlflow, sagemaker, vertex
    - container registry :  ecr, docekr hub
    - serving : k8, databricks, azure, sagemaker
    - evaluation :  grafana, elastic, 
    - vectore db : https://superlinked.com/vector-db-comparison
# RAG
- Which model
    - architecture
    - mixture
    - Context length
    - Tranined on which data
        - Is it close to internal data
        - Is there a conflict in pre-trained vs new data
    - Size
    - Cost
    - Quantization
        - gptq, gguf, awq, exl2, hqq
    - alignment
        - dpo, ppo, orpo, kto, ipo, nac
        - dno
    - Guardrails
        - nemo
        - guardralis ai
        - aws guardrails
        - llamaguard
        - prompt injection
        - anti jailbreak
    - Techniques to improve
        - multi query retrival
        - CRAG : judge t5
        - self-RAG
        - Self-CRAG
        - Reranking
        - Hyde
        - Pre retreval
            - query routing
            - query rewriting
            - query expansion
        - Post retrival 
            - rerank
            - summary
            - fusion
        - graph rag
        - Temprature
            - top p and top k
        - model merging mergekit, LM-Cocktail
    - Citation required? 
        - llama index citation query engine
        - from langchain.chains import create_citation_fuzzy_match_chain
    - Metadata details
- Which framework
    - llamaindex
    - langchain
    - dspy
    - Verba
    - semantic kernel
    - autogen
    - taskWeive
    - Haystack
- Embedding using which model
    - OpenAl
    - bge-large
    - lim-embedder
    - Cohere-v2
    - Cohere-v3
    - Voyage
    - JinaAl
- Chunking style
    - raptor
    - semantic
    - doc summary
    - colbertv2
    - agentic
    - length
    - stop words
    - Semantic splitting
- How to choose vector store
    - familiarity
    - felxible
    - ease of implementation
        - abstraction
        - Integration
    - performance
        - Queries per second (QPS)
        - Recall rate
    - scalability
        - Vector dimensions supported
        - Number of embeddings
    - Consider trade-offs between cost, recall, throughput and latency
- Which vector store : 
    - pinecone (cloud)
    - chroma
    - Faiss
    - Vespa
    - Milvus
    - weaviate (cloud)
    - redis
    - AWS aurora with pgvector postgres
    - AWS Open search
    - qdrant
- Caching required?
- Feedback mechanism
    - store for that
    - logs
- Data
    - is conversion of pre processing it into a specific format required?
- Should we look into RAFT?
- Evaluation
    - LangSmith
    - RAGAS
    - Prometheus 2 : judge jury
    - AWS clarify
    - Giskard
    - GPT Eval
    - GPT Score
    - WEAT
    - BertScore
    - Vectara
    - replacing judge with juries
    - Panel of LLMs
    - weights and bias
    - Phoenix eval
    - MMLU
    - big bench
    - MATH
    - human eval
    - Vicuna bench
    - MT bench
    - Alpacafarm
    - Truelens
    - multiple logically related questions maxsat solver
    - self-refinement
    - DeepEval
    - AutoRAGAS (costly)
    - CICD CircleCI
- metrics
    - f1-over-words
    - Precision@k
    - NDCG
    - Hit rate
    - MRR : Mean Reciprocal Rank (MRR)the quality of information retrieval systems
    - Correctness, haluucaination, toxicity
    - faithfullness, 
    - adherence to guidlines
- Optimization strategy
    - Better Parsers
    - Chunk Sizes
    - Prompt Engineering
    - Customizing Models
    - Metadata Filtering
    - Recursive Retrieval
    - Embedded Tables
    - Small-to-big Retrieval
    - Knowledge base
# Fine tune
- Which fine tuning
    - peft
    - lora
    - qlora
    - 1 bit
    - reft
    - LISA
    - LONGLORA, LOFTQ, RSLORA, QLORA, LORA+, GALORE, DORA, NEFT, unsloth, PISSA

# Agent





- Understand what users can do, expect the unexpected