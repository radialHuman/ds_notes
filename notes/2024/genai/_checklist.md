# RAG
- Which model
    - architecture
    - mixture
    - Context length
    - Tranined on which data
        - Is it close to internal data
        - Is there a conflict in pre-trained vs new data
    - Size
    - Cost
    - Quantization
        - gptq, gguf, awq, exl2, hqq
    - alignment
        - dpo, ppo, orpo, kto, ipo, nac
- Which framework
    - llamaindex
    - langchain
    - dspy
    - semantic kernel
    - autogen
    - taskWeive
- Embedding using which model
- Chunking style
    - raptor
    - semantic
    - doc summary
    - colbertv2
    - agentic
    - length
    - stop words
- Which vector store
    - pinecone (cloud)
    - chroma
    - Faiss
    - Vespa
    - Milvus
    - weaviate (cloud)
    - redis
- Which fine tuning
    - peft
    - lora
    - qlora
    - 1 bit
    - reft
    - LISA
    - LONGLORA, LOFTQ, RSLORA, QLORA, LORA+, GALORE, DORA, NEFT, unsloth, PISSA
- Caching required?
- Feedback mechanism
    - store for that
    - logs
- Data
    - is conversion of pre processing it into a specific format required?
- Should we look into RAFT?
- Evaluation

# Agent





