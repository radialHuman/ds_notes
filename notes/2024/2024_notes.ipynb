{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats\n",
    "1. Introduction to Basic Terms\n",
    "2. Variables\n",
    "3. Random Variables\n",
    "4. Population, Sample, Population Mean, Sample Mean\n",
    "5. Population Distribution, Sample Distribution and Sampling Distribution\n",
    "6. Mean, Median ,Mode\n",
    "7. Range \n",
    "8. Measure Of Dispersion\n",
    "9. Variance \n",
    "10. Standard Deviation \n",
    "11. Gaussian/Normal Distribution\n",
    "12. Q-Q plot\n",
    "13. Chebyshev's inequality\n",
    "14. Discrete And Continuous Distribution\n",
    "15. Bernoulli And Binomial Distribution\n",
    "16. Log Normal Distribution\n",
    "17. Power Law Distribution\n",
    "18. Box Cox Transform\n",
    "19. Poisson Distribution\n",
    "20. Application Of Non Gaussian Distribution\n",
    "21. Z test, T test,Chi square, Anova test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Algebra\n",
    "1. Vectors\n",
    "2. Matrices\n",
    "3. Transpose of a matrix\n",
    "4. Inverse of a matrix\n",
    "5. Determinant of a matrix\n",
    "6. Trace of a matrix\n",
    "7. Dot product\n",
    "8. Eigenvalues\n",
    "9. Eigenvectors\n",
    "10. Single Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md' target='_blank'>../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md</a><br>"
      ],
      "text/plain": [
       "/home/radial/Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [youtube](https://youtu.be/ZTt9gsGcdDo)\n",
    "# Essential Matrix Algebra for Neural Networks, Clearly Explained\n",
    "FileLink('../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differentiation\n",
    "\n",
    "<img src=\"../../images/math_1.png\">\n",
    "\n",
    "##### 1. Chain Rule Of Differentiation\n",
    "##### 2. Partial Derivatives\n",
    "##### 3. Integrations\n",
    "##### 4. Beta and gamma functions\n",
    "##### 5. Functions Of Multiple Variable , Limit, continuity, partial derivatives\n",
    "##### 6. Variants Of Optimizers \n",
    "##### 7. Loss Functions\n",
    "##### 8. Back Propagation\n",
    "##### 9. Minima And Maxima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. Gradient Descent\n",
    "> Backpropagation\n",
    "- https://youtu.be/O3sRvkUBag0\n",
    "- In traininig of a NN labeled data is taken in as an input and the output is guessed by setting paramerter values to minimize overall misses\n",
    "- These parameters are the weights in each nueurons\n",
    "- The update of weights are done by plotting all the weights as dimesions against the loss calculated for those weights\n",
    "- Then using GD, the minimum point is found in the space where the loss is lowest and the weights at that point is taken as trained weights for inferencing\n",
    "    \n",
    "    <img src=\"../../images/bp_1.png\">\n",
    "\n",
    "    <img src=\"../../images/bp_2.png\">\n",
    "\n",
    "    <img src=\"../../images/bp_3.png\">\n",
    "\n",
    "- The minus in the formula is when the slope is downwards else positive\n",
    "https://youtu.be/IHZwWFHWa-w\n",
    "https://youtu.be/qg4PchTECck\n",
    "https://youtu.be/ORyfPJypKuU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. Optimizations\n",
    "https://youtu.be/mdKjMPmcWjY TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "#### Activation Functions\n",
    "> Why?\n",
    "- A neural network without an activation function is essentially just a linear regression model.\n",
    "\n",
    "> Binary step function\n",
    "- f(x) = 1, x>=0\n",
    "     = 0, x<0\n",
    "- f'(x) = 0, for all x\n",
    "     <img src =\"../../images/func_1.webp\">\n",
    "- if the input to the activation function is greater than a threshold, then the neuron is activated, else it is deactivated\n",
    "- this function will not be useful when there are multiple classes in the target variable\n",
    "- the gradient of the step function is zero which causes a hindrance in the back propagation \n",
    "> Linear function\n",
    "- f(x)=ax\n",
    "- f'(x) = a\n",
    "     <img src =\"../../images/func_2.png\">\n",
    "- gradient here does not become zero, but it is a constant which does not depend upon the input value x at all. This implies that the weights and biases will be updated during the backpropagation process but the updating factor would be the same.\n",
    "- The neural network will not really improve the error since the gradient is the same for every iteration. The network will not be able to train well and capture the complex patterns from the data.\n",
    "\n",
    "> sigmoid\n",
    "- f(x) = 1/(1+e^-x)\n",
    "- f'(x) = sigmoid(x)*(1-sigmoid(x))\n",
    "     <img src =\"../../images/func_3.webp\">\n",
    "- Sigmoid transforms the values between the range 0 and 1\n",
    "- gradient values are significant for range -3 and 3 but the graph gets much flatter in other regions.\n",
    "- As the gradient value approaches zero, the network is not really learning\n",
    "- the sigmoid function is not symmetric around zero. So output of all the neurons will be of the same sign.\n",
    "\n",
    "> tanh\n",
    "- tanh(x)=2sigmoid(2x)-1\n",
    "- The only difference is that it is symmetric around the origin. The range of values in this case is from -1 to 1\n",
    "     <img src =\"../../images/func_4.webp\">\n",
    "- gradient of the tanh function is steeper as compared to the sigmoid function\n",
    "- tanh is preferred over the sigmoid function since it is zero centered and the gradients are not restricted to move in a certain direction\n",
    "\n",
    "> relu\n",
    "- f(x)=max(0,x)\n",
    "- f'(x) = 1, x>=0\n",
    "      = 0, x<0\n",
    "     <img src =\"../../images/func_5.png\">\n",
    "- The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.\n",
    "- the neurons will only be deactivated if the output of the linear transformation is less than 0\n",
    "- the negative side of the graph, you will notice that the gradient value is zero. Due to this reason, during the backpropogation process, the weights and biases for some neurons are not updated\n",
    "\n",
    "> leaky relu\n",
    "- f(x)= 0.01x, x<0\n",
    "    =   x, x>=0\n",
    "- f'(x) = 1, x>=0\n",
    "     =0.01, x<0\n",
    "- Instead of defining the Relu function as 0 for negative values of x, we define it as an extremely small linear component of x.\n",
    "- the gradient of the left side of the graph comes out to be a non zero value. Hence we would no longer encounter dead neurons in that region\n",
    "     <img src =\"../../images/func_6.webp\">\n",
    "\n",
    "> Parameterised ReLU\n",
    "- introduces a new parameter as a slope of the negative part of the function\n",
    "- f(x) = x, x>=0\n",
    "    = ax, x<0 , 'a‘ is also a trainable parameter\n",
    "- If value of a is fixed to 0.01, the function acts as a Leaky ReLU function\n",
    "- f'(x) = 1, x>=0 \n",
    "      = a, x<0\n",
    "     <img src =\"../../images/func_7.webp\">\n",
    "- It is used when the leaky ReLU function still fails to solve the problem of dead neurons and the relevant information is not successfully passed to the next layer\n",
    "\n",
    "> Exponential Linear Unit\n",
    "- Unlike the leaky relu and parametric ReLU functions, instead of a straight line, ELU uses a log curve for defning the negatice values\n",
    "- f(x) = x,   x>=0\n",
    "    = a(e^x-1), x<0\n",
    "     <img src =\"../../images/func_8.webp\">\n",
    "- f'(x) = x,   x>=0\n",
    "    = a(e^x), x<0\n",
    "\n",
    "> Swish\n",
    "- computationally efficient as ReLU and shows better performance than ReLU on deeper models\n",
    "- the curve of the function is smooth and the function is differentiable at all points. This is helpful during the model optimization process and is considered to be one of the reasons that swish outoerforms ReLU\n",
    "- swich function is not monotonic. This means that the value of the function may decrease even when the input values are increasing\n",
    "- f(x) = x*sigmoid(x)\n",
    "     f(x) = x/(1-e^-x)\n",
    "     <img src =\"../../images/func_9.webp\">\n",
    "\n",
    "> Softmax\n",
    "- Softmax function is often described as a combination of multiple sigmoids used for binary classification problems.\n",
    "- softmax function can be used for multiclass classification problems\n",
    "     <img src =\"../../images/func_10.webp\">\n",
    "- if you have three classes, there would be three neurons in the output layer. Suppose you got the output from the neurons as [1.2 , 0.9 , 0.75].\n",
    "- softmax function over these values, you will get the following result – [0.42 ,  0.31, 0.27]. These represent the probability for the data point belonging to each class. Note that the sum of all the values is 1\n",
    "\n",
    "https://youtu.be/s-V7gKrsels TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other functions\n",
    "> logit\n",
    "- .\n",
    "-\n",
    "\n",
    "> Loss function\n",
    "- https://youtu.be/QBbC3Cjsnjg TODo\n",
    "-\n",
    "\n",
    "> Likelihood estimation\n",
    "https://youtu.be/-eGJuwQ5A2o TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML\n",
    "### Working \n",
    "- https://youtu.be/IqT551LjKHw TODO\n",
    "- https://youtu.be/0yY-U4LnZOM Bias variance trade off\n",
    "- https://youtu.be/a86WxNgMv7E corss validtion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Regression?\n",
    "To predict value of something (target) based on values of multiple factors (features).\n",
    "Since it is supervised, it must have training data with values of both target and features to get a sum of products (equation) which would have be the best among all possible equations describing the relations between the two by reducing the amount of distance between the actual value and the value predicted by the equation's line.\n",
    "Depending on the nature of the factors it will be linear (straight line/plane/hyperplane) or non linear (curved).\n",
    "\n",
    "Linear : target = A*feature1 + B*feature2 + C*Feature3 + x\n",
    "Non Linear : target = A*feature1^4 + B*feature2^2 + C*Feature3 + x (order is 4, as thats the highest)\n",
    "\n",
    "1. How is it learning?\n",
    "\n",
    "2. Where are weight and biases getting updated?\n",
    "\n",
    "3. Whats the loss function?\n",
    "\n",
    "4. What are the hyper parameters?\n",
    "\n",
    "5. https://youtu.be/K_EH2abOp00 TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assumptions\n",
    "1. Linearity: The relationship between the dependent and independent variables is linear and additive. Must have linear betas.\n",
    "- A linear relationship suggests that a change in response Y due to one unit change in X¹ is constant, regardless of the value of X¹.\n",
    "- An additive relationship suggests that the effect of X¹ on Y is independent of other variables.\n",
    "- If this is ignored, then the line will be off the actual value by a lot and the model will not accurately represent the data\n",
    "\n",
    "- How to check if this is met before modeling: Plot scatter plot of each feature against the target and see the pattern. If its like a stright line then fine\n",
    "\n",
    "- How to check after modeling : Plot residual vs predicted values and if there is any pattern and lack of randomness across the middle like there a non linear relationship was involved\n",
    "\n",
    "- How to fix : If its not then polynomial regression might be needed on the variable in the equation by applying transformations like log or exp or root or power etc depending on the relation between that variable and the target\n",
    "\n",
    "\n",
    "2. Independence (No Autocorrelation in residuals/errors): The observations are independent of each other.\n",
    "- This means that a residual of an observation should not predict the next observation.\n",
    "- Usually an issue in time series but if features are not choosen wisely, if can occur in tabular too\n",
    "- Might be caused by missing variable\n",
    "\n",
    "- How to check before modeling : We can conduct durbinWatsonTest on our model for an output with a p-value, which will determine whether the assumption is met or not. \n",
    "        The null hypothesis states that the errors are not auto-correlated with themselves (they are independent). Thus, if we achieve a p-value >0.05, we would fail to reject the null hypothesis.\n",
    "        It must lie between 0 and 4. If DW = 2, implies no autocorrelation, 0 < DW < 2 implies positive autocorrelation while 2 < DW < 4 indicates negative autocorrelation.\n",
    "\n",
    "- How to check after modeling : Draw the line over the points and look at the residual plot for patterns. If exists then there is no independence. Must be random.\n",
    "        Residual vs time plot and look for the seasonal or correlated pattern in residual values.\n",
    "\n",
    "- How to fix :\n",
    "        Do more domain searching to add missing variables\n",
    "        Depending on the equation, or the relation between x and y, add polynomial or exp or log\n",
    "        <img src=\"../../images/autocorr_correction.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "\n",
    "3. Homoscedasticity: The variance of the errors is constant across all levels of the independent variables.\n",
    "- The variance of the residuals should not vary as the target is increased in residual plot\n",
    "- Generally, non-constant variance arises in presence of outliers or extreme large values. \n",
    "- If a feature has a wide range of numbers then this can happen as the feature might not be a good predictor\n",
    "\n",
    "- How to check before modeling: Plot a variable against the target, find best fit and look at residual plot. If there is a funnel/dual cone/reverse funnel pattern then it might be heteroscedastic. \n",
    "\n",
    "- How to check after modeling : residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern. Wreusch-Pagan / Cook – Weisberg test or White general test or Goldfledt-Quant test to detect this phenomenon\n",
    "\n",
    "- How to fix it : There might be two/multiple regression liens that can explain if divided based on pattern. Log variables might help.\n",
    "        <img src=\"../../images/hetero_example.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "4. Normality: The errors follow a normal distribution.\n",
    "- The spread of the errors at a given point, should be normally distributed\n",
    "- prediction errors/residuals need to be normally distributed\n",
    "- Sometimes the error distribution is \"skewed\" by the presence of a few large outliers.\n",
    "- Calculation of confidence intervals and various significance tests for coefficients are all based on the assumptions of normally distributed errors.\n",
    "- If the error distribution is significantly non-normal, confidence intervals may be too wide or too narrow. \n",
    "- The dependent and independent variables in a regression model do not need to be normally distributed by themselves   \n",
    "- the normal error assumption is usually justified by appeal to the central limit theorem, which holds in the case where many random variations are added together.\n",
    "\n",
    "- How to check before modeling: plot a histogram on your residuals or employ a Q-Q plot, which also helps us visually determine if our residuals follow a normal distribution. if there is a skew in your residuals. A log transformation on your dependent variable may help.\n",
    "\n",
    "- How to check after modeling : The Kolmogorov-Smirnov test, the Shapiro-Wilk test, the Jarque-Bera test, and Anderson-Darling test.\n",
    "\n",
    "- How to fix it : check the extreme values of the variable\n",
    "\n",
    "5. No multicollinearity: The independent variables are not highly correlated with each other.\n",
    "- independent variables show moderate to high correlation\n",
    "- tough task to figure out the true relationship of a predictors with response variable\n",
    "- difficult to find out which variable is actually contributing to predict the response variable.\n",
    "- the standard errors tend to increase and the confidence interval becomes wider leading to less precise estimates of slope parameters\n",
    "- two variables vary very similarly and contain the same kind of information.\n",
    "- only the complexity of the model increase, and no new information or pattern is learned by the model.\n",
    "- \n",
    "\n",
    "- How to check before modeling:  use scatter plot/heat map to visualize correlation effect among variables\n",
    "\n",
    "- How to check after modeling : VIF factor. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity.\n",
    "        VIF = 1/(1-R2) for each variable\n",
    "\n",
    "- How to fix it : drop and check vif\n",
    "\n",
    "6. No endogeneity: There is no relationship between the errors and the independent variables.\n",
    "- Causes of endogeneity include omitted variables, simultaneity, and sample selection in the estimated equation. \n",
    "- OLS estimator will be inconsistent and biased,\n",
    "\n",
    "- How to check before modeling:  \n",
    "\n",
    "- How to check after modeling : Durbin-Wu-Hausman test, which compares the OLS estimator with an alternative estimator that is robust to endogeneity, such as the instrumental variables (IV) estimator.\n",
    "\n",
    "- How to fix it : reduce the sources of endogeneity by including more relevant variables, improving the data quality, or using a different research design that avoids the causal ambiguity.\n",
    "        instrumental variables (IV) estimator, which uses a variable that is correlated with the endogenous explanatory variable but not with the error term\n",
    "\n",
    "Source : \n",
    "[youtube](https://www.youtube.com/playlist?list=PLTNMv857s9WUI1Nz4SssXDKAELESXz-bi)\n",
    "[blog 0](https://medium.com/@andrewhnberry/checking-your-linear-regression-assumptions-and-how-to-check-them-338f770acb57)\n",
    "[blog 1](https://godatadrive.com/blog/basic-guide-to-test-assumptions-of-linear-regression-in-r)\n",
    "[blog 2](https://people.duke.edu/~rnau/testing.htm)\n",
    "[blog 3](https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/)\n",
    "[blog 4](https://www.geeksforgeeks.org/assumptions-of-linear-regression/)\n",
    "[blog 5](https://www.jmp.com/en_in/statistics-knowledge-portal/what-is-regression/simple-linear-regression-assumptions.html)\n",
    "\n",
    "\n",
    "> https://youtu.be/Bu1WCOQpBnM TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Working\n",
    " * The aim is to find BLUE (best linear unbiased estimate) or the best fit line which explains all the variance across the data\n",
    " * This is usually an equation involving target on one side and linear additive combination of features on the other which are not in polynomial or expoenential or logarithmic form\n",
    " * y = coefficient + ax1 + bx2 + cx3 + dx4 | ... | + error\n",
    " * where a,b,c,d ... are slope values and coefficient is the intercept in multi dimensional plane\n",
    " * here coeeficient is the value when features are 0\n",
    " * and slope is the unit change in target wrt. unit change in feature\n",
    "\n",
    " * Cost function : to reduce overall distance between each point in dataset and predcited value by the line\n",
    " * in the below equation is one way to find the distance b/w predicted and actual, this can be replaced with absolute also. where m is total number of values\n",
    "\n",
    "  <img src=\"../../images/linear_cost.png\">\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/1-OGRohmH2s)\n",
    "\n",
    "* Gradient Descent\n",
    "\n",
    "  - Infinite number of lines can be drawn and then the cost function can be calculated for all but thats computationally expensive\n",
    "  - So by keeping coefficent constant, and varying the slope values, for all the variables values, cost function is calculated.\n",
    "  - Then a grpah is drawn between all possible m values and cost function's output to find the lowest point (gloabl minima) on the curve which will give the best m for line \n",
    "  - This finding of global minima (slope = 0) is using gradient descent which is based on convergence theorem which moves towards minima using derivative values of slope and a learning rate\n",
    "  - To know which way is global minima, all the sides needs to be considered before moving in any direction\n",
    "  - TODO https://youtu.be/sDv4f4s2SB8\n",
    "\n",
    "\n",
    "* LR using Ordinary Least square (OLR)\n",
    "- TODO  \n",
    "https://youtu.be/KZ1mWboXE6g\n",
    "https://youtu.be/IHZwWFHWa-w\n",
    "\n",
    " \n",
    "* Multiple LR using Matrix \n",
    "  - In this image, the a is all the beta values including coefficient\n",
    "  - <img src=\"../../images/multi_reg_matrix.png\"> \n",
    "    \n",
    "  - Using direct formula : (Xt.X)^-1.Xt.Y\n",
    "  - Using single valur decomposition\n",
    "  - Using QR decomposition\n",
    "\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/_OyKjstWe80)\n",
    "[blog](https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0, b1, b2 = [-2.40119363  1.62732095 -0.29177719]\n",
      "Predictions = [1. 2. 3.]\n",
      "Error = [-0. -0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# MLR using Matrix\n",
    "import numpy as np\n",
    "x = np.array([[100, 212, 356], [4.25, 10.99, 16.2],  [-234, -465, -678]], np.int32)\n",
    "y = np.array([1,2,3])   \n",
    "# formula : (Xt.X)^-1.Xt.Y\n",
    "betas = np.dot(np.dot(np.linalg.inv(np.dot((x.T), x)),(x.T)),y)\n",
    "np.set_printoptions(suppress=False)\n",
    "print(\"b0, b1, b2 = {}\".format(betas))\n",
    "preds = np.dot(x,betas)\n",
    "print(\"Predictions = {}\".format(preds))\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"Error = {}\".format(preds-y))\n",
    "\n",
    "#source : https://cmdlinetips.com/2020/03/linear-regression-using-matrix-multiplication-in-python-using-numpy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Code\n",
    "\n",
    "source : [blog](https://www.freecodecamp.org/news/data-science-with-python-8-ways-to-do-linear-regression-and-measure-their-speed-b5577d75f8b/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 1.0\n",
      "Betas : [ 0.00863046  0.01082108 -0.01180701]\n",
      "Beta0 : 2.046179467416178\n",
      "Predictions : [2.11436926]\n"
     ]
    }
   ],
   "source": [
    "# MLR using scikit learn\n",
    "# !pip install scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(x, y)\n",
    "print(\"Score : {}\".format(reg.score(x, y)))\n",
    "print(\"Betas : {}\\nBeta0 : {}\".format(reg.coef_, reg.intercept_))\n",
    "print(\"Predictions : {}\".format(reg.predict(np.array([[3, 5, 1]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betas : [-2.40119363  1.62732095 -0.29177719]\n",
      "Model : \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                    nan\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Tue, 02 Jan 2024   Prob (F-statistic):                nan\n",
      "Time:                        17:16:02   Log-Likelihood:                 81.846\n",
      "No. Observations:                   3   AIC:                            -157.7\n",
      "Df Residuals:                       0   BIC:                            -160.4\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -2.4012        inf         -0        nan         nan         nan\n",
      "x2             1.6273        inf          0        nan         nan         nan\n",
      "x3            -0.2918        inf         -0        nan         nan         nan\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   1.012\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.488\n",
      "Skew:                           0.643   Prob(JB):                        0.784\n",
      "Kurtosis:                       1.500   Cond. No.                     1.42e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.42e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 3 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1795: RuntimeWarning: divide by zero encountered in divide\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1795: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1717: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n"
     ]
    }
   ],
   "source": [
    "# MLR using statsmodel\n",
    "# !pip install statsmodels\n",
    "import statsmodels.api as sm\n",
    "model = sm.OLS(y,x)\n",
    "results = model.fit()\n",
    "print(\"Betas : {}\".format(results.params))\n",
    "print(\"Model : \\n{}\".format(results.summary()))\n",
    "# source  : [blog](https://datatofish.com/statsmodels-linear-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Output analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Metrics\n",
    "\n",
    "> MSE : Mean Squared error\n",
    "- Considers square of distance between actual and predicted value\n",
    "- Since its a quadratic in nature, the curve is differentiable at all points to find minima\n",
    "- one local and global minima as its a paraobla\n",
    "- Can tackle outliers as the distance is squared.\n",
    "- Since its a square the units are not the same for easy explanation\n",
    "\n",
    "> RSME\n",
    "- This is root of mse\n",
    "- suffers from same outlier impact\n",
    "- but has the same unit\n",
    "- also is differentiable at all points\n",
    "\n",
    "> RSLME\n",
    "- log of RSME to slows down the scale of error\n",
    "\n",
    "> MAE : Mean Absolute error\n",
    "- absolute difference between predicted and actual value\n",
    "- so the nature of the functionis like a V and is not differentiable in the origin\n",
    "- but is robust as outliers dont affect it as much as mse\n",
    "- Also since its |y-yhat| the unit is same for explainability\n",
    "- but its optimization takes time and is complicated as sub gradients needs to be calculated\n",
    "\n",
    "> MAPE\n",
    "- percentage of MAE, i.e. (MAE*100)/N\n",
    "\n",
    "> R2\n",
    "- 1-((square of sum of actual minus predicted)/(square of sum of actual minus mean))\n",
    "- 0.8-0.90 is good depending on the case\n",
    "- 1 might indicate overfitting\n",
    "- always increases if more features are added and might over fit so must be penalized\n",
    "    \n",
    "> Adjuested R2\n",
    "- To prevent increasing of R2 by adding useless features\n",
    "- if the feautre is not good, it will reduce the adjusted R2\n",
    "- 1-((1-R2)(N-1))/(N-P-1)\n",
    "- N is # of points\n",
    "- P is # of variables\n",
    "\n",
    "Source \n",
    "[youtube](https://youtu.be/BGlEv2CTfeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Regularization\n",
    "- When model is overfitting, i.e. good on training and bad on val and test, it might be learning the noise and not understanding the pattern\n",
    "- Overfitting models have high variance\n",
    "- When it doesn perform well even in the training then its underfitting, can be solved by making the model more complex or adding new features\n",
    "- Simple model which underfits is highly biased\n",
    "- Regularization helps in bias variance trade off by penalizing models for overfitting\n",
    "<img src = \"../../images/under_over_fit.png\">\n",
    "<img src  = \"../../images/regularization.webp\">\n",
    "\n",
    "\n",
    "1. Lasso - Least absolute shrinkage and selection operator regression (L1)\n",
    "- The cost function used in regression where distance is squared, a product of lambda and absolute slope is added to penalize.\n",
    "- If there are features in a model which doesn contribute much, LASSO makes its weight to 0 there by acting as feature selector\n",
    "- <img src = \"../../images/lasso.png\">\n",
    "- m is # of features, n is # of samples, w is the slope of the feature, yi is actual and yhat is predicted\n",
    "- lambda is the hyperparameter, selected using cross valdidation (0-infinity)\n",
    "- When the value of lambda is very high, it will make the weights of slopes of features which dont contribute much go to 0\n",
    "- since the absolute graph is like a diamond which has 0\n",
    "\n",
    "    ```python\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    lasso=Lasso()\n",
    "    parameters={ 'alpha' :[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,28, 38,35,55,100]}\n",
    "    lasso_regressor=GridSearchCV( lasso, parameters, scoring='neg_mean_error' ,cv-5)\n",
    "    lasso_regressor. fit(X,y)\n",
    "    print(lasso_regressor.best_params_)\n",
    "    print(lasso_regressor.best_score_)\n",
    "    ```\n",
    "\n",
    "2. Ridge - (L2)\n",
    "- The cost function is modified by adding a squared value of the slope\n",
    "- <img src = \"../../images/ridge.png\">\n",
    "- This reduces overfitting\n",
    "- lambda is the hyperparameter (0-infinity)\n",
    "- high lambda will reduce the weight but not make it 0\n",
    "- since square graph is like a circle and doesn go to 0\n",
    "\n",
    "    ```python\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.model_ selection import GridSearchcv\n",
    "    ridge=Ridge() \n",
    "    paraneters={ \"alpha\" :[1e-15,le-10,1e-8,1e-3,1e-2,1,5,16,20, 38, 35,46,45,58,55,168]}\n",
    "    ridge_regressor=Gridsearchcv( ridge, parameters, scoring='neg_mean_squared_error',cv=5)\n",
    "    ridge_regressor.fit(x,y)\n",
    "    ```\n",
    "\n",
    "3. Elastic net (L1 and L2)\n",
    "- A combination of both Lasso and Ridge\n",
    "- Cost function has both absolute and squared values of slope added\n",
    "- <img src = \"../../images/elastic.png\">\n",
    "- this has two hyper parameters alpha and lambda\n",
    "\n",
    "    Source  :    \n",
    "    [youtube](https://youtu.be/2bx9Os2gPu4)\n",
    "    [youtube](https://www.youtube.com/watch?v=9lRv01HDU0s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantile Regression\n",
    "https://youtu.be/JvIzB3hULCo TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "#### Basics\n",
    "> Parametric vs non parametric\n",
    "- Parametric models will have limited parameters and to predict new data, you only need to know the parameter of the model.\n",
    "- Non-Parametric models have no limits in taking a number of parameters, allowing for more flexibility and to predict new data. You need to know the state of the data and model parameters.\n",
    "\n",
    "> Cost function\n",
    "- Cross-entropy loss(y,p) = – yT log(p)  -(y1 log(p1) + y2 log(p2) + ……yn log(pn) )\n",
    "- Binary cross entropy/Log loss\n",
    "    - Cross-entropy(D) = – y*log(p) when y = 1\n",
    "    - Cross-entropy(D) = – (1-y)*log(1-p) when y = 0\n",
    "    <img src=\"../../images/classi_loss_1.png\">\n",
    "- Hinge loss\n",
    "    <img src=\"../../images/classi_loss_2.jpeg\">\n",
    "    - primarily developed for use with Support Vector Machine (SVM) models.\n",
    "    - encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values.\n",
    "- Square hinge\n",
    "    - squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of smoothing the surface of the error function and making it numerically easier to work with.\n",
    "- Category ccross entropy TODO\n",
    "    - suitable for multi-class classification problems\n",
    "    - quantifies the dissimilarity between predicted class probabilities and the true one-hot encoded labels\n",
    "    - maximize the correct class probability while minimizing others, promoting accurate class assignments during training\n",
    "- KL Divergence\n",
    "    - used in probabilistic models, such as variational autoencoders\n",
    "    - measures the difference between the predicted probability distribution and the true distribution\n",
    "    - Minimizing KL Divergence encourages the model to learn the underlying structure of the data and generate samples close to the true distribution\n",
    "Source [blog](https://medium.com/@evertongomede/understanding-loss-functions-in-deep-learning-9f06e5090f20)\n",
    "\n",
    "> Grid vs random search CV\n",
    "-\n",
    "> class_weight\n",
    "- parameter to tell the model if the dataset target is balanced or not \n",
    "> Bootstrapping\n",
    "- Process of randomly selecting rows of data with replacement to generate mode datasets\n",
    "- Used usually in ensemble models like Random Forests \n",
    "> Bagging\n",
    "- Combination of bootstrapping and aggregation (majority wins) this is a paralell process as all the different sets are independent of each other\n",
    "> Boosting\n",
    "- This is a sequential process to work on weak learners by boosting the values of misclassified data points\n",
    "- first build a model on the training dataset and then build a second model to rectify the errors present in the first model\n",
    "> Stacking\n",
    "- \n",
    "> Blending\n",
    "- \n",
    "> Optimizer\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic\n",
    "> Working\n",
    "- For binary classification, things can be done using linear regression and a threshold like 0.5 can be set\n",
    "- but this might be susceptible to outliers\n",
    "- The aim of Log Reg is to find a best fit line which will linearly seperate the groups\n",
    "- equation is same as regression y = mx+c\n",
    "- The idea is to convert multidimensional points into just 0 or 1\n",
    "- This is done using sigmoid function\n",
    "- <img src = \"../../images/sigmoid.png\">\n",
    "- Instead of equation from Lin reg, if there is a sigmoid of that equation, the predicted value will always be b/w 0,1\n",
    "- This also gives the proabability given the value x, what are the chances the point belongs in either classes\n",
    "- Where the predicted value will be perpendicular distance of the point from the line\n",
    "- The unknown variable is the wieghts that needs to be assigned to each feature\n",
    "- <img src =\"../../images/log_weights.png\">\n",
    "- Here w is the weights, which laong with x which is the input, goes through summation and sigmoid non linear function to get predictions.\n",
    "- Which is then used to find the error or the loss function which then is partially derivated using gradient descent to find the global minima\n",
    "- <img src = \"../../images/log_chain.png\">\n",
    "- The cost function is\n",
    "- <img src = \"../../images/log_cost.png\">\n",
    "- This needs to be partially differentiated \n",
    "- <img src = \"../../images/log_diff.png\">\n",
    "- Difference between lir and log reg is that\n",
    "    - the yhat is sigmoid in log\n",
    "    - the cost function is not convex so there are many local minima and the initial parameters of weights determine where the local minima will be found after grad desct (no guarantee of global minima)\n",
    "- the Logit/log odds function\n",
    "- <img src = \"../../images/logit.png\">\n",
    "- Regularization also can be done\n",
    "- <img src =\"../../images/log_regularization.png\">\n",
    "\n",
    "> Code\n",
    "```python\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "    clf.predict(X[:2, :])\n",
    "    clf.predict_proba(X[:2, :])\n",
    "    clf.score(X, y)\n",
    "    # parameters : penalty, C, solver, \n",
    "```\n",
    "\n",
    "> Pros and cons\n",
    "+ Simple to build and explain\n",
    "+ less overfitting in lower dimensions\n",
    "- creates linear boundaries, we won't obtain better results when dealing with complex or non-linear data\n",
    "- cant be used if the number of observations is fewer than the number of features; otherwise, it may result in overfitting.\n",
    "- affected by outliers\n",
    "- has to qualify the assumptions like in linear regression\n",
    "\n",
    "1. How is it learning?\n",
    "\n",
    "2. Where are weight and biases getting updated?\n",
    "\n",
    "3. Whats the loss function?\n",
    "\n",
    "4. What are the hyper parameters?\n",
    "\n",
    "5. https://youtu.be/YMJtsYIp4kg TODO https://youtu.be/slBI5YuVUTM TODO https://youtu.be/hpkI-7wM03I\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/z9XAXXGwUzM)\n",
    "[youtube](https://youtu.be/xXvgkILaFT4?list=PLyqSpQzTE6M-SISTunGRBRiZk7opYBf_K)\n",
    "[blog](https://www.analyticsvidhya.com/blog/2021/10/building-an-end-to-end-logistic-regression-model/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "> Assumptions\n",
    "- If the values are continuous then they are discretized prior to building the model\n",
    "- \n",
    "> Working\n",
    "- Works for bth regression and classification\n",
    "- Its multiple linear lines, not exactly non linear but better than just one linear line\n",
    "- hierarchical tree structure consisting of a root node, branches, internal nodes, and leaf nodes\n",
    "- algorithmic model utilizes conditional control statements and is non-parametric\n",
    "- The tree is built by splitting on each feature and the hierarchy of feature is decided based on the purity of the feature in predicting the target\n",
    "- The purity is based on\n",
    "    - Entropy and information gain\n",
    "        - <img src=\"../../images/decision_entropy.png\">\n",
    "        - <img src=\"../../images/decision_entropy_ex.png\">\n",
    "        - entropy before the split and after the split is compared\n",
    "        - <img src=\"../../images/decision_entropy_ex2.png\">\n",
    "        - Entropy is found for all features and which ever has the mx change is taken\n",
    "        - I.e. higher the Entropy, the lower will be the purity\n",
    "        - Which ever feature gives most information gain is used as node as thats the most purest\n",
    "        - Steps\n",
    "            - First entropy of the whole target is calulated\n",
    "            - Entropy of each feature is calculated for each class in it\n",
    "            - Information gain of each feature is found, which is total entropy minus (individual class entropy of the feature into its proability)\n",
    "            - Which feature has the highest IG becomes the root node with brances as its classes\n",
    "            - Now for each of the branches, its takes subset by filtering onlny for those value and then IG is calculated again to find the next split\n",
    "            - This continues, till there are leaf nodes that are pure split\n",
    "    - Gini index\n",
    "        - Unlike IG, minimum GI is taken as root node as its 1-(probability)\n",
    "        - <img src=\"../../images/decision_gini.png\">\n",
    "        - Steps\n",
    "            - Almost the same just that lowest GI will be used as root and for splitting\n",
    "    - <img src=\"../../images/decision_gi_IG.png\">\n",
    "    - [youtube](https://youtu.be/5aIFgrrTqOw)\n",
    "    - Entropy ranges from 0-1\n",
    "    - GI ranges from 0-0.5\n",
    "    - GI doesn have log and hence is faster\n",
    "- The nodes have threshold in case of numerical values\n",
    "    - [youtube](https://youtu.be/5O8HvA9pMew?list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe)\n",
    "    - Sorts the tables in ascending manner\n",
    "    - Each number is taken and IG or GI is calculated\n",
    "    - Time consuming for large dataset\n",
    "- For regression tree \n",
    "    - TODO\n",
    "- Pruning : When we remove sub-nodes of a decision node, cutting down of branches\n",
    "- Early stopping :  not allowing the layes to go beyond a point\n",
    "> Code\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(criterion ='entropy').fit(X_train,y_train)\n",
    "prediction = tree.predict(X_test)\n",
    "tree.score(test_features,test_targets)\n",
    "# parameters : max_depth, min_samples_split, criterion, min_samples_leaf, max_features, \n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "cross_val_score(regressor, X, y, cv=10)\n",
    "```\n",
    "> Pros and cons\n",
    "+ Highly explainable\n",
    "+ resistant to outliers\n",
    "- Generally overfits, hence high variance\n",
    "- parametric, hence new data point if added, needs retraining\n",
    "- not suitable for large datasets as its just one tree\n",
    "- not for imbalanced dataset\n",
    "\n",
    "1. How is it learning?\n",
    "\n",
    "2. Where are weight and biases getting updated?\n",
    "\n",
    "3. Whats the loss function?\n",
    "\n",
    "4. What are the hyper parameters?\n",
    "\n",
    "5. \n",
    "\n",
    "Source \n",
    "[youtube](https://youtu.be/uPwZ8DhQNjg?list=PLyqSpQzTE6M-SISTunGRBRiZk7opYBf_K)\n",
    "[youtube](https://youtu.be/CWzpomtLqqs)\n",
    "[blog]()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "> Working\n",
    "- DT are highly sensitive to training data and can overfit without generalizing\n",
    "- First multiple datsets are generated using bootstraping\n",
    "- Then for these randomly selected rows, randomly selected features are selected \n",
    "    - The usual number for feature # is log or sqr root of total # of features\n",
    "- These two add diversity to the datasets and reduces chances of overfitting\n",
    "- Then for each dataset DTs are created\n",
    "- For predicting, the data point is passed through all the trees and the output is generated\n",
    "- Majority wins (this is aggregation)\n",
    "- bootstrapping + aggregation = bagging\n",
    "- In case of regression, a mean or median can be considered as the output\n",
    "\n",
    "> Code\n",
    "    ```python\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf.fit(X, y)\n",
    "    clf.predict([[0, 0, 0, 0]])\n",
    "    clf.feature_importances_\n",
    "\n",
    "    # parameters : (n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "    regr.fit(X, y)\n",
    "    regr.predict([[0, 0, 0, 0]])\n",
    "\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- Complex computation, slower\n",
    "- interpretation is difficult\n",
    "\n",
    "+ Scaling is not really required\n",
    "+ Doesn care about unwanted features\n",
    "+ Generalized\n",
    "\n",
    "1. How is it learning?\n",
    "\n",
    "2. Where are weight and biases getting updated?\n",
    "\n",
    "3. Whats the loss function?\n",
    "\n",
    "4. What are the hyper parameters?\n",
    "\n",
    "5. \n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/v6VJ2RO66Ag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ada Boost\n",
    "> Working\n",
    "- Boosting algorithms improve the prediction power by converting a number of weak learners to strong learners.\n",
    "- In Adaptive boosting, the weights are re-assigned to each instance, with higher weights assigned to incorrectly classified instances\n",
    "- Overview\n",
    "    - To begin with  it builds a model and gives equal weights to all the data points\n",
    "    - then assigns higher weights to points that are wrongly classified\n",
    "    - Now all the points with higher weights are given more importance in the next model\n",
    "    - keep training models until and unless a lower error is received.\n",
    "- Base learner\n",
    "    - The inital models are based on shallow trees which are just stumps based on gini index for each feature. Which ever has the least entropy or impurity or gini index will be selected as a base learner\n",
    "    - Once the stump is made and the predictions are ready, total erro will be calculated by summing up all the weights which were misclassified \n",
    "    - Performance of the model will be calculated using\n",
    "    - <img src =\"../../images/ada_boost_1.png\">\n",
    "    - Now to update the weights, first for the misclassified ones with positive performance in exp\n",
    "    - <img src =\"../../images/ada_boost_2.png\">\n",
    "    - Then for the properly classified ones, with negative performance in exp\n",
    "    - This new updated weights might not add up to 1 unlike the inital weights \n",
    "    - So the new weights are divided by the sum of all new weights to get fractions that will add upto 1\n",
    "    - This will be the normalized weights which will be used to create new dataset for boosting\n",
    "- Boosted dataset\n",
    "    - This will be created by again sampling from orginal dataset\n",
    "    - A random number is selected and based on which bucket it falls on the cumilative normalized weight, that data point is inserted in the dataset\n",
    "    - Since, the weight is high for the misclassified data point, it will get selected more often based on central limit theorem ???\n",
    "    - There by creating a new dataset with empahsis on misclassified data point with the same # of rows as the original\n",
    "    - This is then again used to create base learner like earlier with equal weightage\n",
    "    - The process then repeates till # of base learner = # specified in parameter\n",
    "- For predicting, all the stumps are used and which ever class gets the stumps with high summed up performance will win\n",
    "> Code   \n",
    "    ```python\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    clf.fit(X, y)\n",
    "    clf.predict([[0, 0, 0, 0]])\n",
    "    clf.score(X, y)\n",
    "\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "    regr.fit(X, y)\n",
    "    regr.predict([[0, 0, 0, 0]])\n",
    "    regr.score(X, y)\n",
    "    # Parameters : estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated'\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- Adaboost is that it needs a quality dataset, outliers might disturb it\n",
    "- Slower than XGboost\n",
    "- Computationally expensive\n",
    "- Not good for imbalanced\n",
    "\n",
    "+ Flexible mode\n",
    "+ Adaboost is less prone to overfitting as the input parameters are not jointly optimized\n",
    "+ extended beyond binary classification and has found use cases in text and image classification as well\n",
    "\n",
    "1. How is it learning?\n",
    "\n",
    "2. Where are weight and biases getting updated?\n",
    "\n",
    "3. Whats the loss function?\n",
    "\n",
    "4. What are the hyper parameters?\n",
    "\n",
    "5. https://youtu.be/MIPkK5ZAsms TODO\n",
    "\n",
    "Source \n",
    "- [blog](https://www.analyticsvidhya.com/blog/2021/09 adaboost-algorithm-a-complete-guide-for-beginners/)\n",
    "- [Youtube](https://youtu.be/LsK-xG1cLYA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost\n",
    "> Working\n",
    "- Overview\n",
    "    - Unlike stumps, here small trees with 8-32 leafs are built\n",
    "    - \n",
    "- Steps (regression)\n",
    "    - First an average of the target is taken and is used as predicted\n",
    "    - Next difference b/w the avg and actual target is calulated as a new column (residuals)\n",
    "    - The new column now becomes the target and a small decision tree is built\n",
    "    - If a leaf has two residuals, its averaged\n",
    "    - Now the residuals are predicted and added with the average which might overfit (new target)\n",
    "    - hence a parameter is multiplied with the residual before adding which is the learning rate and is a hyperparameter (0-1)\n",
    "    - Lots of small steps in the right direction by keeping the learning rate small leads to better accuracy\n",
    "    - Now the new target is used to find new residuals again to repeat the process\n",
    "    - Now the average target + lr * tree 1's prediction + lr * tree 2's prediction ...\n",
    "    - Till the difference between the prediction and actual is small or the # of trees has reached specified limit\n",
    "- Intution behind Gradient Descent Regression\n",
    "    - This needs a loss function thats differentiable\n",
    "    - which can be anything as its flexible. Usually a simple regression loss function\n",
    "    0.5(actual - predicted)^2 is taken\n",
    "    - 0.5 is for differentiablity using the chain rule\n",
    "    1. To use the average of the target as a constant value\n",
    "        <img src=\"../../images/grad_boost_1.png>\n",
    "    2. make the first tree which is just a leaf with the average value of the target\n",
    "        - There will be M trees 1/1 where M is a hyperparameter specified, for which the following will be repeated\n",
    "        - <img src=\"../../images/grad_boost_2.png\">\n",
    "        - Find derivative of the loss function (which is just the residual for all the rows of dataset)\n",
    "        - <img src=\"../../images/grad_boost_3.png>\n",
    "        - Now using this residual build a tree to predict new residuals\n",
    "        - Find the avg. values of last leaves of the tree if multiple values in it\n",
    "        - <img src=\"../../images/grad_boost_4.png>\n",
    "        - Add original average of the target with new residuals after multiplying it with the learnign rate and update the target\n",
    "        - <img src=\"../../images/grad_boost_5.png>\n",
    "    3. Output of  step 2 till residual is minimum or the # of trees are reached\n",
    "- Classification overview\n",
    "    - In this since the target is a class and not a numerical value, the log(odds) of the target is taken for each point\n",
    "        -  log(# of class 1/ #of class 2)\n",
    "    -  Logistic function of the log(odds) is also calculated\n",
    "        - <img src=\"../../images/grad_boost_6.png>\n",
    "    - This proability is then set against threshold of 0.5 to find new target constant values which will be 1 of prob is >=0.5 else 0\n",
    "    - New residuals will be calcuated = (observed/the initial constant probability - predicted)\n",
    "    - Build a tree to predict residuals\n",
    "    - In regression, if a leaf contains two values it can be averaged while in classification since it is a log odd's proability, it will be transformed using the following for each leaf\n",
    "    - <img src=\"../../images/grad_boost_7.png>\n",
    "    - once all the values for leaves are calculated, the constant prediction of single leaf will be updated using product of new * learning rate and adding it to the constant value\n",
    "    - This gives a new log(odds) which is used to get new probability using logistic function for each data point\n",
    "    - Now the residuals is calculated (observed - residuals)\n",
    "- Intution behind Gradient Descent Classification\n",
    "    - SKIPPING, too much to handle\n",
    "> Code   \n",
    "    ```python\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "    clf.score\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    reg = GradientBoostingRegressor(random_state=0)\n",
    "    reg.fit(X_train, y_train)\n",
    "    reg.predict(X_test[1:2])\n",
    "    reg.score(X_test, y_test)\n",
    "\n",
    "\n",
    "    # parameters : loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0\n",
    "\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- models can be computationally expensive and take a long time to train, especially on CPUs;\n",
    "- hard to interpret the final models.\n",
    "- overemphasize outliers and cause overfitting\n",
    "- large grid search during hyper tuning.\n",
    "- Less interpretative in nature\n",
    "\n",
    "+ Lots of flexibility - can optimize on different loss functions and provides several hyper parameter tuning options that make the function fit very flexible.\n",
    "+ train faster especially on larger datasets,\n",
    "+ most of them provide support handling categorical features, so no pre-processing required\n",
    "+ some of them handle missing values natively so imputation taken care of\n",
    "\n",
    "1. How is it learning?\n",
    "\n",
    "2. Where are weight and biases getting updated?\n",
    "\n",
    "3. Whats the loss function?\n",
    "\n",
    "4. What are the hyper parameters?\n",
    "\n",
    "5. \n",
    "\n",
    "Source\n",
    "[youtube](https://youtu.be/3CC4N4z3GJc)\n",
    "[youtube](https://youtu.be/2xudPOBz-vs)\n",
    "[youtube](https://youtu.be/StWY5QWMXCw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light Gradient boost\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "1. How is it learning?\n",
    "\n",
    "2. Where are weight and biases getting updated?\n",
    "\n",
    "3. Whats the loss function?\n",
    "\n",
    "4. What are the hyper parameters?\n",
    "\n",
    "5. \n",
    "\n",
    "Source : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xtream Gradient\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "1. How is it learning?\n",
    "\n",
    "2. Where are weight and biases getting updated?\n",
    "\n",
    "3. Whats the loss function?\n",
    "\n",
    "4. What are the hyper parameters?\n",
    "\n",
    "5. \n",
    "\n",
    "Source \n",
    "[youtube](https://www.youtube.com/watch?v=OtD8wVaFm6E)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "1. How is it learning?\n",
    "\n",
    "2. Where are weight and biases getting updated?\n",
    "\n",
    "3. Whats the loss function?\n",
    "\n",
    "4. What are the hyper parameters?\n",
    "\n",
    "5. \n",
    "\n",
    "Source : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolation tree\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "1. How is it learning?\n",
    "\n",
    "2. Where are weight and biases getting updated?\n",
    "\n",
    "3. Whats the loss function?\n",
    "\n",
    "4. What are the hyper parameters?\n",
    "\n",
    "5. \n",
    "\n",
    "Source : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolation tree\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "1. How is it learning?\n",
    "\n",
    "2. Where are weight and biases getting updated?\n",
    "\n",
    "3. Whats the loss function?\n",
    "\n",
    "4. What are the hyper parameters?\n",
    "\n",
    "5. \n",
    "\n",
    "Source : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "> Working\n",
    "- Overview\n",
    "    - Unlike in linear regression hwere the line is drawn considering all the points, here its only based on the points that are at the edges of the classes and closest to each other (support vectors)\n",
    "    - Margin : The distance between the support vectors from the hyper plane\n",
    "    - The aim is to have a hyperplane which has the largest margin from both the extreme points\n",
    "    - This uses maximum margin classifiers, which are suseptible to outliers\n",
    "    - Outliers must not affect the hyperplance so misclassification must be allowed to get the largest margins\n",
    "    - This is the bias variance tradeoff\n",
    "    - Using CV of these margins, the best margin is found and since its misclassifying, its called soft margins\n",
    "    - The points within the soft margin and on the edge are called Support Vector\n",
    "    - But when they fail like in case where class 1 is surrounded by class 2, support vector classifiers are useless\n",
    "    - Then comes support vector machines, which moves the data to higher dimensions\n",
    "        - This adds new dimensions to data which then can be used to find the hyperplane that seperates the seemingly unclassifyable data\n",
    "        - This adding of new dimension can be like squaring of data or root or cube or exp etc has to be considered based on the data\n",
    "        - This moving to higher dimension by transforming data is done using kernel functions\n",
    "        - Kernel : ‘linear’, ‘poly’, radial basis function, ‘sigmoid’, ‘precomputed’\n",
    "        - rbf : finds plane in infinite dimensions bt in 1 d acts as weighted nearest neighbour\n",
    "        - The kernel trick : not transforming the data but acting as if its transformed to find the hyperplane\n",
    "        - This makes computation easy\n",
    "    - Polynomial kernel\n",
    "        - (a x b + r)^d\n",
    "        - a and b are the data points its comparing, r is coefficient of polynomial and d is the polynomial degree passed as parameter\n",
    "        - by passing r and d, new coordinates of the points can be calculated, which are determined using cross validation\n",
    "    - RBF\n",
    "        - e^(-y(a-b)^2)\n",
    "        - y is to be found using CV\n",
    "        - can be expanded to infinity using taylor series\n",
    "> Code   \n",
    "    ```python\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "    clf.fit(X, y)\n",
    "    # parameters :  C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- not suitable for large data sets.\n",
    "- does not perform very well when the data set has more noise\n",
    "- underperforms when number of features for each data point exceeds the number of training data samples\n",
    "- here is no probabilistic explanation for the classification.\n",
    "- Choosing a “good” kernel function is not easy\n",
    "- time consuming\n",
    "- Difficult to understand and interpret the final model, variable weights and individual impact.\n",
    "- not that easy to fine-tune these hyper-parameters. It is hard to visualize their impact\n",
    "\n",
    "+ kernel trick is real strength of SVM for complex hyperplane\n",
    "+ SVM is not solved for local optima.\n",
    "+ the risk of over-fitting is less in SVM.\n",
    "+ works relatively well when there is a clear margin of separation \n",
    "+ more effective in high dimensional spaces.\n",
    "+ effective in cases where the number of dimensions is greater than the number of samples.\n",
    "+ relatively memory efficient\n",
    "\n",
    "1. How is it learning?\n",
    "\n",
    "2. Where are weight and biases getting updated?\n",
    "\n",
    "3. Whats the loss function?\n",
    "\n",
    "4. What are the hyper parameters?\n",
    "\n",
    "5. https://youtu.be/wBVSbVktLIY TODO https://youtu.be/05VABNfa1ds\n",
    "\n",
    "Source : \n",
    "[youtube](https://www.youtube.com/watch?v=efR1C6CvhmE&pp=ygUNc3ZtIGV4cGxhaW5lZA%3D%3D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validations\n",
    "> K fold\n",
    "- .\n",
    "> OOO\n",
    "- .\n",
    "> .\n",
    "- .\n",
    "\n",
    "Source : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalanced \n",
    "> SMOTE\n",
    "- .\n",
    "> Imbalanced-learn\n",
    "- .\n",
    "\n",
    "Source : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "> Confusion Matrix\n",
    "- <img src =\"../../images/conf_mat1.webp\">\n",
    "- <img src =\"../../images/conf_mat.webp\">\n",
    " - Type 1 (False Positive rate) = fp/(fp+tn)\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    false_negative_rate = fn / (tp + fn)\n",
    "    ```\n",
    " - Type 2 (False Negative Rate) = fn/(tp+fn)\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    false_positive_rate = fp / (fp + tn)\n",
    "    ```\n",
    "\n",
    "> Accuracy\n",
    "- (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "> Precision (Positive predict value)\n",
    "- how many of the correctly predicted cases actually turned out to be positive.\n",
    "- False Positive is a higher concern than False Negatives.\n",
    "- ex : music or video recommendation systems, e-commerce websites, etc. Wrong results could lead to customer churn and be harmful to the business.\n",
    "- ex : raising false alerts is costly, \n",
    "- <img src =\"../../images/pre_recall.png\">\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix, precision_score\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    precision = precision_score(y_true, y_pred_class) # or optionally tp/ (tp + fp)\n",
    "    ```\n",
    "\n",
    "> Recall\n",
    "- how many of the actual positive cases we were able to predict correctly with our model.\n",
    "- ex : really care about catching all fraudulent transactions even at a cost of false alerts\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix, recall_score\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    recall = recall_score(y_true, y_pred_class) # or optionally tp / (tp + fn)\n",
    "    ```\n",
    "> False Discovery Rate\n",
    "- measures how many predictions out of all positive predictions were incorrect. (1- precision)\n",
    "- When raising false alerts is costly and when you want all the positive predictions to be worth looking at you should optimize for precision.\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    false_discovery_rate = fp/ (tp + fp)\n",
    "    ```\n",
    "\n",
    "> F1, Threshold\n",
    "- harmonic mean of Precision and Recall, \n",
    "- Used when there is imbalance\n",
    "- Threshold can be found which will give the highest F1\n",
    "- It is maximum when Precision is equal to Recall. Not interpretable\n",
    "- <img src =\"../../images/f1_1.png\">\n",
    "- <img src =\"../../images/f1_2.png\">\n",
    "    - F1 beta\n",
    "        - Higher the beta higher regards to recall\n",
    "        - if beta =1 then its normal f1 score, beta=2 is f2 score and so on\n",
    "        - <img src =\"../../images/f1_3.png\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import fbeta_score\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    f1= f1_score(y_true, y_pred_class)\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    f2 = fbeta_score(y_true, y_pred_class, beta = 2)\n",
    "    ```\n",
    "\n",
    "> Specificity (True negative rate)\n",
    "- the number of correct negative predictions divided by the total number of negatives\n",
    "- where False Negative trumps False Positive.\n",
    "- ex :  important in medical cases where it doesn’t matter whether we raise a false alarm, but the actual positive cases should not go undetected!\n",
    "- ex : really want to be sure that you are right when you say something is safe\n",
    "- <img src =\"../../images/specificity.jpg\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    true_negative_rate = tn / (tn + fp)\n",
    "    ```\n",
    "\n",
    "> Sensitivity also recall also True Positive rate\n",
    "- the number of correct positive predictions divided by the total number of positives\n",
    "- ex : really care about catching all fraudulent transactions even at a cost of false alerts\n",
    "- <img src =\"../../images/sensitivity.jpg\">\n",
    "\n",
    "    ```python\n",
    "    # in recall below\n",
    "    ``` \n",
    "\n",
    "> AUC-ROC\n",
    "- visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart.\n",
    "- ROC score tells us how good our curve is, we can calculate the Area Under the ROC Curve, or ROC AUC score.\n",
    "    ```python\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from scikitplot.metrics import plot_roc\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_roc(y_true, y_pred, ax=ax)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_pos)\n",
    "    ``` \n",
    "\n",
    "> Cohen Kappa\n",
    "- Cohen Kappa tells you how much better is your model over the random classifier that predicts based on class frequencies.\n",
    "- mostly used when imbalance exists\n",
    "- <img src =\"../../images/cohen_kappa.webp\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "    cohen_kappa = cohen_kappa_score(y_true, y_pred_class)\n",
    "    ```\n",
    "\n",
    "> Matthews Correlation Coefficient \n",
    "- Imbalanced dataset and easy to interpret\n",
    "- <img src =\"../../images/mcc_eq.webp\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import matthews\n",
    "- objective function that is optimized under the hood and can be used as metric\n",
    "- <img src =\"../../images/log_loss.webp\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import log_loss\n",
    "\n",
    "    loss = log_loss(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "> Kolmogorov-Smirnov plot\n",
    "- assess the separation between prediction distributions for positive and negative classes\n",
    "- problem is about sorting/prioritizing the most relevant observations and you care equally about positive and negative classes.\n",
    "- Good addition to roc score\n",
    "    ```python\n",
    "    from scikitplot.metrics import plot_ks_statistic\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_ks_statistic(y_true, y_pred, ax=ax)\n",
    "    ```\n",
    "\n",
    "Source\n",
    "[blog](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/#Precision_vs._Recall)\n",
    "[blog](https://neptune.ai/blog/evaluation-metrics-binary-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised\n",
    "#### PCA\n",
    "https://youtu.be/9oSkUej63yk TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "https://medium.com/nlplanet/a-brief-timeline-of-nlp-from-bag-of-words-to-the-transformer-family-7caad8bbba56\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[blog](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/) TODO\n",
    "### Word Embeddings\n",
    "> Working\n",
    "- Aim is to convert words in to numbers to be fed into NN\n",
    "- Random number assignment can be done but that doesn signify the relation between two words like synonyms or opposites\n",
    "- Word embeddings map the words as real-valued numerical vectors. It does so by tokenizing each word in a sequence (or sentence) and converting them into a vector space. Word embeddings aim to capture the semantic meaning of words in a sequence of text\n",
    "- Bag of words : faster, simpler, low memory use\n",
    "- Skip gram : good for small datasets, takes care fo rare words too\n",
    "1. Feq based : TFIDF, count vector, co-occurance vector\n",
    "2. prediction based : word2vec, glove bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "> Working\n",
    "- based on a statistical measure of finding the relevance of words in the text\n",
    "- combination of two metrics: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "- <img src=\"../../images/tf_idf_1.jpg>\n",
    "- <img src=\"../../images/tf_idf_2.jpg>\n",
    "- TF (i) = log (frequency (i,j)) / log (N (j))\n",
    "    - TF score is based on the frequency of words in a document. Words are counted for their number of occurrences in the documents. TF is calculated by dividing the number of occurrences of a word (i) by the total number (N) of words in the document (j).\n",
    "- IDF (i) = log (N (d) / frequency (d,i))\n",
    "    - IDF score calculates the rarity of the words. It is important because TF gives more weightage to words that occur more frequently. However, words that are rarely used in the corpus may hold significant information. IDF captures this information. It can be calculated by dividing the total number (N) of documents (d) by the number of documents containing the word (i).\n",
    "- The log is taken in the above formulas to dampen the effect of large scores for TF and IDF. \n",
    "- The final TF-IDF score is calculated by multiplying TF and IDF scores.\n",
    "> Code   \n",
    "    ```python\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tfidf = TfidfVectorizer()\n",
    "    result = tfidf.fit_transform(string)\n",
    "    tfidf.vocabulary_\n",
    "    result.toarray()\n",
    "\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- It cannot capture the semantic meaning of words in a sequence efficiently.\n",
    "- gives value to rare words there by overfitting\n",
    "- decimal values in vector\n",
    "\n",
    "Source\n",
    "[youtube]()\n",
    "[blog](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "> Working\n",
    "- It can iterate over a large corpus of text to learn associations or dependencies among words.\n",
    "- This is a pre-trained embedding model for freature representation but i can be trained from scratch also\n",
    "- Word2Vec finds similarities among words by using the cosine similarity metric. If the cosine angle is 1, that means words are overlapping. If the cosine angle is 90, that means words are independent or hold no contextual similarity.\n",
    "- two neural network-based variants: Continuous Bag of Words (CBOW) and Skip-gram.\n",
    "    - In CBOW, the shallow neural network model takes various words as input and predicts the target word that is closely related to the context of the input words\n",
    "        - The input layer will have window  size -1 input with vector of adjacent words\n",
    "        - The fully connected hidden layer with be of window size\n",
    "        - Embedding layer ???\n",
    "        - The output layer which predicts 1 word will have 1 layer with vector size neurons and softmax layer\n",
    "        - Or n to n+wth word's vector is taken as input and n+w+1th word's vector is expected as output and the training is done accordingly\n",
    "        - w can be 1 for the very next word or can be more than that\n",
    "        - inital vector representation will be an one hot encoded output\n",
    "        - after training, the vector representation of the words are finalized\n",
    "    - the Skip-gram architecture takes one word as input and predicts its closely related context words\n",
    "        - The input into the NN is the target word and n words to its left and right needs to be predicted\n",
    "        - this n will be the window size    \n",
    "        \n",
    "- CBOW is quick and finds better numerical representations for frequent words, while Skip Gram can efficiently represent rare words\n",
    "- Unlabeled data is trained via artificial neural networks to create the Word2Vec model that generates word vectors. \n",
    "- The size of the vector can be selected according to the corpus size and the type of project\n",
    "- So its not as sparse as the previous ones\n",
    "- \n",
    "> Code\n",
    "    ```python\n",
    "    from gensim.models import Word2Vec\n",
    "    w2v_model = Word2Vec(min_count=4,\n",
    "                         window=4,\n",
    "                         size=300, \n",
    "                         alpha=0.03, \n",
    "                         min_alpha=0.0007, \n",
    "                         sg = 1,\n",
    "                         workers=cores-1)\n",
    "    \n",
    "    w2v_model.build_vocab(w2v_df, progress_per=10000)\n",
    "    w2v_model.train(w2v_df, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n",
    "    return w2v_model\n",
    "    w2v_model.wv.most_similar(positive=[\"boston\"])\n",
    "\n",
    "    #parameters : min_count, window, size, alpha, min_alpha, sg, workers\n",
    "    ```\n",
    "> Pros and cons\n",
    "+ vector size is not large and not sparse\n",
    "+ semantic relations are preserved and can be checked using cosine similarity\n",
    "    - which is the cos of angle between the two word's vectors\n",
    "- Needs to be cleaned and stop words removed\n",
    "- Word2Vec only captures the local context of words.\n",
    "- During training, it only considers neighboring words to capture the context\n",
    "- cant handle rich morpholigical languages like  gender based languages\n",
    "Source\n",
    "[blog](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)\n",
    "[blog](https://towardsdatascience.com/word-embedding-techniques-word2vec-and-tf-idf-explained-c5d02e34d08)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove\n",
    "- Global vectors is combination of predicting and count based word embedding techniques\n",
    "> Working\n",
    "- Rather than using a window to define local context, GloVe constructs an explicit word-context or word co-occurrence matrix using statistics across the whole text corpus. The result is a learning model that may result in generally better word embeddings.\n",
    "- GloVe considers the entire corpus and creates a large matrix that can capture the co-occurrence of words within the corpus\n",
    "- GloVe combines the advantages of two-word vector learning methods: matrix factorization like latent semantic analysis (LSA) and local context window method like Skip-gram.\n",
    "- GloVe technique has a simpler least square cost or error function that reduces the computational cost of training the model\n",
    "- Here window is used to look for words on either side\n",
    "- After word co-occurance is calculated, proability is calculated for each word wrt to another using\n",
    "- <img src = \"../../images/glove_1.png\">\n",
    "- where xij is occurance of i and love in the same window divided by total# of times the word has occured\n",
    "- A neural netwrok architecture is built to optimize the proability ratio objective\n",
    "- SKIPPING, too much maths\n",
    "> Code\n",
    "- .\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "+ GloVe performs significantly better in word analogy and named entity recognition problems\n",
    "+ better than Word2Vec in some tasks and competes in others\n",
    "Source\n",
    "\n",
    "Source\n",
    "[youtube](https://youtu.be/viZrOnJclY0)\n",
    "[youtube]()\n",
    "[blog](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp)\n",
    "[blog](https://neptune.ai/blog/word-embeddings-guide)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fastext\n",
    "> Working\n",
    "- Uses CBoW as Skip grams\n",
    "> Code\n",
    "- .\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "+ \n",
    "\n",
    "Source\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT\n",
    "> Intro\n",
    "- belongs to a class of NLP-based language algorithms known as transformers.\n",
    "- massive pre-trained deeply bidirectional encoder-based transformer model \n",
    "    - Base ; 110m parameters\n",
    "    - Large : 340m\n",
    "- relies on an attention mechanism. It generates high-quality context-aware or contextualized word embeddings.\n",
    "- During the training process, embeddings are refined by passing through each BERT encoder layer.\n",
    "- For each word, the attention mechanism captures word associations based on the words on the left and the words on the right\n",
    "- Word embeddings are also positionally encoded to keep track of the pattern or position of each word in a sentence.\n",
    "- BERT can be improved by fine-tuning the embeddings on task-specific datasets.\n",
    "> Working\n",
    "- https://youtu.be/xI0HHN5XKDo\n",
    "- https://youtu.be/7kLi8u2dJz0\n",
    "- https://youtu.be/OR0wfP2FD3c\n",
    "- https://youtu.be/mw7ay38--ak\n",
    "> Code\n",
    "- .\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "+ \n",
    "Source\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL\n",
    "### Working\n",
    "https://youtu.be/GRRMi7UfZHg GPU in NN TODO  https://youtu.be/EKD1kEMNeeU\n",
    "https://youtu.be/bOIPwdWso_0 TODO regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few shot learning\n",
    "https://youtu.be/VqPmrYFvKf8 TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One shot learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero shot learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaseme N/w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "> https://youtu.be/DtEq44FTPM4 TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "https://youtu.be/m8pOnJxOcqY TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "https://youtu.be/yZv_yRgOvMg TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "https://youtu.be/QciIcRxJvsM TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational ENcoders\n",
    "https://youtu.be/fcvYpzHmhvA TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "#### Intro\n",
    "> https://youtu.be/kCc8FmEb1nY : Let's build GPT: from scratch, in code, spelled out - Andrej [colab](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)\n",
    "- Builds a GPT with shakespear data\n",
    "- Google uses this for sub words or byte pair encoding of words which are input [github](https://github.com/google/sentencepiece)\n",
    "- OpenAI uses this for sub words [github](https://github.com/openai/tiktoken)\n",
    "- The encoders takes input words, convert them into sub words and then translates them into embeddings\n",
    "\n",
    "    <img src=\"../../images/gpt_1.png\">\n",
    "    \n",
    "- Embedded seq length vs sub words vocabulary size trade off\n",
    "- Encode the whole of datset\n",
    "- Split it into train and test 90-10 in this case\n",
    "- The training data is chucked into block sizes of fixed length to train the n/w\n",
    "- The training occurs by predictig the next word so block_size +1 inputs are sent <- context length\n",
    "\n",
    "    <img src=\"../../images/gpt_2.png\">\n",
    "\n",
    "- By having small to big input the context is understood\n",
    "- Multiple of these chuncks are processed at the same time using GPU for faster computation <- batch size\n",
    "- Self attention\n",
    "    - SKIPPING , the desi guy explains better below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://www.youtube.com/watch?v=rPFkX5fJdRY : [ 100k Special ] Transformers: Zero to Hero\n",
    "- History\n",
    "    - RNN : Feedforward NN rolled out over time\n",
    "        - Deals with sequence data with order and gave raise to architectures like\n",
    "        - Vector to sequence model, which takes in input of fixed size and give output of any length, ex: image captioning\n",
    "        - Sequence to vector model, does the reverse, ex : sentiment analysis\n",
    "        - sequence to sequence model, is more popular as i/p and o/p are not fixed ex : lang translation\n",
    "        - But they are very slow to train, even after using truncated back progpagation through time\n",
    "        - Also during processing of long seq, there is a vanishing/exploding gradient issue\n",
    "    - LSTM : Are RNNs with Long-short term memory cell for neurons with activation function\n",
    "        - Allows past info to move on to the next cell but increases complexity\n",
    "        - There by retaining memory for long seq i.e. 10x of RNN\n",
    "        - But after this the trianing is even slower\n",
    "        - Since the flow of info is sequential and there is a dependency b/w past and next data, GPUs also wont help\n",
    "        - Also, there a doubt about the context capturing as the vector generated for each word depended on just the previous word\n",
    "    - Bi-directional RNN\n",
    "        - To counter previous only dependency, both previous and next were taken into account\n",
    "        - and concatenate the context\n",
    "        - But the context was not taken as a whole it was previous and after sepearated and then combined\n",
    "        - There by missing the overall context which can be taken if both are considered\n",
    "    - To solve the issue of parallaizing processing of sequential data Transformers were created\n",
    "    - It has encoder decoder architecture like RNNs but the input could be sent parallely\n",
    "    - Where RNN had to take in one word at a time for embedding and there were hidden state dependencies between the past nd the current input, Transformers took it all in one go\n",
    "    \n",
    "    <img src=\"../../images/transformers_1.png\">\n",
    "-  Transformer Architecture \n",
    "    - There are two inputs, one is the sentence to be translated and the other is translated sentence to train the n/w\n",
    "    - X_train goes into Input embeddin (encoder) and y_train into output embedding (decoder)\n",
    "    - Encoder block\n",
    "        - Input embeddings\n",
    "            - Since words are not numerical, they need to be converted into embeddings\n",
    "            - Such that they retain semantic meaning and not random numeric allotment\n",
    "            - This means in the embedding space, the words that relate must be closer than unrelated ones\n",
    "            - This space can be created from scratch for customized data or use existing pre-trianed model like word2vec or glove which was trained on internet\n",
    "            - The output of this will be a unique vector which would represent the word in space\n",
    "            - But sometimes, the words mean different in different context like a bank, which can be an institution and a river bank too\n",
    "        - Positional Encoders\n",
    "            - To solve the issue of same word having different meaning, a vector is created which will look at the position of the word in a sentence\n",
    "            - The original paper uses sin and cosine function to generate the vector with positional awareness\n",
    "            - The output is the embedding of word with contextual information\n",
    "            - In the paper the positional vector size is 512 diemnsions\n",
    "        - Encoder Block\n",
    "            - It has a multi headed attention layer and a feed fwd layer\n",
    "            - Attention :  which part of the input should receive focus\n",
    "                - This is used to create a vector for each word which shows how imp is the word wrt to other words in the i/p sentence\n",
    "                - ???\n",
    "            - Feed fwd net : Applied to each attention vector. So 1 n/w for each word, to convert each attention vector into a form that can be taken by the next encoder/decoder block\n",
    "            - The goal is to make attention vector more contextually aware than positional vectors\n",
    "    - Decoder block\n",
    "        - similar to encoder, it takes sentence, turns it into embedding and for context uses positional embedding \n",
    "        - Now starts the decoder block with a Masked-multi-headed attention, a multi headed attention and feed fwd net\n",
    "        - Here MMHA receives the output potisionla embedding which the MHA gets both output of MMHA and the output of previous Encoder block\n",
    "        - First the output vector is passed through attention to get attention vector\n",
    "        - Then the output, positional, attention vector is compared with the output of input encoder i.e. input positional attention vector in the encoder-decoder attention block\n",
    "        - The E-DA block will determine how related each vector of word from input is related to each vector of word from output\n",
    "        - This is where word mapping happens for translation scenario\n",
    "        - The o/p of this block is attention vector for each word of input and output\n",
    "        - This is then passed on to feed fwd n/w to make the vectors more digestible for next linear or decoder block\n",
    "        - Linear layer is another feed fwd network, which changes the dimensions of the output into # of words in the translted language\n",
    "        - Then the softmax layer changes vector into prob. distribution\n",
    "        - The final o/p is the word whocse vector gets the highest probability form softmax\n",
    "        - Which is the prediction of the next word\n",
    "        - This repeats entill the <EOS> is generated\n",
    "- 08:42\n",
    "    - The Encoder's Multi head attention block\n",
    "        - If a word needs to be rated based on its importance in the sentence, it will rank itself highest, wrt to other words, which might not be useful to find which word needs to be focused up on (give attention to)\n",
    "        - So 8 attention vectors are formed for eac word and averaged out ??? why cant they all be the same\n",
    "        - Since multiple vectors are generated to avgd out, its multi headed\n",
    "    - Encoder's feed fwd n/w\n",
    "        - This is where ||ly attention vectors for each word are sent to condense \n",
    "    - Decoder's MMHA\n",
    "        - Masked because here, training uses all the words from input but only the previous word from o/p to predict next word\n",
    "        - This is done by making the matrix value of hte other words in a sentence 0\n",
    "- 11:11\n",
    "    - MHA details\n",
    "        - The input to this layer are vectors which extacts different compoenent of input word in this case denoted by V K and Q\n",
    "        - This is used to calculate scale-dot product attention vector \n",
    "        - Since it is multiheaded all these vectors become matrices\n",
    "        - And the output is also attension vectors for each word but the next feed fwd layer expects just one vector so the output also gets weighted\n",
    "        <img src=\"../../images/transformers_3.png\">\n",
    "    - Add and Normalization layer\n",
    "        - Batch norm is across each sample but layer normalization, used here, is across each feature\n",
    "        - For optimization a with larger learning rate\n",
    "        <img src=\"../../images/transformers_2.png\">\n",
    "- 12:30\n",
    "    - Attention Mechanism\n",
    "        - Due to issues in RNN, BRNN of loss of context in vector generation\n",
    "        - Attention was used\n",
    "            - Where each word's focus, wrt other words in the sentence is found ??? using correlation how\n",
    "            - Since the sentence is focusing on itself its self-attention\n",
    "            - The output of a sentence from the encoder block is # of attention vectors = # of words\n",
    "            - but the way words are processed are by converting them into sub-words or word pieces or byte pair encoding\n",
    "            - This is sent into decoder while the actual input to decoder is a token <start>\n",
    "            - The output of the decoder will be the next word it predicts which will be again used as input for the decoder block to generate the second next word and so on till<stop> is generated\n",
    "    - Since its not serial unlike RNN, GPUs can be used to speed up the process\n",
    "    - The quality of the attenion vectors are higher than those created by previous word context in RNN \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working\n",
    "> 19:06 Multi-Head Attention mechanism\n",
    "- <img src=\"../../images/transformers_4.png\">\n",
    "- <img src=\"../../images/transformers_6.png\">\n",
    "    - Each word thats is put through the MHA layer is divided into 3 vectors, ??? how do we get them\n",
    "        - Q : query : what to look for \n",
    "        - K : key : what to offer\n",
    "        - V : value : what can it offer\n",
    "    - The length of each Q K and V is the lenght of the sentence\n",
    "    - so for a sentence with 4 words, there will be 3*4*dk, dk can vary\n",
    "    - For self attention calculation for each word, where M is the masked vector which i 0 if MHA and lower unity triangular matrix if MMHA\n",
    "        - <img src=\"../../images/transformers_5.png\">\n",
    "        - sqr root of dimenation of q or k is used to minimize variance and stabalize Q.KT hence scaled\n",
    "        - this gives attention vector\n",
    "    - Decoder   \n",
    "        - For masking a triangular matrix (sentence length X sentence length) with bottom as 1 and top as 0 is created to prevent training to know the predicted word\n",
    "        - Now 1 is made a 0 and 0 as -infinity. so that when added with the softmax attention output it gets lower triangle populated with actual attention from encoder\n",
    "        - infitnity is useful as the combination of attention vector from encoder and masked vector from decoder is sent to softmax\n",
    "        - This will convert the vector into probability distribution\n",
    "        - Now, as per the diagram V is multipled with wht eoutput of softmax\n",
    "- This is done multiple times for multiple attention vectors and stack them\n",
    "- 26:45\n",
    "    - Code for multi head attention SKIPPING, more of less same except tensored\n",
    "    - .\n",
    "- 39:30 Positional encoders\n",
    "    - The input sentence is first broekn into sub words ??? and converted into a fixed size by padding and then each word is one hot encoded\n",
    "    - The length of the sentence is fixed as vocab size\n",
    "    - These vectors go into feed fwd network, where each one hot encoded vector will be mapped to 512 dimension vector and the parameters (vocab size * 512) are learned by back propagation ??? without any target\n",
    "    - To this positional encoding is added of the same 512 size\n",
    "    - This is based on functions like sin and cos for each vector\n",
    "    - <img src=\"../../images/transformers_8.png\">\n",
    "        - dmodel here is 512\n",
    "        - Since sin and cos are repeated in a curve, the attention it can give to words after a fixed distance is better\n",
    "        - the range lies between -1 and 1 for both sin and cos\n",
    "    - This gives another set of vectors which are in 512 dimensions X'\n",
    "    - For each vector Q,K, and V are to be generated which will be 512*512 via learnable Weights of Q,K and V\n",
    "    - The output will be 3*max seq lenght * 512\n",
    "    - These are then turned in to multiple heads and do MHA\n",
    "    - <img src=\"../../images/transformers_7.png\">\n",
    "- 48:50\n",
    "    - Layer normalization : addition and normalizing layers\n",
    "    - After the QKV vectors are formed they are split into 8 heads \n",
    "    - Each goes to seperate attenion blocks\n",
    "    - Now the 19:06 MHA occurs\n",
    "    - The output will be attention martices which will be seq length* seq length\n",
    "    - This is multipied by corresponding head's value vector which is seq lenght * 64. To get how much importance should be paid on other words\n",
    "    - These will be 8 seq length * 64 vectors which will be concatenated\n",
    "    - This is so far till here <img src=\"../../images/transformers_9.png\">\n",
    "    - Add and norm\n",
    "        - Has two input one from above as well as direct input from positional encoders\n",
    "        - This is a residual input so as to not let information lose since there are so many n/ws\n",
    "        - This prevents vanishing gradient during back propagation\n",
    "        - Layer Normalization (X' + output of MHA) is the formula\n",
    "        - X' is just before the vectors are sent via weights of QKV\n",
    "    - Layer Normalization\n",
    "        - Normalization reduces the values in a range with mean 0\n",
    "        - stable training in back propagation, even steps and faster to optimize\n",
    "        - Layer norm is for activation functions of each layer in NN with mean as 0 and std dev as 1\n",
    "        - Its has 2 learnable parameters gamma and beta\n",
    "        - <img src=\"../../images/transformers_10.png\">\n",
    "- 01:00:53 Recap by blowing it up\n",
    "    - [github](https://github.com/ajhalthor/Transformer-Neural-Network/tree/main)\n",
    "    - <img src=\"../../images/transformers_0.png\">\n",
    "    - SKIPPING as it repeats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types\n",
    "##### GPT\n",
    "> Working\n",
    "- https://youtu.be/3IweGfgytgY - GPT - Explained!\n",
    "    - Transformers will need a lot of training data for anguage translation\n",
    "    - This can be made easy with less data using transfer learning\n",
    "    - Both the endocder and the decoder can learn a good representation of language\n",
    "    - So multiple of each are stacked one/one to form a Biderectional encoder representation of transformer\n",
    "\n",
    "        <img src=\"../../images/gpt_3.png\"> \n",
    "\n",
    "    - Transfer learning\n",
    "        - A model is pretrained to get parameters close to desired outcome and then fine tuned to perfect it for new needs\n",
    "        - Pretrained is a language model (next word in a sentence predictor, using a self supervised task)\n",
    "        - Fine tuning is like training it for text translation, summarization etc.  (this will be supervised but with less data as it already knows whats a language)\n",
    "\n",
    "        <img src=\"../../images/gpt_4.png\"> \n",
    "\n",
    "    \n",
    "            - Like college grad and then train for specific job via internship\n",
    "        - Issues\n",
    "            - but the issue is still a lot of data is required ~100K (humans need less)\n",
    "            - Since the fine tuning has less data it can overfit the pretained one\n",
    "            - distribution of both the task must be the same if not then if wont train well\n",
    "        - Solved using metalearninig in GPT 2\n",
    "            - Pre-training + zero shot learing\n",
    "            - but zero shot needs to have a lot more parameters in Billions\n",
    "        - To make it better the architecture was made into 175 B parameters plus meta learning\n",
    "            - This also could have one-shot\n",
    "            - few shot learning\n",
    "\n",
    "\n",
    "        <img src=\"../../images/gpt_5.png\"> \n",
    "\n",
    "    \n",
    "        <img src=\"../../images/gpt_6.png\"> \n",
    "\n",
    "    \n",
    "        <img src=\"../../images/gpt_7.png\"> \n",
    "\n",
    "    - Fine tuning is still beter than meta learning as used in GPT 3.5\n",
    "\n",
    "- https://youtu.be/6i0xss-Vo2s - ChatGPT\n",
    "    - First a language model is fine tuned with labled dataset\n",
    "    - Then A human labeller ranks the output in likert scale\n",
    "    - using reward model this is again fine tuned\n",
    "    - Refinforement learning is again done on this to get good outputs\n",
    "    - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BERT\n",
    "https://youtu.be/xI0HHN5XKDo TODO\n",
    "\n",
    "https://youtu.be/TLPmlVeEf1k TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLMS\n",
    "- https://youtu.be/zjkBMFhNj_g - Andrej\n",
    "- https://www.youtube.com/watch?v=Pft04KLw5Lk : Emerging application\n",
    "- https://www.youtube.com/watch?v=C0ZUdFg-iTo : AI Engineering 101\n",
    "\n",
    "Source\n",
    "Youtube : Codemporium "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud formation\n",
    "#### Parts (designer, CDK)\n",
    "> Designer : \n",
    "- https://youtu.be/AkihqHHWHvs\n",
    "    - \n",
    "\n",
    "#### Working\n",
    "#### Code\n",
    "#### Code anatomy\n",
    "#### Videos\n",
    "    - mlops in aws https://www.youtube.com/watch?v=BpF05ncMh60&pp=ygUVY2xvdWQgZm9ybWF0aW9uIG1sb3Bz\n",
    "    - Advanced playlist : https://www.youtube.com/watch?v=fc6tfw2tcGE&list=PL5KTLzN85O4LNGYy-dm1wJ-sKE5l4b5P5\n",
    "    - CF only : https://www.youtube.com/playlist?list=PL_OdF9Z6GmVb1r0U0AvbfuqdQu_mGhXfp\n",
    "    - Python AWS CDK : https://www.youtube.com/watch?v=QibPrs1LOYY&pp=ygUVY2xvdWQgZm9ybWF0aW9uIG1sb3Bz\n",
    "    - CFT : https://www.youtube.com/watch?v=ov4WmWgQgsA&pp=ygUVY2xvdWQgZm9ybWF0aW9uIG1sb3Bz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[github](https://github.com/aws/amazon-sagemaker-examples)\n",
    "#### Parts\n",
    "> architecture : https://youtu.be/UnAN35gu3Rw\n",
    "- <img src=\"../../images/mlops_1.png\">\n",
    "1. CodeCommit\n",
    "2. ECR\n",
    "3. Sagemaker\n",
    "    - Pipeline\n",
    "    - Model Registry\n",
    "4. CodePipeline\n",
    "5. EventBridge\n",
    "6. Endpoint\n",
    "7. Lambda (check in setup)\n",
    "8. API Gateway (check in setup)\n",
    "9. Model Monitor\n",
    "10. Batch transform\n",
    "11. Feature store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "> https://youtu.be/stD47vPDadI - setup with lambda and API gateway\n",
    "- Sagemaker\n",
    "    - After training, the model must be saved as a binary file and also must be used for inferencing\n",
    "    - i.e. Model registry and storage & endpoint\n",
    "    - needs boto3, sagemaker.amazon.common\n",
    "    - Initialize to read things from s3\n",
    "    - After splitting, save train test and validate data in s3 using\n",
    "        ```python\n",
    "        validation_file = \"linear_validation.data\"\n",
    "        f = io.BytesI0()\n",
    "        smac.write_numpy_to_dense_tensor(f, val_X.astype(\"float32\"), val_y.astype(\"float32\")) f.seek(@)\n",
    "        boto3.Session().resource(\"s3\") .Bucket (bucket) .Object(\n",
    "        os.path.join(prefix, \"validation\", validation_file) ).upload_fileobj(f)\n",
    "        ```\n",
    "- Lambda\n",
    "    - Triggers end point with a task and get back an output\n",
    "    - but lambda doesn interface with the end user\n",
    "- API Gateway\n",
    "    - Using rest api, user's input can be taken in\n",
    "    - Sends it to lambda\n",
    "    - And gets output from it for user\n",
    "- SKIPPING, too AWS like code not the regular one [documentation](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video 1\n",
    "> https://youtu.be/stB-F6jswno : new sagemaker studio\n",
    "- Sage maker space : multiple instance, scalable at will, shareable, co-editable, run them in parallel\n",
    "    - Now has Open source VS code with AWS toolkit\n",
    "    - IDE in Application gallery\n",
    "    - Exp tracking and pipeline building in ML tools\n",
    "    - \n",
    "- Juypter lab and VS Code\n",
    "    - Powered by code wispher to make things faster\n",
    "    - Chatbot is there for suggestions\n",
    "    - Foundation model hub with JumpStart has a list of open source models\n",
    "    - Changes python in SDK\n",
    "        - @remote decorator to directly run code in aws from local\n",
    "        - @step decorator to build functions for pipeline\n",
    "        - @??? Schedule notebook as jobs\n",
    "        - pipeline of notebooks as a DAG\n",
    "        - deploy models in 2 steps\n",
    "            - model.build a model to make an high throughput and low latency endpoint and understand requierments, make a code space, find best instance, get input and send output in rest API\n",
    "- SG Distribution\n",
    "    - ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demo\n",
    "    ```python\n",
    "        import os\n",
    "        # Set path to config file for the SageMaker Python SOK\n",
    "        os. environ [\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()\n",
    "        # Thanks to the SageMaker Distribition the most popular ML frameworks and libraries are pre-intalled in the environment\n",
    "        import pandas\n",
    "        import xgboost\n",
    "        import sklearn\n",
    "        import torch\n",
    "        print(f\"Pandas version: {pandas.__version_}\")\n",
    "        print(f\"XGBoost version: {xgboost.__version__}\")\n",
    "        print(f\"SKLearn version: {sklearn.__version__}\")\n",
    "\n",
    "        # down load data from website\n",
    "        import urllib\n",
    "        import os\n",
    "        input_data_dir = 'data/'\n",
    "        if not os.path.exists(input_data_dir):\n",
    "            os.makedirs(input_data_dir)\n",
    "        input_data_path = os.path.join(input_data_dir, 'predictive_maintenance_raw_data_header.csv')\n",
    "        dataset_url = \"https://archive. ics.uci.edu/ml/machine-learning-databases/0@601/ai4i2020. csv\"\n",
    "        urllib. request.urlretrieve(dataset_url, input_data_path)\n",
    "\n",
    "        # !!! comment can be turned into code\n",
    "        import joblib\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "        from sklearn.compose import ColumnTransformer\n",
    "        from sklearn.exceptions import DataConversionWarning\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import precision_score\n",
    "        from sklearn.metrics import recall_score\n",
    "        from sagemaker.remote_function import remote\n",
    "\n",
    "        # this is the newly added decorator to make this function run on aws form local with cli established\n",
    "        @remote (keep_alive_period_in_seconds=3600)\n",
    "        def preprocess(df):\n",
    "                columns = ['Type', 'Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'Machine failure\n",
    "                cat_columns = ['Type']\n",
    "                num_columns = ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']\n",
    "                target_column = 'Machine failure'\n",
    "                df = df(columns)\n",
    "                training_ratio = 0.8\n",
    "                validation_ratio = 0.1\n",
    "                X = df.drop(target_column, axis=1)\n",
    "                y = df[target_column)\n",
    "                print(f'Splitting data training ({training_ratio}), validation ({validation_ratio}), and test ({test_ratio}) sets ')\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=0, stratify=y)\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_ratio/(validation_ratio+training_ratio), random_state=2,\n",
    "                # Apply transformations \n",
    "                transformer = ColumnTransformer(transformers=(('numeric', StandardScaler(), num_columns), ('categorical', OneHotEncoder(), cat_columns)], remainder='passthrough')\n",
    "                featurizer_model = transformer. fit(X_train)\n",
    "                X_train = featurizer_model.transform(X_train)\n",
    "                X_val = featurizer_model.transform(X_val)\n",
    "                y_train = y_train. values. reshape(-1)\n",
    "                y_val = y_val.values. reshape(-1)\n",
    "                print(f'Shape of train labels after preprocessing: {y_train.shape}')\n",
    "                print(f'Shape of validation labels after preprocessing: {y_val.shape}')\n",
    "                print(f'Shape of test labels after preprocessing: {y_test.shape}')\n",
    "                model_file_path=\"/opt/ml/model/sklearn_model. joblib\"\n",
    "                os.makedirs(os.path.dirname(model_file_path), exist_ok=True)\n",
    "                joblib.dump(featurizer_model, model_file_path)\n",
    "                returs X_train, y_train, X_val, y_val, X_test, y_test, featurizer_model\n",
    "\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test, featurizer_model = preprocess(df)\n",
    "\n",
    "        # model training\n",
    "        import os\n",
    "        import shutil\n",
    "        import xgboost\n",
    "        import numpy as np\n",
    "        from sagemaker.remote_function import remote\n",
    "\n",
    "\n",
    "        # function returns a trained model\n",
    "        #@remote (keep_alive_period_in_seconds=36006)\n",
    "        def train(X_train, y_train, X_val, y_val, eta=0.1, max_depth=2, gamma=0.0, min_child_weight=1, verbosity=0, objective='binary: logistic', eval_metric='auc', num_boost_round=5):\n",
    "            print('Train features shape: {}'.format(X_train. shape) )\n",
    "            print('Train labels shape: {}'.format(y_train.shape))\n",
    "            print('Validation features shape: {}'.format(X_val.shape))\n",
    "            print('Validation labels shape: {}'. format(y_val.shape))\n",
    "            # Creating DMatrix(es)\n",
    "            dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "            dval = xgboost.DMatrix(X_val, label=y_val)\n",
    "            watchlist = [(dtrain, \"train\"), (dval, \"validation\")]\n",
    "            print('')\n",
    "            print (f'===Starting training with max_depth {max_depth}===')\n",
    "            param_dist = { \"max_depth\": max_depth, \"eta\": eta, \"gamma\": gamma, \"min_child_weight\": min_child_weight, \"verbosity\": verbosity, \"objective\": objective, \"eval_metric\": eval_metric\n",
    "            }\n",
    "            xyb = xgboost.train( params=param_dist, dtrain=dtrain, evals=watchlist,num_boost_round=num_boost_round)\n",
    "            predictions = xgb.predict(dval)\n",
    "            print (\"Metrics for validation set\")\n",
    "            print('')\n",
    "            print (pd.crosstab(index=y_val, columns=np.round(predictions), rownames=['Actuals'], colnames=['Predictions'], margins=True))\n",
    "            print('')\n",
    "\n",
    "            rounded_predict = np.round(predictions)\n",
    "            val_accuracy = accuracy_score(y_val, rounded_predict) val_precision = precision_score(y val, rounded_predict)\n",
    "            val_recall = recall_score(y_val, rounded_predict)\n",
    "            print(\"Accuracy Model A: %.2f%%\" % (val_accuracy * 100.@))\n",
    "            print(\"Precision Model A: %.2f\" % (val_precision)) print(\"Recall Model A: %.2f\" % (val_recall))\n",
    "\n",
    "            from sklearn.metrics import roc_auc_score\n",
    "            val_auc = roc_auc_score(y_val, predictions)\n",
    "            print(\"Validation AUC A: %.2f\" % (val_auc))\n",
    "            model_file_path=\"/opt/ml/model/xgboost_model.bin\"\n",
    "            os.makedirs(os.path.dirname(model_file_path), exist_ok=True)\n",
    "            xgb.save_model(model_file_path)\n",
    "            return xgb \n",
    "\n",
    "    eta=0.3\n",
    "    max_depth=8\n",
    "    booster = train(X_train, y_train, X_val, y_val, eta=eta, max_depth=max_depth)\n",
    "\n",
    "\n",
    "    # testing the model\n",
    "    #@remote(keep_alive_period_in_seconds=60@)\n",
    "    def test(featurizer_model, booster, X_test, y_test):\n",
    "        X_test = featurizer_model.transform(X_test)\n",
    "        y_test = y_test.values. reshape(-1)\n",
    "        dtest = xgboost.DMatrix(X_test, labelzy_test)\n",
    "        test_predictions = booster.predict(dtest)\n",
    "        print (\"===Metrics for Test Set===\")\n",
    "        print('')\n",
    "        print (pd.crosstab(indexsy_test, columns=np.round(test_predictions), rownames=['Actuals'],\n",
    "        colnames=['Predictions'], margins=True) )\n",
    "        print('*')\n",
    "        rounded_predict = np.round(test_predictions)\n",
    "        accuracy = accuracy_score(y_test, rounded_predict)\n",
    "        precision = precision_score(y_test, rounded_predict)\n",
    "        recall = recall_score(y_test, rounded_predict)\n",
    "        print('')\n",
    "        print(\"Accuracy Model A: %.2f%%\" % (accuracy * 100.0))\n",
    "        print(\"Precision Model A: %.2f\" % (precision))\n",
    "        print(\"Recall Model A: %.2f\" % (recall))\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        auc = roc_auc_score(y_test, test_predictions)\n",
    "        print(\"AUC A: %.2f\" % (auc))\n",
    "\n",
    "    test(featurizer_model, booster, X_test, y_test)\n",
    "    ```\n",
    "    - Notebooks can be scheduled from the notebook calender icon\n",
    "    - Or it can be run with the @remote decorator \n",
    "        - Jobs will get reflected in the jobs section of the space\n",
    "        - This will automatically create a docker environment using SG distribution to run the job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deployment\n",
    "    - needs a deploy.py file\n",
    "    \n",
    "    ```python\n",
    "        import io\n",
    "        import os\n",
    "        import sys \n",
    "        import json \n",
    "        import joblib \n",
    "        import subprocess\n",
    "        import xgboost \n",
    "        import sklearn \n",
    "        import numpy as np \n",
    "        import pandas as pd\n",
    "        import boto3 3\n",
    "        from sagemaker import get_execution_role\n",
    "        from sagemaker.s3 import S3Downloader\n",
    "        from sagemaker.pipeline import PipelineModel\n",
    "        from sagemaker import ModelMetrics, MetricsSource\n",
    "        from sagemaker.s3_utils import s3_path_join\n",
    "        from sagemaker.utils import unique_name_from_base\n",
    "        from sagemaker.image_uris import retrieve as get_image_uri\n",
    "        from sagemaker.deserializers import NumpyDeserializer\n",
    "        from sagemaker.predictor import Predictor\n",
    "        from sagemaker.serve import Modelserver\n",
    "        from sagemaker.serve import InferenceSpec\n",
    "        from sagemaker.serve. builder.model_builder import ModelBuilder\n",
    "        from sagemaker.serve. builder.schema_builder import SchemaBuilder\n",
    "        from sagemaker.serve import CustomPayloadTrans lator\n",
    "\n",
    "       \n",
    "\n",
    "        def load_models(sklearn_job_name, xgboost_job_name):\n",
    "            subprocess.call(['rm', '-rf', 'sklearn_model/'])\n",
    "            subprocess.call(['rm', '-rf', 'xgboost_model/'])\n",
    "            sagemaker_client = boto3.client(\"sagemaker\")\n",
    "            response = sagemaker_client.describe_training_job(TrainingJobName=sklearn_job_name)\n",
    "            sklearn_s3_model_artifacts = response[\"ModelArtifacts\"] [\"S3ModelArtifacts\"]\n",
    "            response = sagemaker_client. describe_training_job(TrainingJobName=xgboost_job_name)\n",
    "            xgboost_s3_model_artifacts = response[\"ModelArtifacts\"] [\"S3ModelArtifacts\"]\n",
    "\n",
    "            print(S3Downloader.download(sklearn_s3_model_artifacts, \"sklearn_model/\"))\n",
    "            print (S3Downloader.download(xgboost_s3_model_artifacts, \"xgboost_model/\"))\n",
    "            subprocess.call(['tar', '-xvzf', 'sklearn_model/model.tar.gz', '-C', 'sklearn_model/'])\n",
    "            subprocess.call(['rm', 'sklearn_model/model,tar.gz'])\n",
    "            subprocess.call(('tar', '-xvzf', 'xgboost_model/model.tar.gz'', '-C', 'xgboost_model/'})\n",
    "            subprocess.call(['rm', 'xgboost_model/model.tar.gz'])\n",
    "            featurizer = joblib.load( 'sklearn_model/sklearn_model. joblib')\n",
    "            booster = xgboost.Booster()\n",
    "            booster. load_model( ' xgboost_model/xgboost_modets bin')\n",
    "            return featurizer, booster\n",
    "\n",
    "        def build_sklearn_sagemaker_model(role, featurizer):\n",
    "            feature_columns_names = ['Type', 'Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]}\"]\n",
    "            class SklearnRequestTranslator (CustomPay loadTranslator):\n",
    "                # This function converts the payload to bytes - happens on client side\n",
    "                def serialize_payload_to_bytes(self, payload: object) -> bytes:\n",
    "                        return payload.encode(\"utf-8\");\n",
    "                # This function converts the bytes to payload - happens on server side\n",
    "                def deserialize_payload_from_stream(self, stream) -> pd.DataFrame:\n",
    "                    df = pd. read_csv(io.Bytes10(stream.read()), header=None)\n",
    "                    df.columns = feature_columns_names\n",
    "                    return df\n",
    "            class SklearnModelSpec(InferenceSpec):\n",
    "                def invoke(self, input_object: object, model: object): \n",
    "                    features = model. transform(input_object)\n",
    "                    return features\n",
    "                def load(self, model_dir: str):\n",
    "                    model_path = model_dir+'/sklearn_model. joblib'\n",
    "                    print(model_path)\n",
    "                    model = joblib. load(model_path)\n",
    "                    return model\n",
    "            schema_builder=SchemaBuilder(\n",
    "                sample_input=\"L, 298.4, 308.2,1582,70.7,216\",\n",
    "                sample_output=np.array( (0.647088, 0.467287 ,-0.191472,@.720195,-0.536976,\n",
    "                _ input_translator=SklearnRequestTranslator())))  \n",
    "\n",
    "            model_file_path=\"sklearn_model/sklearn_model. joblib\"\n",
    "            os.makedirs(os.path.dirname(model_file_path), exist_ok=True)\n",
    "            joblib.dump(featurizer, model_file_path)\n",
    "\n",
    "            model_builder = ModelBuilder( model_path=\"sklearn_model/\",\n",
    "                name=\"sklearn_featurizer\", \n",
    "                dependencies={\"requirements\": \"requirements_inference. txt\"}\n",
    "                image_uri=get_image_uri(framework=\"sklearn\", region=\"us-west-2\", version=\"1.2-1\"), schema_builder=schema_builder,\n",
    "                model_server=Mode1Server.TORCHSERVE,\n",
    "                inference_spec=SkLearnModelSpec(),\n",
    "                role_arn=role)\n",
    "            return model_builder.build()\n",
    "\n",
    "\n",
    "        def build_xgboost_sagemaker_model(role, booster):\n",
    "            class RequestTranslator(CustomPay loadTrans lator): \n",
    "                # This function converts the payload to bytes - happens on client side\n",
    "                def serialize_payload_to_bytes(self, payload: object) -> bytes:\n",
    "                    return self._convert_numpy_to_bytes (payload)\n",
    "                # This function converts the bytes to payload - happens on server side\n",
    "                def deserialize_payload_from_stream(self, stream) -> xgboost.DMatrix:\n",
    "                    np_array = np. load(io.BytesI0(stream. read())).reshape((1, -1))\n",
    "                    dmatrix = xgboost.DMatrix(np_array)\n",
    "                    return dmatrix\n",
    "\n",
    "                def _convert_numpy_to_bytes(self, np_array: np.ndarray) -> bytes:\n",
    "                    buffer = io.BytesI0()\n",
    "                    np.save(buffer, np_array)\n",
    "                    return buffer.getvalue()\n",
    "\n",
    "                schema_builder=Schenabulider(\n",
    "                sample_input=np.array ( [@.647088,0.467287,-@.191472,0. 720195, -@.536976,0.0,1.0,0.0]),Sample_output=np. array( [@.15388985]),\n",
    "                input_translator=RequestTranslator()\n",
    "                )\n",
    "\n",
    "                model_file_path = 'xgboost_model/xgboost_model. bin' \n",
    "                os.makedirs(os.path.dirname(model_file_path), exist_ok=True)\n",
    "                booster. save_model(model_file_path)\n",
    "\n",
    "                model_builder = ModelBuilder( model=booster, model_path=\"xgboost_model/\", dependencies={\"requirements\": \"requirements_inference.txt\"},  schema_builder=schema_builder, role_arn=role)\n",
    "                return model_builder.build()\n",
    "\n",
    "        def build_pipeline_model(role, sklearn_model, xgboost_model):\n",
    "            pipeline_model_name = unique_name_from_base(\"sagemaker-btd-pipeline—model\")\n",
    "            pipeline_model = PipelineModel( name=pipeline_modet_name, role=role, models=[ sklearn_model, xgboost_model] )\n",
    "            return pipeline_model\n",
    "\n",
    "        def deploy_Model(pipeline_model, instance_type, wait):\n",
    "            endpoint_name = unique_name_from_base(\"sagemaker—btd-endpoint\")\n",
    "            pipeline_model.deploy(initial_instance_count=1, instance_type=instance_type, endpoint_name=endpoint_name, wait=wait)\n",
    "        \n",
    "        def run_test_inferences(endpoint_name) :\n",
    "            from io import BytesIO\n",
    "            predictor = Predictor(endpoint_name=endpoint_name)\n",
    "            payload = \"L,298.4,308.2,1582,78.7,216\"\n",
    "            buffer = predictor. predict (payload)\n",
    "            np_bytes = BytesI0(buffer)\n",
    "            print(f\"Inference payload: {payload}\")\n",
    "            print(f\"Inference result: {np.load(np_bytes, allow_pickle=True) }\")\n",
    "            payload = \"M,298.4,308.2, 1582, 30.2, 214\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if __mame__ == \"__main__\":\n",
    "            if len(sys.argv) > 2 and sys.argv[1] == \"--run-inference\":\n",
    "                    endpoint_name = sys.argv[2] run_test_inferences(endpoint_name)\n",
    "            else:\n",
    "                role=get_execution_role()\n",
    "                print(role)\n",
    "                sklearn_job_name = \"preprocess-2@23-11-30-17-59-14-989\"\n",
    "                xgboost_job_name = \"train-2023-11-30-18-04-28-537\"\n",
    "                featurizer, booster = load_models(sklearn_job_name, xgboost_job_name)\n",
    "                sklearn_model = build_sklearn_sagemaker_model(role, featurizer)\n",
    "                xgboost_model  = build_xgboost_sagemaker_model(role, booster)\n",
    "                pipeline_model = build_pipeline_model(role, sklearn_model, xgboost_model)\n",
    "                deploy_model(pipe Line_model, \"ml.m5.xlarge\", wait=False)\n",
    "    ```\n",
    "    - This code can be run via in built terminal and the packages for models will be created\n",
    "    - This will be reflected in the Spaces -> Deployment -> endpoints\n",
    "    - To get inference form an endpoint\n",
    "        python deploy.py --run-inference \"inference endpoint name\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/new_studio.png\">\n",
    "Here in the image all the functions from previously built notebooks are placed sperately\n",
    "\n",
    "- Pipeline\n",
    "    - Create a pipeline.py in a newfolder workflow\n",
    "    - test.py will contain somethign extra to store the model in registry\n",
    "        ```python\n",
    "        report_dict = {\"binary_classification_metrics\": { \"recall\": {\"value\": recall, \"standard_deviation\": \"\"}, \"precision\": {\"value\": precision, \"standard_deviation\": \"\"}, \"accuracy\": {\"value\": accuracy, \"standard_deviation\": \"\"}, } \"auc\"; {\"value\"; auc, \"standard_deviation\": \"\"},\n",
    "        print(f\"evaluation report: {report_dict}\"\n",
    "        return report_dict\n",
    "        ```\n",
    "    - register.py\n",
    "        - will contain functions form deployment\n",
    "        - build_sklearn_sagemaker_model and build_xgboost_sagemaker_mode\n",
    "        - The models are put ina ppeline and registerd\n",
    "        - ???\n",
    "    - deploy.py\n",
    "        - def deploy(role, model_package_arn, deploy_model):\n",
    "            if deploy_model:\n",
    "                sagemaker_client = boto3.client(\"sagemaker\")\n",
    "                response = sagemaker_client.update_model_package( Mode lPackageArn=model_package_arn, Mode LApprovalStatus='Approved', ApprovalDescription='Auto-approved via SageMaker Pipelines')\n",
    "                model_package = VPackage( role = role, model_package_arn = model_package_arn)\n",
    "                endpoint_name = unique_name_from_base(\"sagemaker-btd-endpoint\")\n",
    "                mode l_package. deploy (initial_instance_count=1,instance_type=\"ml.c5.xlarge\",\n",
    "                endpoint_name=endpoint_name)\n",
    "            else:\n",
    "                print(\"Skipped deploy model step based on parameter configuration.\")\n",
    "    - pipeline.py\n",
    "        ```python\n",
    "            def create_steps(role, input_data_s3_uri, bucket_name, model_package_group_name, model_approval_status, eta_parameter, max_depth_parameter, deploy_model_parameter):\n",
    "                # step decorator is used as function heresince this is a modular code and not a notebook\n",
    "                preprocess_result = step(preprocess, name=\"Preprocess\", keep_alive_period_in_seconds=300) ( input_data_s3_uri)\n",
    "\n",
    "                train_result = step(train, name=\"Train\", keep_alive_period_in_seconds=3000) ( X_train=preprocess_result[0], )\n",
    "                \n",
    "                y_train=preprocess_result[1], X_val=preprocess_result(2), y_val=preprocess_result (3), eta=eta_parameter, max_depth=max_depth_parameter)\n",
    "\n",
    "                test_result = step(test, name=\"Evaluate\", keep_alive_period_in_seconds=300) ( featurizer_model=preprocess_result[6], booster=train_result,\n",
    "\n",
    "                X_test=preprocess_result[4], y_test=preprocess_result [5] )\n",
    "\n",
    "                register_result = step(register, name=\"Register\", keep_alive_period_in_seconds=300) (role, featurizer_model=preprocess_result[6], booster=train_result, bucket_name=bucket_name, model_report_dict=test_result, mode l_package_group_name=model_package_group_name, mode1l_approval_status=model_approval_status)\n",
    "                deploy_result = step(deploy, name=\"Deploy\", keep_alive_period_in_seconds=300) (role,\n",
    "                model_package_arn=register_result, deploy_model=deploy_model_parameter) return [deploy_result]\n",
    "\n",
    "                if _name_ == \"__main_\":\n",
    "                        os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()\n",
    "                        role=get_execution_role()\n",
    "                        bucket_name = Session().default_bucket()\n",
    "                        pipeline_name = \"sagemaker-btd-pipgline\"\n",
    "                        model_package_group_name = \"sagemaker-btd-model-package-group\"\n",
    "                        model_approval_status = \"PendingManualApproval\"\n",
    "                        eta_parameter = ParameterFloat( name=\"eta\", default_value=0.3\n",
    "                        )\n",
    "                        max_depth_parameter = ParameterInteger( name=\"max_depth\", default_value=8\n",
    "                        )\n",
    "                        deploy_model_parameter = ParameterBoolean(\n",
    "                        name=\"deploy_model\", default_value=True)\n",
    "\n",
    "                        input_data_s3_uri = download_data_and_upload_to_s3(bucket_name)\n",
    "                        steps=create_steps(role, input_data_s3_uri, bucket_name, model_package_group_name, model_approval_status, eta_parameter, max_depth_parameter, deploy_model_parameter)\n",
    "                        pipeline = Pipeline( name=pipeline_name, parameters=[deploy_model_parameter, eta_parameter, max_depth_parameter], steps=steps\n",
    "                        )\n",
    "                        pipeline. upsert(role_arn=role)\n",
    "                        pipeline. start()\n",
    "        ```\n",
    "        - python pipeline.py\n",
    "        - Check for pipelines in home page\n",
    "        - DAG can be seen and updates will be live\n",
    "        - Models will be registered in Model tab\n",
    "        - comaprision can be don\n",
    "        - ??? No monitoring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Video 2\n",
    "> https://youtu.be/mRNcVKJ6UNo : pipeline, batch transform, model monitoring, baseline, Dec 2022\n",
    "- experiments\n",
    "  <img src=\"../../images/sage_exp.png\">\n",
    "- pipeline\n",
    "  - automate model building process\n",
    "  - pipelines : local mode to test on local and not create things on cloud\n",
    "- Pilot \n",
    "  - uses automl framework : autogluon\n",
    "  <img src=\"../../images/automl.png\">\n",
    "- Cross account dev stage prod code and pipeline sharing\n",
    "- model monitoring\n",
    "  <img src=\"../../images/model_monitor.png\">\n",
    "  - batch inference\n",
    "  <img src=\"../../images/model_mon_batch.png\">\n",
    "- Demo\n",
    "  <img src=\"../../images/pipeline_1.png\">\n",
    "  - [github](https://github.com/aws-samples/sagemaker-custom-project-templates)\n",
    "  - example 1\n",
    "    <img src=\"../../images/pipeline_2a.png\">\n",
    "    1. First pipeline\n",
    "      - Train and baseline the trained model using statistical analysis \n",
    "      - Compare it monthly to see if there is any drift\n",
    "      - Package the model and deploy\n",
    "    <img src=\"../../images/pipeline_2b.png\">\n",
    "    2. Second pipeline\n",
    "      - Batch inferencing and monitoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Code : In this section, we'll import libraries and setup variables needed to configure pipeline steps and construct the model build pipeline. We'll also upload the scripts that will be used for the data preprocessing and model evaluation steps to S3.\n",
    "    \n",
    "      ```python\n",
    "        import os\n",
    "        import time\n",
    "        from time import gmtime, strftime ikport sagemaker\n",
    "        import json\n",
    "        import boto3 oe\n",
    "        import pandas as pd\n",
    "\n",
    "        # SageMaker Job Imports (ex. Processing, Training) from sagemaker.sklearn.processing import SKLearnProcessor from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "        from sagemaker.model import Model from sagemaker. inputs import TrainingInput\n",
    "        from sagemaker.mode|_monitor import DatasetFormat, model_monitoring from sagemaker.clarify import (\n",
    "          BiasConfig,\n",
    "          DataConfig,\n",
    "          ModelConfig )\n",
    "        from sagemaker.model_metrics import ModelMetrics, MetricsSource\n",
    "        from sagemaker.drift_check_baselines import DriftCheckBaselines\n",
    "\n",
    "        # SageMaker Pipeline \n",
    "        from sagemaker.workflow.properties import PropertyFile\n",
    "        from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "        from sagemaker.workflow.condition_step import ConditionStep\n",
    "        from sagemaker.workflow. functions import JsonGet\n",
    "\n",
    "        from sagemaker.workflow.pipeline import Pipeline\n",
    "        from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep, ???\n",
    "        from sagemaker.workflow.model_step import ModelStep\n",
    "        from sagemaker.workflow.parameters import ( ParameterInteger, ParameterString,)\n",
    "        from sagemaker.workflow. functions import Join \n",
    "        from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "        from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "        from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "        from sagemaker.workflow.clarify_check_step import ( DataBiasCheckConfig, ClarifyCheckStep, )\n",
    "        from sagemaker.workflow.quality_check_step import ( DataQualityCheckConfig, QualityCheckStep,)\n",
    "        from sagemaker.workflow.parameters import ( ParameterBoolean, ParameterInteger, ParameterString, )\n",
    "\n",
    "        from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Setup S3 paths for pipeline inputs, outputs, & artifacts\n",
    "        print(\"Demo Bucket: {}\".format(bucket))\n",
    "        bucket_prefix = f\".../{int(time. time())}\"\n",
    "        model_artifacts = \"s3://{}/{}/model-artifacts\". format(bucket, bucket_prefix)\"\n",
    "        transformed_data_path = \"s3://{}/{}/transformed-data\". format (bucket, bucket_prefix)\"\n",
    "        raw_input_data = \"s3://{}/aim321-demo-model-monitor-batch-transform/raw-customer-data/\"\n",
    "        print(\"Raw Data Path: {}\".format(raw_input_data))\n",
    "        print(\"Transformed Data Path: {}\".format(transformed_data_path))\n",
    "        print(\"Model Artifact Path: {}\".format(model_artifacts))\n",
    "\n",
    "\n",
    "        # SageMaker Model Registry\n",
    "        # Specify the model package group for registering high performing model versions.\n",
    "        model_package_group_name=\"aim321-customer-churn\"\n",
    "        print(\"SageMaker Model Registry - Model Package Group:\", model_package_group_name) \n",
    "\n",
    "        # Setup Step Caching Configuration\n",
    "        # This configuration can be enabled on pipeline steps to allow SageMaker Pipelines to automatically check if a previous (successful) run of a pipeline step with the same values for specific parameters is found. If it is found, Pipelines propogates the results of that step to the next step without re-running the step saving both time and compute costs.\n",
    "        cache_config = CacheConfig(enable_caching»True, expire_aftere\"PT12H\")\n",
    "\n",
    "\n",
    "        # Setup Runtime Parameters\n",
    "        # Configurable parameters that can be passed in at runtime without changing pipeline code. pipeline. start (parameters=dict(...))\n",
    "        # setup Runtime Parameters\n",
    "        # Data Preparation Step — S3 URI to input data\n",
    "        input_data = ParameterString(name=\"InputData\", default_value=raw_input_data)\n",
    "\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/pipeline_3.png\">\n",
    "\n",
    "<img src=\"../../images/pipeline_4.png\">\n",
    "\n",
    "```python\n",
    "    # continues from above\n",
    "    # Configure the Data Quality Baseline Job\n",
    "    # Configure the transient compute environment\n",
    "    check_job_config = CheckJobConfig( role*role_arn, instance_count@1, instance_type=\"m1l.c5.xlarge\", volyme_size_in_gb=120, sagemaker_session=session,\n",
    "    )\n",
    "    # Configure the data quality check input (training data), dataset format, and $3 outpu\n",
    "    data_quality_check_config = DataQualityCheckConfig( baseline_dataset=data_preparation_step.properties.ProcessingOutputConfig.Outputs[\" dataset_format=DatasetFormat.csv(header=False, output_columns_position=\"START\"), output_s3_urisJoin(on='/', values#['s3:/', bucket, bucket_prefix, ExecutionVariabl.\n",
    "\n",
    "\n",
    "    # Configure Pipeline Step - 'QualityCheckStep'\n",
    "    baseline_model_data_step = QualityCheckStep(\n",
    "    name=\"DataQualityCheckStep\",\n",
    "    # skip_check, indicates a baselining job\n",
    "    skip_checkeTrue,\n",
    "    register_new_baseline=True,\n",
    "    quality_check_config=data_quality_check_config,\n",
    "    check_job_config=check_job_config,\n",
    "    mode l_package_group_name=model_package_group_name)\n",
    "\n",
    "```\n",
    "<img src=\"../../images/pipeline_5.png\">\n",
    "\n",
    "```python\n",
    "    # continues from above\n",
    "    # missing in video\n",
    "```\n",
    "<img src=\"../../images/pipeline_6.png\">\n",
    "\n",
    "```python\n",
    "    # Condition step for evaluating model quality and branching execution\n",
    "    cond_gte = ConditionGreaterThanOrEqualTo\n",
    "        ( left=JsonGet\n",
    "            ( step_name»evaluation_step.name, property_fil aluation_report, json_path='binary_classification_metrics.accuracy.value', )\n",
    "        right=0.4, # value to be compared to\n",
    "        ) \n",
    "    condition_step = ConditionStep(\n",
    "        name='PerformanceConditionalCheck', conditions=[cond_gte], \n",
    "        if_steps=[baseline_model_data_step,create_model_step, register_step],\n",
    "        else_steps=[],\n",
    "        )\n",
    "```\n",
    "<img src=\"../../images/pipeline_7.png\">\n",
    "\n",
    "```python\n",
    "    from sagemaker.wrokflow.pipeline import Pipeline\n",
    "    pipeline_name = 'aim321-train-baseline-pipeline-1'\n",
    "    step_list = [\n",
    "        data_preparation_step,\n",
    "        training_step,\n",
    "        evaluation_step,\n",
    "        condition_step]\n",
    "    training_pipeline = Pipeline(name=pipeline_name, parameters=[ input_data, ], steps = step_list)\n",
    "    # if exists then overwritten\n",
    "    training_pipeline.upsert (role_arn=role_arn)\n",
    "    execution= training_pipeline.start()\n",
    "\n",
    "\n",
    "    # view baseline\n",
    "    from sagemaker.model_monitor import MonitoringExecution from sagemaker.s3 import S3Downloader\n",
    "    monitoring_step = [step for step in execution. list_steps() if \"QualityCheck\" in step[\"!\n",
    "    s3_baseline_statistics»monitoring_step[\"Metadata\"] [\"QualityCheck\"] [\"BaselineUsedForDri\n",
    "    print(\"S3 URI FOR BASELINE STATISTICS ==>\", s3_baseline_statistics)\n",
    "    s3_baseline_constraints=monitoring_step[\"Metadata\"] [\"QualityCheck\"] [\"BaselineUsedForDr\n",
    "    print(\"S3 URI FOR BASELINE CONSTRAINTS ==>\", s3_baseline_constraints)\n",
    "```\n",
    "    - Now inside studio and under pipelines\n",
    "    - Pipelines can be visualized\n",
    "    - This can be automated and scheduled\n",
    "    \n",
    "```python\n",
    "    statistics = json. loads(S3Downloader. read_file(s3_baseline_statistics))\n",
    "    constraints = json. loads(S3Downloader. read_file(s3_baseline_constraints) )\n",
    "\n",
    "\n",
    "    # View Baseline Constraints\n",
    "    import pandas\n",
    "    constraints_df = pd.json_normalize(constraints, record_path=['features'])\n",
    "    constraints_df\n",
    "\n",
    "    statistics_df = pd.json_normalize(statistics, record_path=['features'])\n",
    "    statistics_df\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Code for batch inferencing and model monitoring pipeline\n",
    "```python\n",
    "    import os\n",
    "    import boto3\n",
    "    import re\n",
    "    import time\n",
    "    from time import gmtime, strftime import json\n",
    "    import pandas as pd\n",
    "    import sagemaker\n",
    "    # SageMaker Job Imports (ex. Processing, Batch Inference)\n",
    "    from sagemaker.transformer import Transformer\n",
    "    from sagemaker.image_uris import retrieve\n",
    "    # SageMaker Pipeline Imports\n",
    "    from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "    from sagemaker.workflow.model_step import ModelStep\n",
    "    from sagemaker.workf low. lambda_step import ( LambdaStep, LambdaOutput, LambdaOutputTypeEnum,)\n",
    "    from sagemaker. lambda_helper import Lambda\n",
    "    from sagemaker.workflow,execution_variables import ExecutionVariables\n",
    "\n",
    "    from sagemaker.workflow.parameters import ( ParameterBoolean, ParameterInteger, ParameterString,)\n",
    "    from sagemaker.model_monitor import DefaultModelMonitor\n",
    "    from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "    from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "    from sagemaker.workflow.quality_check_step import DataQualityCheckConfig\n",
    "    from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "\n",
    "    region = boto3.Session().region_name pipeline_session » PipelineSession()\n",
    "    role = sagemaker.get_execution_role()\n",
    "    print(\"RoleArn: {}\".format(role))\n",
    "\n",
    "    # two outputs i.e. batch results prediction , and monitoring result output\n",
    "    bucket = pipeline_session.default_bucket() \n",
    "    prefix = f\"aim321-demo-model-monitor-batch-transform/{int (time. time())}\"\n",
    "    batch_transform_output_path = \"s3://{}/{}/batch-transform-outputs\". format(bucket, pref\n",
    "    batch_monitor_reports_output_path = \"s3://{}/{}/data-quality-monitor-reports\".format(b\n",
    "    print(\"Batch Transform Output path: {}\".format(batch_transform_output_path) )\n",
    "    print(\"Batch Transform Data Drift Monitoring Output path: {}\".format(batch_monitor_rep:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/pipeline_8.png\">\n",
    "- Lambda code\n",
    "\n",
    "```python\n",
    "    %% writefile lambda_getapproved_model.py\n",
    "    # This Lambda function queries SageMaker Model Registry for a specific model package group provided on input to identify the latest # # approved # model version and return relat: The output includes:\n",
    "    # (1) model package arn (2) packaged model name (3) S3 URI for statistics baseline (4) S3 URI for constraints baselii\n",
    "    # The output is then used as input ifto the next step in the pipeline that performs batch monitoring and scoring using the latest approved model.\n",
    "    import json\n",
    "    import boto3 from botocore.exceptions\n",
    "    import ClientError\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    def lambda_handler(event, context): \n",
    "        sm_client = boto3.client(\"sagemaker\")\n",
    "        logger = logging.getLogger()\n",
    "        logger. setLevel(os.getenv(\"LOGGING_LEVEL\", logging. INFO))\n",
    "        # The model package group name\n",
    "        model_package_group_name = event [\"model_package_group_name\"]\n",
    "        print(model_package_group_name)\n",
    "        try:\n",
    "            approved_model_response = sm_client. list_model_packages( \n",
    "                ModelPackageGroupName=model_package_group_name,\n",
    "                ModelApprovalStatus=\"Approved\", SortBy=\"CreationTime\")\n",
    "            model_package_arn = approved_model_response[\"ModelPackageSummaryList\"] [0] (\"Mod\n",
    "            logger.info(f\"Identified the latest approved model package: {model_package_arn\n",
    "            s3_baseline_uri_response = sm_client.describe_model_package( ModelPackageName=model_package_arn )\n",
    "            s3_baseline_uri_statistics = s3_basleline_uri_response[\"Mode\\Metrics\"] [\"ModelWa\n",
    "            s3_baseline_uri_constraints = s3_baseline_uri_response[\"Mode\\Metrics\"] [\"ModelD\n",
    "            model_name = s3_baseline_uri_response[\"CustomerMetadataProperties\"] [\"ModelWame\n",
    "            logger.info(f\"Identified the latest data quality baseline statistics for appro\n",
    "            logger.info(f\"Identified the latest data quality baseline constraints for appr\n",
    "            return { i\n",
    "            \"statusCode\": 200,\n",
    "            \"modelArn\": model_package_arn,\n",
    "            \"s3uriConstraints\": s3_baseline_uri_constraints,\n",
    "            \"s3uriStatistics\": s3_baseline_uri_statistics,\n",
    "            \"mode Name\": model_name\n",
    "            }\n",
    "        except ClientError as e:\n",
    "            error_message = e.response(\"Error\") [\"Message\"]\n",
    "            logger.error(error_message)\n",
    "            raise Exception(error_message)\n",
    "    # create and configure pipeline\n",
    "    # Use the current time to define unique names for the resources created\n",
    "    current_time = time.strftime(\"%m—%d—%H-%w-%S\", time. localtime())\n",
    "    function_name = \"getapprovedmodel-sagemaker-step\" + current_time\n",
    "    # This value should be changed to match the name of your model package group model_package_group_name = \"aim321-—customer—churn\"\n",
    "    # Lambda helper class can be used to create the Lambda function\n",
    "    func = Lambda( function_name=function_name, \n",
    "        execution_role_arn=lambda_role_arn,\n",
    "            script=\"lambda_getapproved_model.py\",\n",
    "            handler=\"Lambda_getapproved_model.lambda_handler\",\n",
    "            timeout=600,\n",
    "            memory_size=128, )\n",
    "    # The dictionary retured by the Lambda function is captured by LambdaOutput, each key # LambdaOutput\n",
    "\n",
    "    output_param_1 = LambdaQutput(output_name=\"statusCode\", output_type=LambdaOutputTypeEn\n",
    "    output_param_2 = LambdaOutput(output_name=\"modelArn\", output_typesLambdaOutputTypeEnum\n",
    "    output_param_3 = LambdaOutput(output_name=\"s3uriConstraints\", output_type=LambdaOutput\n",
    "    output_param_4 = LambdaOutput(output_names\"s3uriStatistics\", output_type=LambdaOutputT\n",
    "    output_param_5 = LambdaOutput(output_name*\"modelName\", output_type=LambdaOutputTypeEnu|\n",
    "    lambda_getmodel_step = LambdaStep( name=\"LambdaStepGetApprovedModel\", lambda_funcefunc, inputse{ \"model_package_group_name\": model_package_group_name ),\n",
    "    outputs= [output_param_1, output_param_2,output_param_3, output_param_4,output_param_5]\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/pipeline_9.png\">\n",
    "\n",
    "```python\n",
    "    # The line below will run the pipeline using clean input - doesn't generate violations # batch_prediction_data = \"s3://aws-ml-blog/artifacts/ml_workshop/AIM321/churn-predicti\n",
    "    # The Line below is using input that has been modified to having values out of range a\n",
    "    batch_prediction_data = \"s3://aws-m\\-blog/artifacts/ml_workshop/AIM321/churn-predictio ...\"\n",
    "\n",
    "    transform_input_param = ParameterString(name=\"transform_input\", default_value=batch_prediction_data,)\n",
    "\n",
    "    # Configure a transformer for SageMaker Batch Transform Job\n",
    "    transformer = Transformer( model_name=lambda_getmodel_step.properties.Outputs[\"modelName\"],\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        accept=\"text/csv\",\n",
    "        assemble_with=\"Line\",\n",
    "        output_pathebatch_transform_output_path,\n",
    "        sagemaker_session=pipeline_session,)\n",
    "\n",
    "    transform_arg = transformer.transform(\n",
    "        transform_input_param,\n",
    "        content_types\"text/csv\",\n",
    "        split_types\"Line\",\n",
    "        input_filtere\"$[1:]\",)\n",
    "\n",
    "    # Configure the data quality monitor for the SageMaker Batch Transform \n",
    "    # The data quality monitor accepts the S3 URI of the baseline statistics and constraints specific to the approved model version from SageMaker Model Registry. The baseline is then used to run the data quality monitoring job during pipeline execution comparing the batch prediction input data with baseline data to identify any violations signaling potential data drift.\n",
    "    job_config = CheckJobConfig(role=role)\n",
    "    data_quality_config = DataQualityCheckConfig(\n",
    "        baseline_dataset=transform_input_param,\n",
    "        dataset_format=DatasetFormat.csv(header=False),\n",
    "        output_s3_uri=batch_monitor_reports_output_path,)\n",
    "\n",
    "    # Use the MonitorBatchTransformStep to run & monitor the transform job\n",
    "    # This step runs a batch transform job using the transformer object configured above and monitors the data passed to the transformer # before executing the job.\n",
    "    # Note: You can configure the step to fail if a violation to Data Quality is found by toggling the fail_on_violation flag.\n",
    "\n",
    "    from sagemaker.workflow.monitor_batch_transform_step import MonitorBatchTransformStep\n",
    "    transform_and_monitor_step = MonitorBatchTransformStep( name=\"MonitorCustomerChurnDataQuality\"\", transform_step_args*transform_arg, ponitor_configuration=data_quality_config, check_job_configuration=job_config, monitor_before_transformeTrue,\n",
    "    # if violation is detected in the monitoring, you can skip it and continue running\n",
    "    fail_on_violation=False),\n",
    "    supplied_baseline_statistics=lambda_getmodel_step.properties.Outputs[\"s3uriStatist\n",
    "    supplied_baseline_constraints=lambda_getmodel_step. properties. Outputs [\"s3uriConstr\n",
    "```\n",
    "\n",
    "<img src=\"../../images/pipeline_10.png\">\n",
    "\n",
    "```python\n",
    "    from sagemaker.wrokflow.pipeline import Pipeline\n",
    "    pipeline_name = 'aim321-batch-baseline-pipeline-1'\n",
    "    step_list = [\n",
    "            lambda_getmodel_step,\n",
    "            trnasform_and_monitor_step,]\n",
    "    batching_pipeline = Pipeline(name=pipeline_name, parameters=[ input_data, ], steps = step_list)\n",
    "    # if exists then overwritten\n",
    "    training_pipeline.upsert (role_arn=role_arn)\n",
    "    execution= training_pipeline.start()\n",
    "```\n",
    "\n",
    "- Cloud watch\n",
    "\n",
    "```python\n",
    "    # Incorporate alerts and/or retraining based on violations found during the monitoring job. When violations are found, SageMaker's managed model monitor container will emit a message to AWS CloudWatch logs.\n",
    "    # Create CloudWatch Logs client import boto3\n",
    "    \n",
    "    from datetime import datetime, timedelta \n",
    "    import time\n",
    "    cw_logs_client = boto3.client('logs')\n",
    "    log_group = '/aws/sagemaker/ProcessingJobs'\n",
    "    log_stream_name = ['pipelines-meixhzlniuhn-MonitorCustomerChurn-1x896vAjbN/algo-1-1669\n",
    "\n",
    "    query = f\"CompletedWithViolations\"\n",
    "\n",
    "    violations_query_response = cw_logs_client.filter_log_events( \n",
    "        logGroupName=log_group,\n",
    "        logStreamNames = log_stream_name,\n",
    "        filterPattern=\"CompletedwithViolations\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/pipeline_11.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://youtu.be/J9T0X9Jxl_w : model monitoring and drift using Sagemaker studio, TODO\n",
    "[notebook](../../../../Codes/monitor_endpoint_example.ipynb)\n",
    "[github](https://github.com/sirimuppala/amazon-sagemaker-multi-model-endpoints-model-monitoring/tree/master)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n",
    "> https://github.com/aws-samples\n",
    "> https://youtu.be/T9llSCYJXxc - MLOPs faster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Youtube\n",
    "https://youtu.be/Le-A72NjaWs - end to end KN CLI, enpoint deployment, no monitoring and replacing, no takign care fo drifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow\n",
    "\n",
    "> [youtube](https://www.youtube.com/watch?v=qdcHHrsXA48&pp=ygULbWxmb3cgbWxvcHM%3D)\n",
    "- MLflow setup from scratch model perfromance comparing\n",
    "- Connect dagshub to github where the code resides\n",
    "- No drift and monitoring\n",
    "- Dagshub is for remote sharing of details that was on local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc\n",
    "#### End to End with KN [youtube](https://www.youtube.com/playlist?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "> [intro](https://youtu.be/S_F_c9e2bz4?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "1. Data ingestion\n",
    "2. data transformation\n",
    "3. model trainer\n",
    "4. model evaluation\n",
    "5. model development\n",
    "- CI/CD pieplines using github actions\n",
    "- Docker environments\n",
    "- Artifacts\n",
    "> [Video 1](https://youtu.be/Rv6UFGNmNZg?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "- Project and github setup\n",
    "    - seperate environment is created using virtualenv so that libraries dont mix up\n",
    "        - python3 -m venv <name of the folder>\n",
    "        - to activate the environment\n",
    "            - source .venv/bin/activate\n",
    "        - to deactivate\n",
    "            - deactivate\n",
    "        - To create files with libraries used and thier version\n",
    "            - python3 -m pip freeze > requirements.txt\n",
    "    - other git related stuff below in Git section\n",
    "    - a source folder is created with empty python file \"__init__.py\"\n",
    "        - Wherever the init.py exists that folder can be imported as a library\n",
    "        - usually this will contain the util functions\n",
    "    - setup.py\n",
    "        - this is used for packaging and using as a library else where\n",
    "        ```python\n",
    "        # find_packages identifies all the packages used in the application\n",
    "        from setuptools import find_packages, setup\n",
    "\n",
    "        HYPHEN_E = \"-e .\"\n",
    "        # Metadata of the application\n",
    "        setup(\n",
    "            name='...',\n",
    "            version='...',\n",
    "            author='...',\n",
    "            author_email='...',\n",
    "            packages=find_packages(),\n",
    "            install_requires=get_requirements('requirements.txt')\n",
    "        )\n",
    "        # a function to get read lines in requirements.txt (file_path)\n",
    "        def get_requirements(file_path):\n",
    "            requirements = []\n",
    "            with open(file_path) as requirements_obj:\n",
    "                requirements = requirements_obj.readlines()\n",
    "                requirements = [req.replace(\"\\n\",\"\") for req in requirements]\n",
    "\n",
    "                if HYPHEN_E in requirements:\n",
    "                    requirements.remove(HYPHEN_E)\n",
    "\n",
    "            return requirements\n",
    "        ```\n",
    "> [Video 2](https://youtu.be/sDWL30CzJT8?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "- This will contain addition to the project structure, Logging and exception handelling\n",
    "- /src/components\n",
    "    - to host functions that will be called upon \n",
    "    - helful for reuseability and compartmentalizing general functions\n",
    "    - This can contain files which are used to\n",
    "        - read data ex : data_ingestion.py\n",
    "        - transform ex: data_transformation.py\n",
    "        - training model ex : model_trainer.py\n",
    "    - this will also contain __init__.py\n",
    "- /src/pipeline\n",
    "    - has the __init__.py\n",
    "    - train_pipeline.py, which calls the components in a sequential manner\n",
    "    - predict_pipeline.py, which will be used for inferencing\n",
    "- /src/config.py\n",
    "    - to store configuration details of databases or cloud etc.\n",
    "- /notebooks\n",
    "    - for eda and experiments\n",
    "- /artifacts\n",
    "    - to store outputs that has significance\n",
    "- /model\n",
    "    - to store trained models\n",
    "- /src/logger.py\n",
    "    - for capturing information while the pipeline runs for debugging and tracing issues\n",
    "    ```python\n",
    "    import logging\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "\n",
    "    # construct a format for log file name\n",
    "    log_file = f\"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\"\n",
    "\n",
    "    logs_path = os.path.join(os.getcwd(), \"logs\", log_file)\n",
    "    os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "    log_file_path = os.path.join(logs_path, log_file)\n",
    "\n",
    "    # Handles logging information\n",
    "    logging.basicConfig(\n",
    "        filename=log_file_path,\n",
    "        format=\"[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s\",\n",
    "        level=logging.INFO\n",
    "    )\n",
    "    ```\n",
    "- /src/exception.py\n",
    "    - to handel exceptions\n",
    "    ```python\n",
    "        '''\n",
    "        Custom Exception Handling\n",
    "        '''\n",
    "\n",
    "        import sys\n",
    "\n",
    "        # a function to load the execution information and return the error message\n",
    "        def error_message_details(error, error_details:sys):\n",
    "            # exc_info() returns the information about the error\n",
    "            _,_,exc_tb = error_details.exc_info()\n",
    "\n",
    "            # get file name from the exc_info() function\n",
    "            file_name = exc_tb.tb_frame.f_code.co_filename\n",
    "\n",
    "            # construct the error message to return\n",
    "            error_message = \"Error occured in Python script [{0}] at line number [{1}] error message [{2}]\".format(file_name, exc_tb.tb_lineno, str(error))\n",
    "\n",
    "            return error_message\n",
    "\n",
    "        # Inheriting the Exception class\n",
    "        class CustomException(Exception):\n",
    "            def __init__(self, error_message, error_details:sys) -> None:\n",
    "                super().__init__(error_message)\n",
    "                self.error_message = error_message_details(error_message, error_details=error_details)\n",
    "\n",
    "            def __str__(self):\n",
    "                return self.error_message\n",
    "    ```\n",
    "- /src/utils.py\n",
    "    - functions which can be used anywhere as its written in a general manner without hardcoding\n",
    "\n",
    "> [Video 3](https://youtu.be/gqqGdu1P2FM?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "- Problem understanding, EDA and model training\n",
    "    - EDA can be done in Jupyter notebook\n",
    "    - Artifacts fromm that can be stored in project\n",
    "    - Column transformer pipeline\n",
    "        ```python\n",
    "        from sklearn.preprocessing import oneHotencoder, StandardScaler\n",
    "        from sklearn.compose import ColumnTransformer\n",
    "        numeric_transformer = StandardScaler() \n",
    "        oh_transformer = OneHotEncoder()\n",
    "        preprocessor = Columntransformer( [\n",
    "             (\"OneHotEncoder\", oh_transformer, cat_features), \n",
    "             (\"Standardscaler\", numeric_transformer, num_features),\n",
    "        ]\n",
    "        )\n",
    "        X = preprocessor.fit(X)\n",
    "        # then train and test 4 splits\n",
    "        ```\n",
    "    - All this in notebook but the actual code must be modularized and will be re-written in .py files as in project structure\n",
    "\n",
    "> [Video 4](https://youtu.be/_0v1UK7smBc?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "- Modular code of data_ingestion.py from MongoDB\n",
    "    - import exceptions, loggers, os and sys\n",
    "    - pandas, train test split form sklearn\n",
    "    - [data_ingestion.py](../../../../Codes/mlproject/src/components/data_ingestion.py)\n",
    "        - dataingestion class can be replace with config file\n",
    "    - Instead of print statements, use logging.info(\"message to be stored)\n",
    "> [Video 5](https://youtu.be/Zs2BZkgoivM?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "- Modular code of data_trnasformation.py and pipelines\n",
    "    - [data_transformation.py](../../../../Codes/mlproject/src/components/data_transformation.py)\n",
    "> [Video 6](https://youtu.be/EAWR1kFtEGo?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "    - [model_trainer.py](../../../../Codes/mlproject/src/components/model_trainer.py)\n",
    "> [Video 9](https://youtu.be/gbJn2Ls2QsI?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "    - Deployment In AWS Cloud Using CICD Pipelines\n",
    "    - Beanstalk\n",
    "    - \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Simpler Production ML Systems using Feature/Training/Inference Pipelines [youtube](https://youtu.be/Mna14aTtD8s)\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git\n",
    "- to initialize a folder to sync with repo by creating a hidden folder .git\n",
    "    - git init\n",
    "- git global config to set username and email\n",
    "    - git config --global user.name “[firstname lastname]”\n",
    "    - git config --global user.email “[valid-email]”\n",
    "- to add a file to the repo\n",
    "    - git add <file name>\n",
    "- to add all changes\n",
    "    - git add .\n",
    "- to commit the file to the repor\n",
    "    - git commit -m \"some message about this commit\"\n",
    "- to remove file from commit\n",
    "    - git rm <file name>\n",
    "- to check the status of commits or changes in repo in local vs repo in cloud\n",
    "    - git status\n",
    "- to push the commit\n",
    "    - git push -u origin <branch name>\n",
    "- to create a file which will have list of keywords which if found in any file will not be considered for uploading like keys or secrets\n",
    "    - touch .gitignore\n",
    "    - then type in words that if found should ignore \n",
    "- to clone a repository from cloud to local\n",
    "    - git clone <ssh link of the repo from github>\n",
    "- to see all the changes made to a repo\n",
    "    - git log\n",
    "- merge the specified branch’s history into the current one\n",
    "    - git merge <name of the branch>\n",
    "- switch to another branch and check it out into your working directory\n",
    "    - git checkout\n",
    "- to change branch\n",
    "    - git branch <name of the new branch to be created>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview TODO\n",
    "- https://youtu.be/LUg-9YLLiRI\n",
    "- https://youtu.be/N9nV-_1F-CU\n",
    "- https://youtu.be/QZ_VK7w3b8g\n",
    "- https://youtu.be/isCFaKZGXFM\n",
    "- https://www.mygreatlearning.com/blog/machine-learning-interview-questions/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
