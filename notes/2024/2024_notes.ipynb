{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md' target='_blank'>../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md</a><br>"
      ],
      "text/plain": [
       "/home/radial/Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [youtube](https://youtu.be/ZTt9gsGcdDo)\n",
    "# Essential Matrix Algebra for Neural Networks, Clearly Explained\n",
    "FileLink('../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "#### Activation Functions\n",
    "> tanh\n",
    "- .\n",
    "> logit\n",
    "- .\n",
    "> sigmoid\n",
    "- .\n",
    "> relu\n",
    "- .\n",
    "> leaky relu\n",
    "- .\n",
    "\n",
    "#### Other functions\n",
    "> Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Regression?\n",
    "To predict value of something (target) based on values of multiple factors (features).\n",
    "Since it is supervised, it must have training data with values of both target and features to get a sum of products (equation) which would have be the best among all possible equations describing the relations between the two by reducing the amount of distance between the actual value and the value predicted by the equation's line.\n",
    "Depending on the nature of the factors it will be linear (straight line/plane/hyperplane) or non linear (curved).\n",
    "\n",
    "Linear : target = A*feature1 + B*feature2 + C*Feature3 + x\n",
    "Non Linear : target = A*feature1^4 + B*feature2^2 + C*Feature3 + x (order is 4, as thats the highest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Assumptions\n",
    "    1. Linearity: The relationship between the dependent and independent variables is linear and additive. Must have linear betas.\n",
    "        > A linear relationship suggests that a change in response Y due to one unit change in X¹ is constant, regardless of the value of X¹.\n",
    "        > An additive relationship suggests that the effect of X¹ on Y is independent of other variables.\n",
    "        > If this is ignored, then the line will be off the actual value by a lot and the model will not accurately represent the data\n",
    "\n",
    "        > How to check if this is met before modeling: Plot scatter plot of each feature against the target and see the pattern. If its like a stright line then fine\n",
    "\n",
    "        > How to check after modeling : Plot residual vs predicted values and if there is any pattern and lack of randomness across the middle like there a non linear relationship was involved\n",
    "\n",
    "        > How to fix : If its not then polynomial regression might be needed on the variable in the equation by applying transformations like log or exp or root or power etc depending on the relation between that variable and the target\n",
    "\n",
    "\n",
    "    2. Independence (No Autocorrelation in residuals/errors): The observations are independent of each other.\n",
    "        > This means that a residual of an observation should not predict the next observation.\n",
    "        > Usually an issue in time series but if features are not choosen wisely, if can occur in tabular too\n",
    "        > Might be caused by missing variable\n",
    "\n",
    "        > How to check before modeling : We can conduct durbinWatsonTest on our model for an output with a p-value, which will determine whether the assumption is met or not. \n",
    "        The null hypothesis states that the errors are not auto-correlated with themselves (they are independent). Thus, if we achieve a p-value > 0.05, we would fail to reject the null hypothesis.\n",
    "        It must lie between 0 and 4. If DW = 2, implies no autocorrelation, 0 < DW < 2 implies positive autocorrelation while 2 < DW < 4 indicates negative autocorrelation.\n",
    "\n",
    "        > How to check after modeling : Draw the line over the points and look at the residual plot for patterns. If exists then there is no independence. Must be random.\n",
    "        Residual vs time plot and look for the seasonal or correlated pattern in residual values.\n",
    "\n",
    "        > How to fix :\n",
    "        Do more domain searching to add missing variables\n",
    "        Depending on the equation, or the relation between x and y, add polynomial or exp or log\n",
    "        <img src=\"../../images/autocorr_correction.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "\n",
    "    3. Homoscedasticity: The variance of the errors is constant across all levels of the independent variables.\n",
    "        > The variance of the residuals should not vary as the target is increased in residual plot\n",
    "        > Generally, non-constant variance arises in presence of outliers or extreme large values. \n",
    "        > If a feature has a wide range of numbers then this can happen as the feature might not be a good predictor\n",
    "\n",
    "        > How to check before modeling: Plot a variable against the target, find best fit and look at residual plot. If there is a funnel/dual cone/reverse funnel pattern then it might be heteroscedastic. \n",
    "\n",
    "        > How to check after modeling : residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern. Wreusch-Pagan / Cook – Weisberg test or White general test or Goldfledt-Quant test to detect this phenomenon\n",
    "\n",
    "        > How to fix it : There might be two/multiple regression liens that can explain if divided based on pattern. Log variables might help.\n",
    "        <img src=\"../../images/hetero_example.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "    4. Normality: The errors follow a normal distribution.\n",
    "        > The spread of the errors at a given point, should be normally distributed\n",
    "        > prediction errors/residuals need to be normally distributed\n",
    "        > Sometimes the error distribution is \"skewed\" by the presence of a few large outliers.\n",
    "        > Calculation of confidence intervals and various significance tests for coefficients are all based on the assumptions of normally distributed errors.\n",
    "        > If the error distribution is significantly non-normal, confidence intervals may be too wide or too narrow. \n",
    "        > The dependent and independent variables in a regression model do not need to be normally distributed by themselves   \n",
    "        > the normal error assumption is usually justified by appeal to the central limit theorem, which holds in the case where many random variations are added together.\n",
    "\n",
    "        > How to check before modeling: plot a histogram on your residuals or employ a Q-Q plot, which also helps us visually determine if our residuals follow a normal distribution. if there is a skew in your residuals. A log transformation on your dependent variable may help.\n",
    "\n",
    "        > How to check after modeling : The Kolmogorov-Smirnov test, the Shapiro-Wilk test, the Jarque-Bera test, and Anderson-Darling test.\n",
    "\n",
    "        > How to fix it : check the extreme values of the variable\n",
    "\n",
    "    5. No multicollinearity: The independent variables are not highly correlated with each other.\n",
    "        > independent variables show moderate to high correlation\n",
    "        > tough task to figure out the true relationship of a predictors with response variable\n",
    "        > difficult to find out which variable is actually contributing to predict the response variable.\n",
    "        > the standard errors tend to increase and the confidence interval becomes wider leading to less precise estimates of slope parameters\n",
    "        > two variables vary very similarly and contain the same kind of information.\n",
    "        > only the complexity of the model increase, and no new information or pattern is learned by the model.\n",
    "        > \n",
    "\n",
    "        > How to check before modeling:  use scatter plot/heat map to visualize correlation effect among variables\n",
    "\n",
    "        > How to check after modeling : VIF factor. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity.\n",
    "        VIF = 1/(1-R2) for each variable\n",
    "\n",
    "        > How to fix it : drop and check vif\n",
    "\n",
    "    6. No endogeneity: There is no relationship between the errors and the independent variables.\n",
    "        > Causes of endogeneity include omitted variables, simultaneity, and sample selection in the estimated equation. \n",
    "        > OLS estimator will be inconsistent and biased,\n",
    "\n",
    "        > How to check before modeling:  \n",
    "\n",
    "        > How to check after modeling : Durbin-Wu-Hausman test, which compares the OLS estimator with an alternative estimator that is robust to endogeneity, such as the instrumental variables (IV) estimator.\n",
    "\n",
    "        > How to fix it : reduce the sources of endogeneity by including more relevant variables, improving the data quality, or using a different research design that avoids the causal ambiguity.\n",
    "        instrumental variables (IV) estimator, which uses a variable that is correlated with the endogenous explanatory variable but not with the error term\n",
    "\n",
    "Source : \n",
    "[youtube](https://www.youtube.com/playlist?list=PLTNMv857s9WUI1Nz4SssXDKAELESXz-bi)\n",
    "[blog 0](https://medium.com/@andrewhnberry/checking-your-linear-regression-assumptions-and-how-to-check-them-338f770acb57)\n",
    "[blog 1](https://godatadrive.com/blog/basic-guide-to-test-assumptions-of-linear-regression-in-r)\n",
    "[blog 2](https://people.duke.edu/~rnau/testing.htm)\n",
    "[blog 3](https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/)\n",
    "[blog 4](https://www.geeksforgeeks.org/assumptions-of-linear-regression/)\n",
    "[blog 5](https://www.jmp.com/en_in/statistics-knowledge-portal/what-is-regression/simple-linear-regression-assumptions.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Working\n",
    " * The aim is to find BLUE (best linear unbiased estimate) or the best fit line which explains all the variance across the data\n",
    " * This is usually an equation involving target on one side and linear additive combination of features on the other which are not in polynomial or expoenential or logarithmic form\n",
    " * y = coefficient + ax1 + bx2 + cx3 + dx4 | ... | + error\n",
    " * where a,b,c,d ... are slope values and coefficient is the intercept in multi dimensional plane\n",
    " * here coeeficient is the value when features are 0\n",
    " * and slope is the unit change in target wrt. unit change in feature\n",
    "\n",
    " * Cost function : to reduce overall distance between each point in dataset and predcited value by the line\n",
    " * in the below equation is one way to find the distance b/w predicted and actual, this can be replaced with absolute also. where m is total number of values\n",
    "\n",
    "  <img src=\"../../images/linear_cost.png\">\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/1-OGRohmH2s)\n",
    "\n",
    "* Gradient Descent\n",
    "\n",
    "  - Infinite number of lines can be drawn and then the cost function can be calculated for all but thats computationally expensive\n",
    "  - So by keeping coefficent constant, and varying the slope values, for all the variables values, cost function is calculated.\n",
    "  - Then a grpah is drawn between all possible m values and cost function's output to find the lowest point (gloabl minima) on the curve which will give the best m for line \n",
    "  - This finding of global minima (slope = 0) is using gradient descent which is based on convergence theorem which moves towards minima using derivative values of slope and a learning rate\n",
    "  - To know which way is global minima, all the sides needs to be considered before moving in any direction\n",
    "\n",
    "* LR using Ordinary Least square (OLR)\n",
    "- TODO  https://youtu.be/KZ1mWboXE6g\n",
    " \n",
    "* Multiple LR using Matrix \n",
    "  - In this image, the a is all the beta values including coefficient\n",
    "  - <img src=\"../../images/multi_reg_matrix.png\"> \n",
    "    \n",
    "  - Using direct formula : (Xt.X)^-1.Xt.Y\n",
    "  - Using single valur decomposition\n",
    "  - Using QR decomposition\n",
    "\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/_OyKjstWe80)\n",
    "[blog](https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0, b1, b2 = [-2.40119363  1.62732095 -0.29177719]\n",
      "Predictions = [1. 2. 3.]\n",
      "Error = [-0. -0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# MLR using Matrix\n",
    "import numpy as np\n",
    "x = np.array([[100, 212, 356], [4.25, 10.99, 16.2],  [-234, -465, -678]], np.int32)\n",
    "y = np.array([1,2,3])   \n",
    "# formula : (Xt.X)^-1.Xt.Y\n",
    "betas = np.dot(np.dot(np.linalg.inv(np.dot((x.T), x)),(x.T)),y)\n",
    "np.set_printoptions(suppress=False)\n",
    "print(\"b0, b1, b2 = {}\".format(betas))\n",
    "preds = np.dot(x,betas)\n",
    "print(\"Predictions = {}\".format(preds))\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"Error = {}\".format(preds-y))\n",
    "\n",
    "#source : https://cmdlinetips.com/2020/03/linear-regression-using-matrix-multiplication-in-python-using-numpy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Code\n",
    "\n",
    "source : [blog](https://www.freecodecamp.org/news/data-science-with-python-8-ways-to-do-linear-regression-and-measure-their-speed-b5577d75f8b/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 1.0\n",
      "Betas : [ 0.00863046  0.01082108 -0.01180701]\n",
      "Beta0 : 2.046179467416178\n",
      "Predictions : [2.11436926]\n"
     ]
    }
   ],
   "source": [
    "# MLR using scikit learn\n",
    "# !pip install scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(x, y)\n",
    "print(\"Score : {}\".format(reg.score(x, y)))\n",
    "print(\"Betas : {}\\nBeta0 : {}\".format(reg.coef_, reg.intercept_))\n",
    "print(\"Predictions : {}\".format(reg.predict(np.array([[3, 5, 1]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betas : [-2.40119363  1.62732095 -0.29177719]\n",
      "Model : \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                    nan\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Tue, 02 Jan 2024   Prob (F-statistic):                nan\n",
      "Time:                        17:16:02   Log-Likelihood:                 81.846\n",
      "No. Observations:                   3   AIC:                            -157.7\n",
      "Df Residuals:                       0   BIC:                            -160.4\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -2.4012        inf         -0        nan         nan         nan\n",
      "x2             1.6273        inf          0        nan         nan         nan\n",
      "x3            -0.2918        inf         -0        nan         nan         nan\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   1.012\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.488\n",
      "Skew:                           0.643   Prob(JB):                        0.784\n",
      "Kurtosis:                       1.500   Cond. No.                     1.42e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.42e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 3 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1795: RuntimeWarning: divide by zero encountered in divide\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1795: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1717: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n"
     ]
    }
   ],
   "source": [
    "# MLR using statsmodel\n",
    "# !pip install statsmodels\n",
    "import statsmodels.api as sm\n",
    "model = sm.OLS(y,x)\n",
    "results = model.fit()\n",
    "print(\"Betas : {}\".format(results.params))\n",
    "print(\"Model : \\n{}\".format(results.summary()))\n",
    "# source  : [blog](https://datatofish.com/statsmodels-linear-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Output analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Metrics\n",
    "> MSE : Mean Squared error\n",
    "- Considers square of distance between actual and predicted value\n",
    "- Since its a quadratic in nature, the curve is differentiable at all points to find minima\n",
    "- one local and global minima as its a paraobla\n",
    "- Can tackle outliers as the distance is squared.\n",
    "- Since its a square the units are not the same for easy explanation\n",
    "\n",
    "> RSME\n",
    "- This is root of mse\n",
    "- suffers from same outlier impact\n",
    "- but has the same unit\n",
    "- also is differentiable at all points\n",
    "\n",
    "> RSLME\n",
    "- log of RSME to slows down the scale of error\n",
    "\n",
    "> MAE : Mean Absolute error\n",
    "- absolute difference between predicted and actual value\n",
    "- so the nature of the functionis like a V and is not differentiable in the origin\n",
    "- but is robust as outliers dont affect it as much as mse\n",
    "- Also since its |y-yhat| the unit is same for explainability\n",
    "- but its optimization takes time and is complicated as sub gradients needs to be calculated\n",
    "\n",
    "> MAPE\n",
    "- percentage of MAE, i.e. (MAE*100)/N\n",
    "\n",
    "> R2\n",
    "- 1-((square of sum of actual minus predicted)/(square of sum of actual minus mean))\n",
    "- 0.8-0.90 is good depending on the case\n",
    "- 1 might indicate overfitting\n",
    "- always increases if more features are added and might over fit so must be penalized\n",
    "    \n",
    "> Adjuested R2\n",
    "- To prevent increasing of R2 by adding useless features\n",
    "- if the feautre is not good, it will reduce the adjusted R2\n",
    "- 1-((1-R2)(N-1))/(N-P-1)\n",
    "- N is # of points\n",
    "- P is # of variables\n",
    "\n",
    "Source \n",
    "[youtube](https://youtu.be/BGlEv2CTfeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Regularization\n",
    "- When model is overfitting, i.e. good on training and bad on val and test, it might be learning the noise and not understanding the pattern\n",
    "- Overfitting models have high variance\n",
    "- When it doesn perform well even in the training then its underfitting, can be solved by making the model more complex or adding new features\n",
    "- Simple model which underfits is highly biased\n",
    "- Regularization helps in bias variance trade off by penalizing models for overfitting\n",
    "<img src = \"../../images/under_over_fit.png\">\n",
    "<img src  = \"../../images/regularization.webp\">\n",
    "\n",
    "\n",
    "> Lasso - Least absolute shrinkage and selection operator regression (L1)\n",
    "- The cost function used in regression where distance is squared, a product of lambda and absolute slope is added to penalize.\n",
    "- If there are features in a model which doesn contribute much, LASSO makes its weight to 0 there by acting as feature selector\n",
    "- <img src = \"../../images/lasso.png\">\n",
    "- m is # of features, n is # of samples, w is the slope of the feature, yi is actual and yhat is predicted\n",
    "- lambda is the hyperparameter, selected using cross valdidation (0-infinity)\n",
    "- When the value of lambda is very high, it will make the weights of slopes of features which dont contribute much go to 0\n",
    "- since the absolute graph is like a diamond which has 0\n",
    "\n",
    "    ```python\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    lasso=Lasso()\n",
    "    parameters={ 'alpha' :[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,28, 38,35,55,100]}\n",
    "    lasso_regressor=GridSearchCV( lasso, parameters, scoring='neg_mean_error' ,cv-5)\n",
    "    lasso_regressor. fit(X,y)\n",
    "    print(lasso_regressor.best_params_)\n",
    "    print(lasso_regressor.best_score_)\n",
    "    ```\n",
    "\n",
    "> Ridge - (L2)\n",
    "- The cost function is modified by adding a squared value of the slope\n",
    "- <img src = \"../../images/ridge.png\">\n",
    "- This reduces overfitting\n",
    "- lambda is the hyperparameter (0-infinity)\n",
    "- high lambda will reduce the weight but not make it 0\n",
    "- since square graph is like a circle and doesn go to 0\n",
    "\n",
    "    ```python\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.model_ selection import GridSearchcv\n",
    "    ridge=Ridge() \n",
    "    paraneters={ \"alpha\" :[1e-15,le-10,1e-8,1e-3,1e-2,1,5,16,20, 38, 35,46,45,58,55,168]}\n",
    "    ridge_regressor=Gridsearchcv( ridge, parameters, scoring='neg_mean_squared_error',cv=5)\n",
    "    ridge_regressor.fit(x,y)\n",
    "    ```\n",
    "\n",
    "> Elastic net (L1 and L2)\n",
    "- A combination of both Lasso and Ridge\n",
    "- Cost function has both absolute and squared values of slope added\n",
    "- <img src = \"../../images/elastic.png\">\n",
    "- this has two hyper parameters alpha and lambda\n",
    "\n",
    "    Source  :    \n",
    "    [youtube](https://youtu.be/2bx9Os2gPu4)\n",
    "    [youtube](https://www.youtube.com/watch?v=9lRv01HDU0s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "### - Basics\n",
    "> Parametric vs non parametric\n",
    "-\n",
    "> Grid vs random search CV\n",
    "-\n",
    "> class_weight\n",
    "- parameter to tell the model if the dataset target is balanced or not \n",
    "> Bootstrapping\n",
    "- Process of randomly selecting rows of data with replacement to generate mode datasets\n",
    "- Used usually in ensemble models like Random Forests \n",
    "> Bagging\n",
    "- Combination of bootstrapping and aggregation (majority wins) this is a paralell process as all the different sets are independent of each other\n",
    "> Boosting\n",
    "- This is a sequential process to work on weak learners by boosting the values of misclassified data points\n",
    "- first build a model on the training dataset and then build a second model to rectify the errors present in the first model\n",
    "> Stacking\n",
    "- \n",
    "> Blending\n",
    "- \n",
    "> Optimizer\n",
    "- \n",
    "### - Logistic\n",
    "> Working\n",
    "- For binary classification, things can be done using linear regression and a threshold like 0.5 can be set\n",
    "- but this might be susceptible to outliers\n",
    "- The aim of Log Reg is to find a best fit line which will linearly seperate the groups\n",
    "- equation is same as regression y = mx+c\n",
    "- The idea is to convert multidimensional points into just 0 or 1\n",
    "- This is done using sigmoid function\n",
    "- <img src = \"../../images/sigmoid.png\">\n",
    "- Instead of equation from Lin reg, if there is a sigmoid of that equation, the predicted value will always be b/w 0,1\n",
    "- This also gives the proabability given the value x, what are the chances the point belongs in either classes\n",
    "- Where the predicted value will be perpendicular distance of the point from the line\n",
    "- The unknown variable is the wieghts that needs to be assigned to each feature\n",
    "- <img src =\"../../images/log_weights.png\">\n",
    "- Here w is the weights, which laong with x which is the input, goes through summation and sigmoid non linear function to get predictions.\n",
    "- Which is then used to find the error or the loss function which then is partially derivated using gradient descent to find the global minima\n",
    "- <img src = \"../../images/log_chain.png\">\n",
    "- The cost function is\n",
    "- <img src = \"../../images/log_cost.png\">\n",
    "- This needs to be partially differentiated \n",
    "- <img src = \"../../images/log_diff.png\">\n",
    "- Difference between lir and log reg is that\n",
    "    - the yhat is sigmoid in log\n",
    "    - the cost function is not convex so there are many local minima and the initial parameters of weights determine where the local minima will be found after grad desct (no guarantee of global minima)\n",
    "- the Logit/log odds function\n",
    "- <img src = \"../../images/logit.png\">\n",
    "- Regularization also can be done\n",
    "- <img src =\"../../images/log_regularization.png\">\n",
    "\n",
    "> Code\n",
    "```python\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "    clf.predict(X[:2, :])\n",
    "    clf.predict_proba(X[:2, :])\n",
    "    clf.score(X, y)\n",
    "    # parameters : penalty, C, solver, \n",
    "```\n",
    "\n",
    "> Pros and cons\n",
    "+ Simple to build and explain\n",
    "+ less overfitting in lower dimensions\n",
    "- creates linear boundaries, we won't obtain better results when dealing with complex or non-linear data\n",
    "- cant be used if the number of observations is fewer than the number of features; otherwise, it may result in overfitting.\n",
    "- affected by outliers\n",
    "- has to qualify the assumptions like in linear regression\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/z9XAXXGwUzM)\n",
    "[youtube](https://youtu.be/xXvgkILaFT4?list=PLyqSpQzTE6M-SISTunGRBRiZk7opYBf_K)\n",
    "[blog](https://www.analyticsvidhya.com/blog/2021/10/building-an-end-to-end-logistic-regression-model/)\n",
    "\n",
    "### - Decision Tree\n",
    "> Assumptions\n",
    "- If the values are continuous then they are discretized prior to building the model\n",
    "- \n",
    "> Working\n",
    "- Works for bth regression and classification\n",
    "- Its multiple linear lines, not exactly non linear but better than just one linear line\n",
    "- hierarchical tree structure consisting of a root node, branches, internal nodes, and leaf nodes\n",
    "- algorithmic model utilizes conditional control statements and is non-parametric\n",
    "- The tree is built by splitting on each feature and the hierarchy of feature is decided based on the purity of the feature in predicting the target\n",
    "- The purity is based on\n",
    "    - Entropy and information gain\n",
    "        - <img src=\"../../images/decision_entropy.png\">\n",
    "        - <img src=\"../../images/decision_entropy_ex.png\">\n",
    "        - entropy before the split and after the split is compared\n",
    "        - <img src=\"../../images/decision_entropy_ex2.png\">\n",
    "        - Entropy is found for all features and which ever has the mx change is taken\n",
    "        - I.e. higher the Entropy, the lower will be the purity\n",
    "        - Which ever feature gives most information gain is used as node as thats the most purest\n",
    "        - Steps\n",
    "            - First entropy of the whole target is calulated\n",
    "            - Entropy of each feature is calculated for each class in it\n",
    "            - Information gain of each feature is found, which is total entropy minus (individual class entropy of the feature into its proability)\n",
    "            - Which feature has the highest IG becomes the root node with brances as its classes\n",
    "            - Now for each of the branches, its takes subset by filtering onlny for those value and then IG is calculated again to find the next split\n",
    "            - This continues, till there are leaf nodes that are pure split\n",
    "    - Gini index\n",
    "        - Unlike IG, minimum GI is taken as root node as its 1-(probability)\n",
    "        - <img src=\"../../images/decision_gini.png\">\n",
    "        - Steps\n",
    "            - Almost the same just that lowest GI will be used as root and for splitting\n",
    "    - <img src=\"../../images/decision_gi_IG.png\">\n",
    "    - [youtube](https://youtu.be/5aIFgrrTqOw)\n",
    "    - Entropy ranges from 0-1\n",
    "    - GI ranges from 0-0.5\n",
    "    - GI doesn have log and hence is faster\n",
    "- The nodes have threshold in case of numerical values\n",
    "    - [youtube](https://youtu.be/5O8HvA9pMew?list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe)\n",
    "    - Sorts the tables in ascending manner\n",
    "    - Each number is taken and IG or GI is calculated\n",
    "    - Time consuming for large dataset\n",
    "- For regression tree \n",
    "    - TODO\n",
    "- Pruning : When we remove sub-nodes of a decision node, cutting down of branches\n",
    "- Early stopping :  not allowing the layes to go beyond a point\n",
    "> Code\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(criterion ='entropy').fit(X_train,y_train)\n",
    "prediction = tree.predict(X_test)\n",
    "tree.score(test_features,test_targets)\n",
    "# parameters : max_depth, min_samples_split, criterion, min_samples_leaf, max_features, \n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "cross_val_score(regressor, X, y, cv=10)\n",
    "```\n",
    "> Pros and cons\n",
    "+ Highly explainable\n",
    "+ resistant to outliers\n",
    "- Generally overfits, hence high variance\n",
    "- parametric, hence new data point if added, needs retraining\n",
    "- not suitable for large datasets as its just one tree\n",
    "- not for imbalanced dataset\n",
    "\n",
    "Source \n",
    "[youtube](https://youtu.be/uPwZ8DhQNjg?list=PLyqSpQzTE6M-SISTunGRBRiZk7opYBf_K)\n",
    "[youtube](https://youtu.be/CWzpomtLqqs)\n",
    "[blog]()\n",
    "### - Random Forest\n",
    "> Working\n",
    "- DT are highly sensitive to training data and can overfit without generalizing\n",
    "- First multiple datsets are generated using bootstraping\n",
    "- Then for these randomly selected rows, randomly selected features are selected \n",
    "    - The usual number for feature # is log or sqr root of total # of features\n",
    "- These two add diversity to the datasets and reduces chances of overfitting\n",
    "- Then for each dataset DTs are created\n",
    "- For predicting, the data point is passed through all the trees and the output is generated\n",
    "- Majority wins (this is aggregation)\n",
    "- bootstrapping + aggregation = bagging\n",
    "- In case of regression, a mean or median can be considered as the output\n",
    "\n",
    "> Code\n",
    "    ```python\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf.fit(X, y)\n",
    "    clf.predict([[0, 0, 0, 0]])\n",
    "    clf.feature_importances_\n",
    "\n",
    "    # parameters : (n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "    regr.fit(X, y)\n",
    "    regr.predict([[0, 0, 0, 0]])\n",
    "\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- Complex computation, slower\n",
    "- interpretation is difficult\n",
    "\n",
    "+ Scaling is not really required\n",
    "+ Doesn care about unwanted features\n",
    "+ Generalized\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/v6VJ2RO66Ag)\n",
    "\n",
    "### - Ada Boost\n",
    "> Working\n",
    "- Boosting algorithms improve the prediction power by converting a number of weak learners to strong learners.\n",
    "- In Adaptive boosting, the weights are re-assigned to each instance, with higher weights assigned to incorrectly classified instances\n",
    "- Overview\n",
    "    - To begin with  it builds a model and gives equal weights to all the data points\n",
    "    - then assigns higher weights to points that are wrongly classified\n",
    "    - Now all the points with higher weights are given more importance in the next model\n",
    "    - keep training models until and unless a lower error is received.\n",
    "- Base learner\n",
    "    - The inital models are based on shallow trees which are just stumps based on gini index for each feature. Which ever has the least entropy or impurity or gini index will be selected as a base learner\n",
    "    - Once the stump is made and the predictions are ready, total erro will be calculated by summing up all the weights which were misclassified \n",
    "    - Performance of the model will be calculated using\n",
    "    - <img src =\"../../images/ada_boost_1.png\">\n",
    "    - Now to update the weights, first for the misclassified ones with positive performance in exp\n",
    "    - <img src =\"../../images/ada_boost_2.png\">\n",
    "    - Then for the properly classified ones, with negative performance in exp\n",
    "    - This new updated weights might not add up to 1 unlike the inital weights \n",
    "    - So the new weights are divided by the sum of all new weights to get fractions that will add upto 1\n",
    "    - This will be the normalized weights which will be used to create new dataset for boosting\n",
    "- Boosted dataset\n",
    "    - This will be created by again sampling from orginal dataset\n",
    "    - A random number is selected and based on which bucket it falls on the cumilative normalized weight, that data point is inserted in the dataset\n",
    "    - Since, the weight is high for the misclassified data point, it will get selected more often based on central limit theorem ???\n",
    "    - There by creating a new dataset with empahsis on misclassified data point with the same # of rows as the original\n",
    "    - This is then again used to create base learner like earlier with equal weightage\n",
    "    - The process then repeates till # of base learner = # specified in parameter\n",
    "- For predicting, all the stumps are used and which ever class gets the stumps with high summed up performance will win\n",
    "> Code   \n",
    "    ```python\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    clf.fit(X, y)\n",
    "    clf.predict([[0, 0, 0, 0]])\n",
    "    clf.score(X, y)\n",
    "\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "    regr.fit(X, y)\n",
    "    regr.predict([[0, 0, 0, 0]])\n",
    "    regr.score(X, y)\n",
    "    # Parameters : estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated'\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- Adaboost is that it needs a quality dataset, outliers might disturb it\n",
    "- Slower than XGboost\n",
    "- Computationally expensive\n",
    "- Not good for imbalanced\n",
    "\n",
    "+ Flexible mode\n",
    "+ Adaboost is less prone to overfitting as the input parameters are not jointly optimized\n",
    "+ extended beyond binary classification and has found use cases in text and image classification as well\n",
    "\n",
    "Source \n",
    "[blog](https://www.analyticsvidhya.com/blog/2021/09 adaboost-algorithm-a-complete-guide-for-beginners/)\n",
    "[Youtube](https://youtu.be/LsK-xG1cLYA)\n",
    "\n",
    "### - Gradient Boost\n",
    "> Working\n",
    "- Overview\n",
    "    - Unlike stumps, here small trees with 8-32 leafs are built\n",
    "    - \n",
    "- Steps (regression)\n",
    "    - First an average of the target is taken and is used as predicted\n",
    "    - Next difference b/w the avg and actual target is calulated as a new column (residuals)\n",
    "    - The new column now becomes the target and a small decision tree is built\n",
    "    - If a leaf has two residuals, its averaged\n",
    "    - Now the residuals are predicted and added with the average which might overfit (new target)\n",
    "    - hence a parameter is multiplied with the residual before adding which is the learning rate and is a hyperparameter (0-1)\n",
    "    - Lots of small steps in the right direction by keeping the learning rate small leads to better accuracy\n",
    "    - Now the new target is used to find new residuals again to repeat the process\n",
    "    - Now the average target + lr * tree 1's prediction + lr * tree 2's prediction ...\n",
    "    - Till the difference between the prediction and actual is small or the # of trees has reached specified limit\n",
    "- Intution behind Gradient Descent Regression\n",
    "    - This needs a loss function thats differentiable\n",
    "    - which can be anything as its flexible. Usually a simple regression loss function\n",
    "    0.5(actual - predicted)^2 is taken\n",
    "    - 0.5 is for differentiablity using the chain rule\n",
    "    1. To use the average of the target as a constant value\n",
    "        <img src=\"../../images/grad_boost_1.png>\n",
    "    2. make the first tree which is just a leaf with the average value of the target\n",
    "        - There will be M trees 1/1 where M is a hyperparameter specified, for which the following will be repeated\n",
    "        - <img src=\"../../images/grad_boost_2.png\">\n",
    "        - Find derivative of the loss function (which is just the residual for all the rows of dataset)\n",
    "        - <img src=\"../../images/grad_boost_3.png>\n",
    "        - Now using this residual build a tree to predict new residuals\n",
    "        - Find the avg. values of last leaves of the tree if multiple values in it\n",
    "        - <img src=\"../../images/grad_boost_4.png>\n",
    "        - Add original average of the target with new residuals after multiplying it with the learnign rate and update the target\n",
    "        - <img src=\"../../images/grad_boost_5.png>\n",
    "    3. Output of  step 2 till residual is minimum or the # of trees are reached\n",
    "- Classification overview\n",
    "    - In this since the target is a class and not a numerical value, the log(odds) of the target is taken for each point\n",
    "        -  log(# of class 1/ #of class 2)\n",
    "    -  Logistic function of the log(odds) is also calculated\n",
    "        - <img src=\"../../images/grad_boost_6.png>\n",
    "    - This proability is then set against threshold of 0.5 to find new target constant values which will be 1 of prob is >=0.5 else 0\n",
    "    - New residuals will be calcuated = (observed/the initial constant probability - predicted)\n",
    "    - Build a tree to predict residuals\n",
    "    - In regression, if a leaf contains two values it can be averaged while in classification since it is a log odd's proability, it will be transformed using the following for each leaf\n",
    "    - <img src=\"../../images/grad_boost_7.png>\n",
    "    - once all the values for leaves are calculated, the constant prediction of single leaf will be updated using product of new * learning rate and adding it to the constant value\n",
    "    - This gives a new log(odds) which is used to get new probability using logistic function for each data point\n",
    "    - Now the residuals is calculated (observed - residuals)\n",
    "- Intution behind Gradient Descent Classification\n",
    "    - SKIPPING, too much to handle\n",
    "> Code   \n",
    "    ```python\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "    clf.score\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    reg = GradientBoostingRegressor(random_state=0)\n",
    "    reg.fit(X_train, y_train)\n",
    "    reg.predict(X_test[1:2])\n",
    "    reg.score(X_test, y_test)\n",
    "\n",
    "\n",
    "    # parameters : loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0\n",
    "\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- models can be computationally expensive and take a long time to train, especially on CPUs;\n",
    "- hard to interpret the final models.\n",
    "- overemphasize outliers and cause overfitting\n",
    "- large grid search during hyper tuning.\n",
    "- Less interpretative in nature\n",
    "\n",
    "+ Lots of flexibility - can optimize on different loss functions and provides several hyper parameter tuning options that make the function fit very flexible.\n",
    "+ train faster especially on larger datasets,\n",
    "+ most of them provide support handling categorical features, so no pre-processing required\n",
    "+ some of them handle missing values natively so imputation taken care of\n",
    "\n",
    "Source\n",
    "[youtube](https://youtu.be/3CC4N4z3GJc)\n",
    "[youtube](https://youtu.be/2xudPOBz-vs)\n",
    "[youtube](https://youtu.be/StWY5QWMXCw)\n",
    "### - Light Gradient boost\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Xtream Gradient\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source \n",
    "[youtube](https://www.youtube.com/watch?v=OtD8wVaFm6E)\n",
    "### - Catboost\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Isolation tree\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Isolation tree\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - SVM\n",
    "> Working\n",
    "- Overview\n",
    "    - Unlike in linear regression hwere the line is drawn considering all the points, here its only based on the points that are at the edges of the classes and closest to each other (support vectors)\n",
    "    - Margin : The distance between the support vectors from the hyper plane\n",
    "    - The aim is to have a hyperplane which has the largest margin from both the extreme points\n",
    "    - This uses maximum margin classifiers, which are suseptible to outliers\n",
    "    - Outliers must not affect the hyperplance so misclassification must be allowed to get the largest margins\n",
    "    - This is the bias variance tradeoff\n",
    "    - Using CV of these margins, the best margin is found and since its misclassifying, its called soft margins\n",
    "    - The points within the soft margin and on the edge are called Support Vector\n",
    "    - But when they fail like in case where class 1 is surrounded by class 2, support vector classifiers are useless\n",
    "    - Then comes support vector machines, which moves the data to higher dimensions\n",
    "        - This adds new dimensions to data which then can be used to find the hyperplane that seperates the seemingly unclassifyable data\n",
    "        - This adding of new dimension can be like squaring of data or root or cube or exp etc has to be considered based on the data\n",
    "        - This moving to higher dimension by transforming data is done using kernel functions\n",
    "        - Kernel : ‘linear’, ‘poly’, radial basis function, ‘sigmoid’, ‘precomputed’\n",
    "        - rbf : finds plane in infinite dimensions bt in 1 d acts as weighted nearest neighbour\n",
    "        - The kernel trick : not transforming the data but acting as if its transformed to find the hyperplane\n",
    "        - This makes computation easy\n",
    "    - Polynomial kernel\n",
    "        - (a x b + r)^d\n",
    "        - a and b are the data points its comparing, r is coefficient of polynomial and d is the polynomial degree passed as parameter\n",
    "        - by passing r and d, new coordinates of the points can be calculated, which are determined using cross validation\n",
    "    - RBF\n",
    "        - e^(-y(a-b)^2)\n",
    "        - y is to be found using CV\n",
    "        - can be expanded to infinity using taylor series\n",
    "> Code   \n",
    "    ```python\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "    clf.fit(X, y)\n",
    "    # parameters :  C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- not suitable for large data sets.\n",
    "- does not perform very well when the data set has more noise\n",
    "- underperforms when number of features for each data point exceeds the number of training data samples\n",
    "- here is no probabilistic explanation for the classification.\n",
    "- Choosing a “good” kernel function is not easy\n",
    "- time consuming\n",
    "- Difficult to understand and interpret the final model, variable weights and individual impact.\n",
    "- not that easy to fine-tune these hyper-parameters. It is hard to visualize their impact\n",
    "\n",
    "+ kernel trick is real strength of SVM for complex hyperplane\n",
    "+ SVM is not solved for local optima.\n",
    "+ the risk of over-fitting is less in SVM.\n",
    "+ works relatively well when there is a clear margin of separation \n",
    "+ more effective in high dimensional spaces.\n",
    "+ effective in cases where the number of dimensions is greater than the number of samples.\n",
    "+ relatively memory efficient\n",
    "\n",
    "Source : \n",
    "[youtube](https://www.youtube.com/watch?v=efR1C6CvhmE&pp=ygUNc3ZtIGV4cGxhaW5lZA%3D%3D)\n",
    "### - Validations\n",
    "> K fold\n",
    "- .\n",
    "> OOO\n",
    "- .\n",
    "> .\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Imbalanced \n",
    "> SMOTE\n",
    "- .\n",
    "> Imbalanced-learn\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Metrics\n",
    "> Confusion Matrix\n",
    "- <img src =\"../../images/conf_mat1.webp\">\n",
    "- <img src =\"../../images/conf_mat.webp\">\n",
    " - Type 1 (False Positive rate) = fp/(fp+tn)\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    false_negative_rate = fn / (tp + fn)\n",
    "    ```\n",
    " - Type 2 (False Negative Rate) = fn/(tp+fn)\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    false_positive_rate = fp / (fp + tn)\n",
    "    ```\n",
    "\n",
    "> Accuracy\n",
    "- (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "> Precision (Positive predict value)\n",
    "- how many of the correctly predicted cases actually turned out to be positive.\n",
    "- False Positive is a higher concern than False Negatives.\n",
    "- ex : music or video recommendation systems, e-commerce websites, etc. Wrong results could lead to customer churn and be harmful to the business.\n",
    "- ex : raising false alerts is costly, \n",
    "- <img src =\"../../images/pre_recall.png\">\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix, precision_score\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    precision = precision_score(y_true, y_pred_class) # or optionally tp/ (tp + fp)\n",
    "    ```\n",
    "\n",
    "> Recall\n",
    "- how many of the actual positive cases we were able to predict correctly with our model.\n",
    "- ex : really care about catching all fraudulent transactions even at a cost of false alerts\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix, recall_score\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    recall = recall_score(y_true, y_pred_class) # or optionally tp / (tp + fn)\n",
    "    ```\n",
    "> False Discovery Rate\n",
    "- measures how many predictions out of all positive predictions were incorrect. (1- precision)\n",
    "- When raising false alerts is costly and when you want all the positive predictions to be worth looking at you should optimize for precision.\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    false_discovery_rate = fp/ (tp + fp)\n",
    "    ```\n",
    "\n",
    "> F1, Threshold\n",
    "- harmonic mean of Precision and Recall, \n",
    "- Used when there is imbalance\n",
    "- Threshold can be found which will give the highest F1\n",
    "- It is maximum when Precision is equal to Recall. Not interpretable\n",
    "- <img src =\"../../images/f1_1.png\">\n",
    "- <img src =\"../../images/f1_2.png\">\n",
    "    - F1 beta\n",
    "        - Higher the beta higher regards to recall\n",
    "        - if beta =1 then its normal f1 score, beta=2 is f2 score and so on\n",
    "        - <img src =\"../../images/f1_3.png\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import fbeta_score\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    f1= f1_score(y_true, y_pred_class)\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    f2 = fbeta_score(y_true, y_pred_class, beta = 2)\n",
    "    ```\n",
    "\n",
    "> Specificity (True negative rate)\n",
    "- the number of correct negative predictions divided by the total number of negatives\n",
    "- where False Negative trumps False Positive.\n",
    "- ex :  important in medical cases where it doesn’t matter whether we raise a false alarm, but the actual positive cases should not go undetected!\n",
    "- ex : really want to be sure that you are right when you say something is safe\n",
    "- <img src =\"../../images/specificity.jpg\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    true_negative_rate = tn / (tn + fp)\n",
    "    ```\n",
    "\n",
    "> Sensitivity also recall also True Positive rate\n",
    "- the number of correct positive predictions divided by the total number of positives\n",
    "- ex : really care about catching all fraudulent transactions even at a cost of false alerts\n",
    "- <img src =\"../../images/sensitivity.jpg\">\n",
    "\n",
    "    ```python\n",
    "    # in recall below\n",
    "    ``` \n",
    "\n",
    "> AUC-ROC\n",
    "- visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart.\n",
    "- ROC score tells us how good our curve is, we can calculate the Area Under the ROC Curve, or ROC AUC score.\n",
    "    ```python\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from scikitplot.metrics import plot_roc\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_roc(y_true, y_pred, ax=ax)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_pos)\n",
    "    ``` \n",
    "\n",
    "> Cohen Kappa\n",
    "- Cohen Kappa tells you how much better is your model over the random classifier that predicts based on class frequencies.\n",
    "- mostly used when imbalance exists\n",
    "- <img src =\"../../images/cohen_kappa.webp\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "    cohen_kappa = cohen_kappa_score(y_true, y_pred_class)\n",
    "    ```\n",
    "\n",
    "> Matthews Correlation Coefficient \n",
    "- Imbalanced dataset and easy to interpret\n",
    "- <img src =\"../../images/mcc_eq.webp\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    matthews_corr = matthews_corrcoef(y_true, y_pred_class)\n",
    "    ```\n",
    "\n",
    "> Precision-Recall Curve\n",
    "- For every threshold, you calculate PPV and TPR and plot it.\n",
    "    ```python\n",
    "    \n",
    "    from scikitplot.metrics import plot_precision_recall\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_precision_recall(y_true, y_pred, ax=ax)\n",
    "\n",
    "    from sklearn.metrics import average_precision_score\n",
    "\n",
    "    avg_precision = average_precision_score(y_true, y_pred_pos)\n",
    "    ``` \n",
    "> Log loss\n",
    "- objective function that is optimized under the hood and can be used as metric\n",
    "- <img src =\"../../images/log_loss.webp\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import log_loss\n",
    "\n",
    "    loss = log_loss(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "> Kolmogorov-Smirnov plot\n",
    "- assess the separation between prediction distributions for positive and negative classes\n",
    "- problem is about sorting/prioritizing the most relevant observations and you care equally about positive and negative classes.\n",
    "- Good addition to roc score\n",
    "    ```python\n",
    "    from scikitplot.metrics import plot_ks_statistic\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_ks_statistic(y_true, y_pred, ax=ax)\n",
    "    ```\n",
    "\n",
    "Source\n",
    "[blog](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/#Precision_vs._Recall)\n",
    "[blog](https://neptune.ai/blog/evaluation-metrics-binary-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[blog](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/) TODO\n",
    "### Word Embeddings\n",
    "> Working\n",
    "- Aim is to convert words in to numbers to be fed into NN\n",
    "- Random number assignment can be done but that doesn signify the relation between two words like synonyms or opposites\n",
    "- Word embeddings map the words as real-valued numerical vectors. It does so by tokenizing each word in a sequence (or sentence) and converting them into a vector space. Word embeddings aim to capture the semantic meaning of words in a sequence of text\n",
    "- Bag of words : faster, simpler, low memory use\n",
    "- Skip gram : good for small datasets, takes care fo rare words too\n",
    "1. Feq based : TFIDF, count vector, co-occurance vector\n",
    "2. prediction based : word2vec, glove bert\n",
    "#### TF-IDF\n",
    "> Working\n",
    "- based on a statistical measure of finding the relevance of words in the text\n",
    "- combination of two metrics: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "- <img src=\"../../images/tf_idf_1.jpg>\n",
    "- <img src=\"../../images/tf_idf_2.jpg>\n",
    "- TF (i) = log (frequency (i,j)) / log (N (j))\n",
    "    - TF score is based on the frequency of words in a document. Words are counted for their number of occurrences in the documents. TF is calculated by dividing the number of occurrences of a word (i) by the total number (N) of words in the document (j).\n",
    "- IDF (i) = log (N (d) / frequency (d,i))\n",
    "    - IDF score calculates the rarity of the words. It is important because TF gives more weightage to words that occur more frequently. However, words that are rarely used in the corpus may hold significant information. IDF captures this information. It can be calculated by dividing the total number (N) of documents (d) by the number of documents containing the word (i).\n",
    "- The log is taken in the above formulas to dampen the effect of large scores for TF and IDF. \n",
    "- The final TF-IDF score is calculated by multiplying TF and IDF scores.\n",
    "> Code   \n",
    "    ```python\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tfidf = TfidfVectorizer()\n",
    "    result = tfidf.fit_transform(string)\n",
    "    tfidf.vocabulary_\n",
    "    result.toarray()\n",
    "\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- It cannot capture the semantic meaning of words in a sequence efficiently.\n",
    "- gives value to rare words there by overfitting\n",
    "- decimal values in vector\n",
    "\n",
    "Source\n",
    "[youtube]()\n",
    "[blog](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)\n",
    "\n",
    "#### Word2Vec\n",
    "> Working\n",
    "- It can iterate over a large corpus of text to learn associations or dependencies among words.\n",
    "- This is a pre-trained embedding model for freature representation but i can be trained from scratch also\n",
    "- Word2Vec finds similarities among words by using the cosine similarity metric. If the cosine angle is 1, that means words are overlapping. If the cosine angle is 90, that means words are independent or hold no contextual similarity.\n",
    "- two neural network-based variants: Continuous Bag of Words (CBOW) and Skip-gram.\n",
    "    - In CBOW, the shallow neural network model takes various words as input and predicts the target word that is closely related to the context of the input words\n",
    "        - The input layer will have window  size -1 input with vector of adjacent words\n",
    "        - The fully connected hidden layer with be of window size\n",
    "        - Embedding layer ???\n",
    "        - The output layer which predicts 1 word will have 1 layer with vector size neurons and softmax layer\n",
    "        - Or n to n+wth word's vector is taken as input and n+w+1th word's vector is expected as output and the training is done accordingly\n",
    "        - w can be 1 for the very next word or can be more than that\n",
    "        - inital vector representation will be an one hot encoded output\n",
    "        - after training, the vector representation of the words are finalized\n",
    "    - the Skip-gram architecture takes one word as input and predicts its closely related context words\n",
    "        - The input into the NN is the target word and n words to its left and right needs to be predicted\n",
    "        - this n will be the window size    \n",
    "        \n",
    "- CBOW is quick and finds better numerical representations for frequent words, while Skip Gram can efficiently represent rare words\n",
    "- Unlabeled data is trained via artificial neural networks to create the Word2Vec model that generates word vectors. \n",
    "- The size of the vector can be selected according to the corpus size and the type of project\n",
    "- So its not as sparse as the previous ones\n",
    "- \n",
    "> Code\n",
    "    ```python\n",
    "    from gensim.models import Word2Vec\n",
    "    w2v_model = Word2Vec(min_count=4,\n",
    "                         window=4,\n",
    "                         size=300, \n",
    "                         alpha=0.03, \n",
    "                         min_alpha=0.0007, \n",
    "                         sg = 1,\n",
    "                         workers=cores-1)\n",
    "    \n",
    "    w2v_model.build_vocab(w2v_df, progress_per=10000)\n",
    "    w2v_model.train(w2v_df, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n",
    "    return w2v_model\n",
    "    w2v_model.wv.most_similar(positive=[\"boston\"])\n",
    "\n",
    "    #parameters : min_count, window, size, alpha, min_alpha, sg, workers\n",
    "    ```\n",
    "> Pros and cons\n",
    "+ vector size is not large and not sparse\n",
    "+ semantic relations are preserved and can be checked using cosine similarity\n",
    "    - which is the cos of angle between the two word's vectors\n",
    "- Needs to be cleaned and stop words removed\n",
    "- Word2Vec only captures the local context of words.\n",
    "- During training, it only considers neighboring words to capture the context\n",
    "- cant handle rich morpholigical languages like  gender based languages\n",
    "Source\n",
    "[blog](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)\n",
    "[blog](https://towardsdatascience.com/word-embedding-techniques-word2vec-and-tf-idf-explained-c5d02e34d08)\n",
    "\n",
    "#### GLove\n",
    "- Global vectors is combination of predicting and count based word embedding techniques\n",
    "> Working\n",
    "- Rather than using a window to define local context, GloVe constructs an explicit word-context or word co-occurrence matrix using statistics across the whole text corpus. The result is a learning model that may result in generally better word embeddings.\n",
    "- GloVe considers the entire corpus and creates a large matrix that can capture the co-occurrence of words within the corpus\n",
    "- GloVe combines the advantages of two-word vector learning methods: matrix factorization like latent semantic analysis (LSA) and local context window method like Skip-gram.\n",
    "- GloVe technique has a simpler least square cost or error function that reduces the computational cost of training the model\n",
    "- Here window is used to look for words on either side\n",
    "- After word co-occurance is calculated, proability is calculated for each word wrt to another using\n",
    "- <img src = \"../../images/glove_1.png\">\n",
    "- where xij is occurance of i and love in the same window divided by total# of times the word has occured\n",
    "- A neural netwrok architecture is built to optimize the proability ratio objective\n",
    "- SKIPPING, too much maths\n",
    "> Code\n",
    "- .\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "+ GloVe performs significantly better in word analogy and named entity recognition problems\n",
    "+ better than Word2Vec in some tasks and competes in others\n",
    "Source\n",
    "\n",
    "Source\n",
    "[youtube](https://youtu.be/viZrOnJclY0)\n",
    "[youtube]()\n",
    "[blog](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp)\n",
    "[blog](https://neptune.ai/blog/word-embeddings-guide)\n",
    "\n",
    "#### Fastext\n",
    "> Working\n",
    "- Uses CBoW as Skip grams\n",
    "> Code\n",
    "- .\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "+ \n",
    "\n",
    "Source\n",
    "\n",
    "#### BERT\n",
    "> Working\n",
    "- belongs to a class of NLP-based language algorithms known as transformers.\n",
    "- massive pre-trained deeply bidirectional encoder-based transformer model \n",
    "    - Base ; 110m parameters\n",
    "    - Large : 340m\n",
    "- relies on an attention mechanism. It generates high-quality context-aware or contextualized word embeddings.\n",
    "- During the training process, embeddings are refined by passing through each BERT encoder layer.\n",
    "- For each word, the attention mechanism captures word associations based on the words on the left and the words on the right\n",
    "- Word embeddings are also positionally encoded to keep track of the pattern or position of each word in a sentence.\n",
    "- BERT can be improved by fine-tuning the embeddings on task-specific datasets.\n",
    "- \n",
    "> Code\n",
    "- .\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "+ \n",
    "Source\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "#### Intro :\n",
    "> https://www.youtube.com/watch?v=rPFkX5fJdRY\n",
    "- History\n",
    "    - RNN : Feedforward NN rolled out over time\n",
    "        - Deals with sequence data with order and gave raise to architectures like\n",
    "        - Vector to sequence model, which takes in input of fixed size and give output of any length, ex: image captioning\n",
    "        - Sequence to vector model, does the reverse, ex : sentiment analysis\n",
    "        - sequence to sequence model, is more popular as i/p and o/p are not fixed ex : lang translation\n",
    "        - But they are very slow to train, even after using truncated back progpagation through time\n",
    "        - Also during processing of long seq, there is a vanishing/exploding gradient issue\n",
    "    - LSTM : Are RNNs with Long-short term memory cell for neurons with activation function\n",
    "        - Allows past info to move on to the next cell but increases complexity\n",
    "        - There by retaining memory for long seq i.e. 10x of RNN\n",
    "        - But after this the trianing is even slower\n",
    "        - Since the flow of info is sequential and there is a dependency b/w past and next data, GPUs also wont help\n",
    "        - Also, there a doubt about the context capturing as the vector generated for each word depended on just the previous word\n",
    "    - Bi-directional RNN\n",
    "        - To counter previous only dependency, both previous and next were taken into account\n",
    "        - and concatenate the context\n",
    "        - But the context was not taken as a whole it was previous and after sepearated and then combined\n",
    "        - There by missing the overall context which can be taken if both are considered\n",
    "    - To solve the issue of parallaizing processing of sequential data Transformers were created\n",
    "    - It has encoder decoder architecture like RNNs but the input could be sent parallely\n",
    "    - Where RNN had to take in one word at a time for embedding and there were hidden state dependencies between the past nd the current input, Transformers took it all in one go\n",
    "    \n",
    "    <img src=\"../../images/transformers_1.png\">\n",
    "-  Transformer Architecture \n",
    "    - There are two inputs, one is the sentence to be translated and the other is translated sentence to train the n/w\n",
    "    - X_train goes into Input embeddin (encoder) and y_train into output embedding (decoder)\n",
    "    - Encoder block\n",
    "        - Input embeddings\n",
    "            - Since words are not numerical, they need to be converted into embeddings\n",
    "            - Such that they retain semantic meaning and not random numeric allotment\n",
    "            - This means in the embedding space, the words that relate must be closer than unrelated ones\n",
    "            - This space can be created from scratch for customized data or use existing pre-trianed model like word2vec or glove which was trained on internet\n",
    "            - The output of this will be a unique vector which would represent the word in space\n",
    "            - But sometimes, the words mean different in different context like a bank, which can be an institution and a river bank too\n",
    "        - Positional Encoders\n",
    "            - To solve the issue of same word having different meaning, a vector is created which will look at the position of the word in a sentence\n",
    "            - The original paper uses sin and cosine function to generate the vector with positional awareness\n",
    "            - The output is the embedding of word with contextual information\n",
    "            - In the paper the positional vector size is 512 diemnsions\n",
    "        - Encoder Block\n",
    "            - It has a multi headed attention layer and a feed fwd layer\n",
    "            - Attention :  which part of the input should receive focus\n",
    "                - This is used to create a vector for each word which shows how imp is the word wrt to other words in the i/p sentence\n",
    "                - ???\n",
    "            - Feed fwd net : Applied to each attention vector. So 1 n/w for each word, to convert each attention vector into a form that can be taken by the next encoder/decoder block\n",
    "            - The goal is to make attention vector more contextually aware than positional vectors\n",
    "    - Decoder block\n",
    "        - similar to encoder, it takes sentence, turns it into embedding and for context uses positional embedding \n",
    "        - Now starts the decoder block with a Masked-multi-headed attention, a multi headed attention and feed fwd net\n",
    "        - Here MMHA receives the output potisionla embedding which the MHA gets both output of MMHA and the output of previous Encoder block\n",
    "        - First the output vector is passed through attention to get attention vector\n",
    "        - Then the output, positional, attention vector is compared with the output of input encoder i.e. input positional attention vector in the encoder-decoder attention block\n",
    "        - The E-DA block will determine how related each vector of word from input is related to each vector of word from output\n",
    "        - This is where word mapping happens for translation scenario\n",
    "        - The o/p of this block is attention vector for each word of input and output\n",
    "        - This is then passed on to feed fwd n/w to make the vectors more digestible for next linear or decoder block\n",
    "        - Linear layer is another feed fwd network, which changes the dimensions of the output into # of words in the translted language\n",
    "        - Then the softmax layer changes vector into prob. distribution\n",
    "        - The final o/p is the word whocse vector gets the highest probability form softmax\n",
    "        - Which is the prediction of the next word\n",
    "        - This repeats entill the <EOS> is generated\n",
    "- 08:42\n",
    "    - The Encoder's Multi head attention block\n",
    "        - If a word needs to be rated based on its importance in the sentence, it will rank itself highest, wrt to other words, which might not be useful to find which word needs to be focused up on (give attention to)\n",
    "        - So 8 attention vectors are formed for eac word and averaged out ??? why cant they all be the same\n",
    "        - Since multiple vectors are generated to avgd out, its multi headed\n",
    "    - Encoder's feed fwd n/w\n",
    "        - This is where ||ly attention vectors for each word are sent to condense \n",
    "    - Decoder's MMHA\n",
    "        - Masked because here, training uses all the words from input but only the previous word from o/p to predict next word\n",
    "        - This is done by making the matrix value of hte other words in a sentence 0\n",
    "- 11:11\n",
    "    - MHA details\n",
    "        - The input to this layer are vectors which extacts different compoenent of input word in this case denoted by V K and Q\n",
    "        - This is used to calculate scale-dot product attention vector \n",
    "        - Since it is multiheaded all these vectors become matrices\n",
    "        - And the output is also attension vectors for each word but the next feed fwd layer expects just one vector so the output also gets weighted\n",
    "        <img src=\"../../images/transformers_3.png\">\n",
    "    - Add and Normalization layer\n",
    "        - Batch norm is across each sample but layer normalization, used here, is across each feature\n",
    "        - For optimization a with larger learning rate\n",
    "        <img src=\"../../images/transformers_2.png\">\n",
    "- 12:30\n",
    "    - Attention Mechanism\n",
    "        - Due to issues in RNN, BRNN of loss of context in vector generation\n",
    "        - Attention was used\n",
    "            - Where each word's focus, wrt other words in the sentence is found ??? using correlation how\n",
    "            - Since the sentence is focusing on itself its self-attention\n",
    "            - The output of a sentence from the encoder block is # of attention vectors = # of words\n",
    "            - but the way words are processed are by converting them into sub-words or word pieces or byte pair encoding\n",
    "            - This is sent into decoder while the actual input to decoder is a token <start>\n",
    "            - The output of the decoder will be the next word it predicts which will be again used as input for the decoder block to generate the second next word and so on till<stop> is generated\n",
    "    - Since its not serial unlike RNN, GPUs can be used to speed up the process\n",
    "    - The quality of the attenion vectors are higher than those created by previous word context in RNN \n",
    "#### Working\n",
    "> 19:06 Multi-Head Attention mechanism\n",
    "- <img src=\"../../images/transformers_4.png\">\n",
    "- <img src=\"../../images/transformers_6.png\">\n",
    "    - Each word thats is put through the MHA layer is divided into 3 vectors, ??? how do we get them\n",
    "        - Q : query : what to look for \n",
    "        - K : key : what to offer\n",
    "        - V : value : what can it offer\n",
    "    - The length of each Q K and V is the lenght of the sentence\n",
    "    - so for a sentence with 4 words, there will be 3*4*dk, dk can vary\n",
    "    - For self attention calculation for each word, where M is ??? looks like it can be ignored\n",
    "        - <img src=\"../../images/transformers_5.png\">\n",
    "        - sqr root of dimenation of q or k is used to minimize variance and stabalize Q.KT hence scaled\n",
    "        - this gives attention vector\n",
    "    - Decoder   \n",
    "        - For masking a triangular matrix (sentence length X sentence length) with bottom as 1 and top as 0 is created to prevent training to know the predicted word\n",
    "        - Now 1 is made a 0 and 0 as -infinity. so that when added with the softmax attention output it gets lower triangle populated with actual attention from encoder\n",
    "        - infitnity is useful as the combination of attention vector from encoder and masked vector from decoder is sent to softmax\n",
    "        - This will convert the vector into probability distribution\n",
    "        - Now, as per the diagram V is multipled with wht eoutput of softmax\n",
    "- This is done multiple times for multiple attention vectors and stack them\n",
    "- 26:45\n",
    "    - Code for multi head attention SKIPPING, more of less same except tensored\n",
    "    - .\n",
    "- 39:30 Positional encoders\n",
    "    - \n",
    "\n",
    "\n",
    "\n",
    "> Intro : https://www.youtube.com/watch?v=gM2k_GFrKrU\n",
    "#### Code\n",
    "#### Types (bert)\n",
    "#### Pros and cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLoud formation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow\n",
    "\n",
    "> [youtube](https://www.youtube.com/watch?v=qdcHHrsXA48&pp=ygULbWxmb3cgbWxvcHM%3D)\n",
    "- MLflow setup from scratch model perfromance comparing\n",
    "- Connect dagshub to github where the code resides\n",
    "- No drift and monitoring\n",
    "- Dagshub is for remote sharing of details that was on local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc\n",
    "#### End to End with KN [youtube](https://www.youtube.com/playlist?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "> [intro](https://youtu.be/S_F_c9e2bz4?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "1. Data ingestion\n",
    "2. data transformation\n",
    "3. model trainer\n",
    "4. model evaluation\n",
    "5. model development\n",
    "- CI/CD pieplines using github actions\n",
    "- Docker environments\n",
    "- Artifacts\n",
    "> [Video 1](https://youtu.be/Rv6UFGNmNZg?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "- Project and github setup\n",
    "    - seperate environment is created using virtualenv so that libraries dont mix up\n",
    "        - python3 -m venv <name of the folder>\n",
    "        - to activate the environment\n",
    "            - source .venv/bin/activate\n",
    "        - to deactivate\n",
    "            - deactivate\n",
    "        - To create files with libraries used and thier version\n",
    "            - python3 -m pip freeze > requirements.txt\n",
    "    - other git related stuff below in Git section\n",
    "    - a source folder is created with empty python file \"__init__.py\"\n",
    "        - Wherever the init.py exists that folder can be imported as a library\n",
    "        - usually this will contain the util functions\n",
    "    - setup.py\n",
    "        - this is used for packaging and using as a library else where\n",
    "        ```python\n",
    "        # find_packages identifies all the packages used in the application\n",
    "        from setuptools import find_packages, setup\n",
    "\n",
    "        HYPHEN_E = \"-e .\"\n",
    "        # Metadata of the application\n",
    "        setup(\n",
    "            name='...',\n",
    "            version='...',\n",
    "            author='...',\n",
    "            author_email='...',\n",
    "            packages=find_packages(),\n",
    "            install_requires=get_requirements('requirements.txt')\n",
    "        )\n",
    "        # a function to get read lines in requirements.txt (file_path)\n",
    "        def get_requirements(file_path):\n",
    "            requirements = []\n",
    "            with open(file_path) as requirements_obj:\n",
    "                requirements = requirements_obj.readlines()\n",
    "                requirements = [req.replace(\"\\n\",\"\") for req in requirements]\n",
    "\n",
    "                if HYPHEN_E in requirements:\n",
    "                    requirements.remove(HYPHEN_E)\n",
    "\n",
    "            return requirements\n",
    "        ```\n",
    "> [Video 2](https://youtu.be/sDWL30CzJT8?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "- This will contain addition to the project structure, Logging and exception handelling\n",
    "- /src/components\n",
    "    - to host functions that will be called upon \n",
    "    - helful for reuseability and compartmentalizing general functions\n",
    "    - This can contain files which are used to\n",
    "        - read data ex : data_ingestion.py\n",
    "        - transform ex: data_transformation.py\n",
    "        - training model ex : model_trainer.py\n",
    "    - this will also contain __init__.py\n",
    "- /src/pipeline\n",
    "    - has the __init__.py\n",
    "    - train_pipeline.py, which calls the components in a sequential manner\n",
    "    - predict_pipeline.py, which will be used for inferencing\n",
    "- /src/config.py\n",
    "    - to store configuration details of databases or cloud etc.\n",
    "- /notebooks\n",
    "    - for eda and experiments\n",
    "- /artifacts\n",
    "    - to store outputs that has significance\n",
    "- /model\n",
    "    - to store trained models\n",
    "- /src/logger.py\n",
    "    - for capturing information while the pipeline runs for debugging and tracing issues\n",
    "    ```python\n",
    "    import logging\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "\n",
    "    # construct a format for log file name\n",
    "    log_file = f\"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\"\n",
    "\n",
    "    logs_path = os.path.join(os.getcwd(), \"logs\", log_file)\n",
    "    os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "    log_file_path = os.path.join(logs_path, log_file)\n",
    "\n",
    "    # Handles logging information\n",
    "    logging.basicConfig(\n",
    "        filename=log_file_path,\n",
    "        format=\"[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s\",\n",
    "        level=logging.INFO\n",
    "    )\n",
    "    ```\n",
    "- /src/exception.py\n",
    "    - to handel exceptions\n",
    "    ```python\n",
    "        '''\n",
    "        Custom Exception Handling\n",
    "        '''\n",
    "\n",
    "        import sys\n",
    "\n",
    "        # a function to load the execution information and return the error message\n",
    "        def error_message_details(error, error_details:sys):\n",
    "            # exc_info() returns the information about the error\n",
    "            _,_,exc_tb = error_details.exc_info()\n",
    "\n",
    "            # get file name from the exc_info() function\n",
    "            file_name = exc_tb.tb_frame.f_code.co_filename\n",
    "\n",
    "            # construct the error message to return\n",
    "            error_message = \"Error occured in Python script [{0}] at line number [{1}] error message [{2}]\".format(file_name, exc_tb.tb_lineno, str(error))\n",
    "\n",
    "            return error_message\n",
    "\n",
    "        # Inheriting the Exception class\n",
    "        class CustomException(Exception):\n",
    "            def __init__(self, error_message, error_details:sys) -> None:\n",
    "                super().__init__(error_message)\n",
    "                self.error_message = error_message_details(error_message, error_details=error_details)\n",
    "\n",
    "            def __str__(self):\n",
    "                return self.error_message\n",
    "    ```\n",
    "- /src/utils.py\n",
    "    - functions which can be used anywhere as its written in a general manner without hardcoding\n",
    "\n",
    "> [Video 3](https://youtu.be/gqqGdu1P2FM?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "- Problem understanding, EDA and model training\n",
    "    - EDA can be done in Jupyter notebook\n",
    "    - Artifacts fromm that can be stored in project\n",
    "    - Column transformer pipeline\n",
    "        ```python\n",
    "        from sklearn.preprocessing import oneHotencoder, StandardScaler\n",
    "        from sklearn.compose import ColumnTransformer\n",
    "        numeric_transformer = StandardScaler() \n",
    "        oh_transformer = OneHotEncoder()\n",
    "        preprocessor = Columntransformer( [\n",
    "             (\"OneHotEncoder\", oh_transformer, cat_features), \n",
    "             (\"Standardscaler\", numeric_transformer, num_features),\n",
    "        ]\n",
    "        )\n",
    "        X = preprocessor.fit(X)\n",
    "        # then train and test 4 splits\n",
    "        ```\n",
    "    - All this in notebook but the actual code must be modularized and will be re-written in .py files as in project structure\n",
    "\n",
    "> [Video 4](https://youtu.be/_0v1UK7smBc?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "- Modular code of data_ingestion.py from MongoDB\n",
    "    - import exceptions, loggers, os and sys\n",
    "    - pandas, train test split form sklearn\n",
    "    - [data_ingestion.py](../../../../Codes/mlproject/src/components/data_ingestion.py)\n",
    "        - dataingestion class can be replace with config file\n",
    "    - Instead of print statements, use logging.info(\"message to be stored)\n",
    "> [Video 5](https://youtu.be/Zs2BZkgoivM?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "- Modular code of data_trnasformation.py and pipelines\n",
    "    - [data_transformation.py](../../../../Codes/mlproject/src/components/data_transformation.py)\n",
    "> [Video 6](https://youtu.be/EAWR1kFtEGo?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "    - [model_trainer.py](../../../../Codes/mlproject/src/components/model_trainer.py)\n",
    "> [Video 9](https://youtu.be/gbJn2Ls2QsI?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n",
    "    - Deployment In AWS Cloud Using CICD Pipelines\n",
    "    - Beanstalk\n",
    "    - \n",
    "\n",
    "\n",
    "#### Build Simpler Production ML Systems using Feature/Training/Inference Pipelines [youtube](https://youtu.be/Mna14aTtD8s)\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git\n",
    "- to initialize a folder to sync with repo by creating a hidden folder .git\n",
    "    - git init\n",
    "- git global config to set username and email\n",
    "    - git config --global user.name “[firstname lastname]”\n",
    "    - git config --global user.email “[valid-email]”\n",
    "- to add a file to the repo\n",
    "    - git add <file name>\n",
    "- to add all changes\n",
    "    - git add .\n",
    "- to commit the file to the repor\n",
    "    - git commit -m \"some message about this commit\"\n",
    "- to remove file from commit\n",
    "    - git rm <file name>\n",
    "- to check the status of commits or changes in repo in local vs repo in cloud\n",
    "    - git status\n",
    "- to push the commit\n",
    "    - git push -u origin <branch name>\n",
    "- to create a file which will have list of keywords which if found in any file will not be considered for uploading like keys or secrets\n",
    "    - touch .gitignore\n",
    "    - then type in words that if found should ignore \n",
    "- to clone a repository from cloud to local\n",
    "    - git clone <ssh link of the repo from github>\n",
    "- to see all the changes made to a repo\n",
    "    - git log\n",
    "- merge the specified branch’s history into the current one\n",
    "    - git merge <name of the branch>\n",
    "- switch to another branch and check it out into your working directory\n",
    "    - git checkout\n",
    "- to change branch\n",
    "    - git branch <name of the new branch to be created>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
