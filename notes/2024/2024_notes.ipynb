{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md' target='_blank'>../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md</a><br>"
      ],
      "text/plain": [
       "/home/radial/Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [youtube](https://youtu.be/ZTt9gsGcdDo)\n",
    "# Essential Matrix Algebra for Neural Networks, Clearly Explained\n",
    "FileLink('../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Regression?\n",
    "To predict value of something (target) based on values of multiple factors (features).\n",
    "Since it is supervised, it must have training data with values of both target and features to get a sum of products (equation) which would have be the best among all possible equations describing the relations between the two by reducing the amount of distance between the actual value and the value predicted by the equation's line.\n",
    "Depending on the nature of the factors it will be linear (straight line/plane/hyperplane) or non linear (curved).\n",
    "\n",
    "Linear : target = A*feature1 + B*feature2 + C*Feature3 + x\n",
    "Non Linear : target = A*feature1^4 + B*feature2^2 + C*Feature3 + x (order is 4, as thats the highest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Assumptions\n",
    "    1. Linearity: The relationship between the dependent and independent variables is linear and additive. Must have linear betas.\n",
    "        > A linear relationship suggests that a change in response Y due to one unit change in X¹ is constant, regardless of the value of X¹.\n",
    "        > An additive relationship suggests that the effect of X¹ on Y is independent of other variables.\n",
    "        > If this is ignored, then the line will be off the actual value by a lot and the model will not accurately represent the data\n",
    "\n",
    "        > How to check if this is met before modeling: Plot scatter plot of each feature against the target and see the pattern. If its like a stright line then fine\n",
    "\n",
    "        > How to check after modeling : Plot residual vs predicted values and if there is any pattern and lack of randomness across the middle like there a non linear relationship was involved\n",
    "\n",
    "        > How to fix : If its not then polynomial regression might be needed on the variable in the equation by applying transformations like log or exp or root or power etc depending on the relation between that variable and the target\n",
    "\n",
    "\n",
    "    2. Independence (No Autocorrelation in residuals/errors): The observations are independent of each other.\n",
    "        > This means that a residual of an observation should not predict the next observation.\n",
    "        > Usually an issue in time series but if features are not choosen wisely, if can occur in tabular too\n",
    "        > Might be caused by missing variable\n",
    "\n",
    "        > How to check before modeling : We can conduct durbinWatsonTest on our model for an output with a p-value, which will determine whether the assumption is met or not. \n",
    "        The null hypothesis states that the errors are not auto-correlated with themselves (they are independent). Thus, if we achieve a p-value > 0.05, we would fail to reject the null hypothesis.\n",
    "        It must lie between 0 and 4. If DW = 2, implies no autocorrelation, 0 < DW < 2 implies positive autocorrelation while 2 < DW < 4 indicates negative autocorrelation.\n",
    "\n",
    "        > How to check after modeling : Draw the line over the points and look at the residual plot for patterns. If exists then there is no independence. Must be random.\n",
    "        Residual vs time plot and look for the seasonal or correlated pattern in residual values.\n",
    "\n",
    "        > How to fix :\n",
    "        Do more domain searching to add missing variables\n",
    "        Depending on the equation, or the relation between x and y, add polynomial or exp or log\n",
    "        <img src=\"../../images/autocorr_correction.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "\n",
    "    3. Homoscedasticity: The variance of the errors is constant across all levels of the independent variables.\n",
    "        > The variance of the residuals should not vary as the target is increased in residual plot\n",
    "        > Generally, non-constant variance arises in presence of outliers or extreme large values. \n",
    "        > If a feature has a wide range of numbers then this can happen as the feature might not be a good predictor\n",
    "\n",
    "        > How to check before modeling: Plot a variable against the target, find best fit and look at residual plot. If there is a funnel/dual cone/reverse funnel pattern then it might be heteroscedastic. \n",
    "\n",
    "        > How to check after modeling : residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern. Wreusch-Pagan / Cook – Weisberg test or White general test or Goldfledt-Quant test to detect this phenomenon\n",
    "\n",
    "        > How to fix it : There might be two/multiple regression liens that can explain if divided based on pattern. Log variables might help.\n",
    "        <img src=\"../../images/hetero_example.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "    4. Normality: The errors follow a normal distribution.\n",
    "        > The spread of the errors at a given point, should be normally distributed\n",
    "        > prediction errors/residuals need to be normally distributed\n",
    "        > Sometimes the error distribution is \"skewed\" by the presence of a few large outliers.\n",
    "        > Calculation of confidence intervals and various significance tests for coefficients are all based on the assumptions of normally distributed errors.\n",
    "        > If the error distribution is significantly non-normal, confidence intervals may be too wide or too narrow. \n",
    "        > The dependent and independent variables in a regression model do not need to be normally distributed by themselves   \n",
    "        > the normal error assumption is usually justified by appeal to the central limit theorem, which holds in the case where many random variations are added together.\n",
    "\n",
    "        > How to check before modeling: plot a histogram on your residuals or employ a Q-Q plot, which also helps us visually determine if our residuals follow a normal distribution. if there is a skew in your residuals. A log transformation on your dependent variable may help.\n",
    "\n",
    "        > How to check after modeling : The Kolmogorov-Smirnov test, the Shapiro-Wilk test, the Jarque-Bera test, and Anderson-Darling test.\n",
    "\n",
    "        > How to fix it : check the extreme values of the variable\n",
    "\n",
    "    5. No multicollinearity: The independent variables are not highly correlated with each other.\n",
    "        > independent variables show moderate to high correlation\n",
    "        > tough task to figure out the true relationship of a predictors with response variable\n",
    "        > difficult to find out which variable is actually contributing to predict the response variable.\n",
    "        > the standard errors tend to increase and the confidence interval becomes wider leading to less precise estimates of slope parameters\n",
    "        > two variables vary very similarly and contain the same kind of information.\n",
    "        > only the complexity of the model increase, and no new information or pattern is learned by the model.\n",
    "        > \n",
    "\n",
    "        > How to check before modeling:  use scatter plot/heat map to visualize correlation effect among variables\n",
    "\n",
    "        > How to check after modeling : VIF factor. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity.\n",
    "        VIF = 1/(1-R2) for each variable\n",
    "\n",
    "        > How to fix it : drop and check vif\n",
    "\n",
    "    6. No endogeneity: There is no relationship between the errors and the independent variables.\n",
    "        > Causes of endogeneity include omitted variables, simultaneity, and sample selection in the estimated equation. \n",
    "        > OLS estimator will be inconsistent and biased,\n",
    "\n",
    "        > How to check before modeling:  \n",
    "\n",
    "        > How to check after modeling : Durbin-Wu-Hausman test, which compares the OLS estimator with an alternative estimator that is robust to endogeneity, such as the instrumental variables (IV) estimator.\n",
    "\n",
    "        > How to fix it : reduce the sources of endogeneity by including more relevant variables, improving the data quality, or using a different research design that avoids the causal ambiguity.\n",
    "        instrumental variables (IV) estimator, which uses a variable that is correlated with the endogenous explanatory variable but not with the error term\n",
    "\n",
    "Source : \n",
    "[youtube](https://www.youtube.com/playlist?list=PLTNMv857s9WUI1Nz4SssXDKAELESXz-bi)\n",
    "[blog 0](https://medium.com/@andrewhnberry/checking-your-linear-regression-assumptions-and-how-to-check-them-338f770acb57)\n",
    "[blog 1](https://godatadrive.com/blog/basic-guide-to-test-assumptions-of-linear-regression-in-r)\n",
    "[blog 2](https://people.duke.edu/~rnau/testing.htm)\n",
    "[blog 3](https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/)\n",
    "[blog 4](https://www.geeksforgeeks.org/assumptions-of-linear-regression/)\n",
    "[blog 5](https://www.jmp.com/en_in/statistics-knowledge-portal/what-is-regression/simple-linear-regression-assumptions.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Working\n",
    " * The aim is to find BLUE (best linear unbiased estimate) or the best fit line which explains all the variance across the data\n",
    " * This is usually an equation involving target on one side and linear additive combination of features on the other which are not in polynomial or expoenential or logarithmic form\n",
    " * y = coefficient + ax1 + bx2 + cx3 + dx4 | ... | + error\n",
    " * where a,b,c,d ... are slope values and coefficient is the intercept in multi dimensional plane\n",
    " * here coeeficient is the value when features are 0\n",
    " * and slope is the unit change in target wrt. unit change in feature\n",
    "\n",
    " * Cost function : to reduce overall distance between each point in dataset and predcited value by the line\n",
    " * in the below equation is one way to find the distance b/w predicted and actual, this can be replaced with absolute also. where m is total number of values\n",
    "\n",
    "  <img src=\"../../images/linear_cost.png\">\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/1-OGRohmH2s)\n",
    "\n",
    "* Gradient Descent\n",
    "\n",
    "  - Infinite number of lines can be drawn and then the cost function can be calculated for all but thats computationally expensive\n",
    "  - So by keeping coefficent constant, and varying the slope values, for all the variables values, cost function is calculated.\n",
    "  - Then a grpah is drawn between all possible m values and cost function's output to find the lowest point (gloabl minima) on the curve which will give the best m for line \n",
    "  - This finding of global minima (slope = 0) is using gradient descent which is based on convergence theorem which moves towards minima using derivative values of slope and a learning rate\n",
    "  - To know which way is global minima, all the sides needs to be considered before moving in any direction\n",
    "\n",
    "* LR using Ordinary Least square (OLR)\n",
    "- TODO  https://youtu.be/KZ1mWboXE6g\n",
    " \n",
    "* Multiple LR using Matrix \n",
    "  - In this image, the a is all the beta values including coefficient\n",
    "  - <img src=\"../../images/multi_reg_matrix.png\"> \n",
    "    \n",
    "  - Using direct formula : (Xt.X)^-1.Xt.Y\n",
    "  - Using single valur decomposition\n",
    "  - Using QR decomposition\n",
    "\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/_OyKjstWe80)\n",
    "[blog](https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0, b1, b2 = [-2.40119363  1.62732095 -0.29177719]\n",
      "Predictions = [1. 2. 3.]\n",
      "Error = [-0. -0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# MLR using Matrix\n",
    "import numpy as np\n",
    "x = np.array([[100, 212, 356], [4.25, 10.99, 16.2],  [-234, -465, -678]], np.int32)\n",
    "y = np.array([1,2,3])   \n",
    "# formula : (Xt.X)^-1.Xt.Y\n",
    "betas = np.dot(np.dot(np.linalg.inv(np.dot((x.T), x)),(x.T)),y)\n",
    "np.set_printoptions(suppress=False)\n",
    "print(\"b0, b1, b2 = {}\".format(betas))\n",
    "preds = np.dot(x,betas)\n",
    "print(\"Predictions = {}\".format(preds))\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"Error = {}\".format(preds-y))\n",
    "\n",
    "#source : https://cmdlinetips.com/2020/03/linear-regression-using-matrix-multiplication-in-python-using-numpy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Code\n",
    "\n",
    "source : [blog](https://www.freecodecamp.org/news/data-science-with-python-8-ways-to-do-linear-regression-and-measure-their-speed-b5577d75f8b/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 1.0\n",
      "Betas : [ 0.00863046  0.01082108 -0.01180701]\n",
      "Beta0 : 2.046179467416178\n",
      "Predictions : [2.11436926]\n"
     ]
    }
   ],
   "source": [
    "# MLR using scikit learn\n",
    "# !pip install scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(x, y)\n",
    "print(\"Score : {}\".format(reg.score(x, y)))\n",
    "print(\"Betas : {}\\nBeta0 : {}\".format(reg.coef_, reg.intercept_))\n",
    "print(\"Predictions : {}\".format(reg.predict(np.array([[3, 5, 1]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betas : [-2.40119363  1.62732095 -0.29177719]\n",
      "Model : \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                    nan\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Tue, 02 Jan 2024   Prob (F-statistic):                nan\n",
      "Time:                        17:16:02   Log-Likelihood:                 81.846\n",
      "No. Observations:                   3   AIC:                            -157.7\n",
      "Df Residuals:                       0   BIC:                            -160.4\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -2.4012        inf         -0        nan         nan         nan\n",
      "x2             1.6273        inf          0        nan         nan         nan\n",
      "x3            -0.2918        inf         -0        nan         nan         nan\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   1.012\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.488\n",
      "Skew:                           0.643   Prob(JB):                        0.784\n",
      "Kurtosis:                       1.500   Cond. No.                     1.42e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.42e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 3 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1795: RuntimeWarning: divide by zero encountered in divide\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1795: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1717: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n"
     ]
    }
   ],
   "source": [
    "# MLR using statsmodel\n",
    "# !pip install statsmodels\n",
    "import statsmodels.api as sm\n",
    "model = sm.OLS(y,x)\n",
    "results = model.fit()\n",
    "print(\"Betas : {}\".format(results.params))\n",
    "print(\"Model : \\n{}\".format(results.summary()))\n",
    "# source  : [blog](https://datatofish.com/statsmodels-linear-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Output analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Metrics\n",
    "> MSE : Mean Squared error\n",
    "- Considers square of distance between actual and predicted value\n",
    "- Since its a quadratic in nature, the curve is differentiable at all points to find minima\n",
    "- one local and global minima as its a paraobla\n",
    "- Can tackle outliers as the distance is squared.\n",
    "- Since its a square the units are not the same for easy explanation\n",
    "\n",
    "> RSME\n",
    "- This is root of mse\n",
    "- suffers from same outlier impact\n",
    "- but has the same unit\n",
    "- also is differentiable at all points\n",
    "\n",
    "> RSLME\n",
    "- log of RSME to slows down the scale of error\n",
    "\n",
    "> MAE : Mean Absolute error\n",
    "- absolute difference between predicted and actual value\n",
    "- so the nature of the functionis like a V and is not differentiable in the origin\n",
    "- but is robust as outliers dont affect it as much as mse\n",
    "- Also since its |y-yhat| the unit is same for explainability\n",
    "- but its optimization takes time and is complicated as sub gradients needs to be calculated\n",
    "\n",
    "> MAPE\n",
    "- percentage of MAE, i.e. (MAE*100)/N\n",
    "\n",
    "> R2\n",
    "- 1-((square of sum of actual minus predicted)/(square of sum of actual minus mean))\n",
    "- 0.8-0.90 is good depending on the case\n",
    "- 1 might indicate overfitting\n",
    "- always increases if more features are added and might over fit so must be penalized\n",
    "    \n",
    "> Adjuested R2\n",
    "- To prevent increasing of R2 by adding useless features\n",
    "- if the feautre is not good, it will reduce the adjusted R2\n",
    "- 1-((1-R2)(N-1))/(N-P-1)\n",
    "- N is # of points\n",
    "- P is # of variables\n",
    "\n",
    "Source \n",
    "[youtube](https://youtu.be/BGlEv2CTfeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Regularization\n",
    "- When model is overfitting, i.e. good on training and bad on val and test, it might be learning the noise and not understanding the pattern\n",
    "- Overfitting models have high variance\n",
    "- When it doesn perform well even in the training then its underfitting, can be solved by making the model more complex or adding new features\n",
    "- Simple model which underfits is highly biased\n",
    "- Regularization helps in bias variance trade off by penalizing models for overfitting\n",
    "<img src = \"../../images/under_over_fit.png\">\n",
    "<img src  = \"../../images/regularization.webp\">\n",
    "\n",
    "\n",
    "> Lasso - Least absolute shrinkage and selection operator regression (L1)\n",
    "- The cost function used in regression where distance is squared, a product of lambda and absolute slope is added to penalize.\n",
    "- If there are features in a model which doesn contribute much, LASSO makes its weight to 0 there by acting as feature selector\n",
    "- <img src = \"../../images/lasso.png\">\n",
    "- m is # of features, n is # of samples, w is the slope of the feature, yi is actual and yhat is predicted\n",
    "- lambda is the hyperparameter, selected using cross valdidation (0-infinity)\n",
    "- When the value of lambda is very high, it will make the weights of slopes of features which dont contribute much go to 0\n",
    "- since the absolute graph is like a diamond which has 0\n",
    "\n",
    "    ```python\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    lasso=Lasso()\n",
    "    parameters={ 'alpha' :[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,28, 38,35,55,100]}\n",
    "    lasso_regressor=GridSearchCV( lasso, parameters, scoring='neg_mean_error' ,cv-5)\n",
    "    lasso_regressor. fit(X,y)\n",
    "    print(lasso_regressor.best_params_)\n",
    "    print(lasso_regressor.best_score_)\n",
    "    ```\n",
    "\n",
    "> Ridge - (L2)\n",
    "- The cost function is modified by adding a squared value of the slope\n",
    "- <img src = \"../../images/ridge.png\">\n",
    "- This reduces overfitting\n",
    "- lambda is the hyperparameter (0-infinity)\n",
    "- high lambda will reduce the weight but not make it 0\n",
    "- since square graph is like a circle and doesn go to 0\n",
    "\n",
    "    ```python\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.model_ selection import GridSearchcv\n",
    "    ridge=Ridge() \n",
    "    paraneters={ \"alpha\" :[1e-15,le-10,1e-8,1e-3,1e-2,1,5,16,20, 38, 35,46,45,58,55,168]}\n",
    "    ridge_regressor=Gridsearchcv( ridge, parameters, scoring='neg_mean_squared_error',cv=5)\n",
    "    ridge_regressor.fit(x,y)\n",
    "    ```\n",
    "\n",
    "> Elastic net (L1 and L2)\n",
    "- A combination of both Lasso and Ridge\n",
    "- Cost function has both absolute and squared values of slope added\n",
    "- <img src = \"../../images/elastic.png\">\n",
    "- this has two hyper parameters alpha and lambda\n",
    "\n",
    "    Source  :    \n",
    "    [youtube](https://youtu.be/2bx9Os2gPu4)\n",
    "    [youtube](https://www.youtube.com/watch?v=9lRv01HDU0s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "### - Basics\n",
    "> Parametric vs non parametric\n",
    "-\n",
    "> Grid vs random search CV\n",
    "-\n",
    "> class_weight\n",
    "- parameter to tell the model if the dataset target is balanced or not \n",
    "> Bootstrapping\n",
    "- Process of randomly selecting rows of data with replacement to generate mode datasets\n",
    "- Used usually in ensemble models like Random Forests \n",
    "> Bagging\n",
    "- Combination of bootstrapping and aggregation (majority wins) this is a paralell process as all the different sets are independent of each other\n",
    "> Boosting\n",
    "- This is a sequential process to work on weak learners by boosting the values of misclassified data points\n",
    "- first build a model on the training dataset and then build a second model to rectify the errors present in the first model\n",
    "> Stacking\n",
    "- \n",
    "> Blending\n",
    "- \n",
    "> Optimizer\n",
    "- \n",
    "### - Logistic\n",
    "> Working\n",
    "- For binary classification, things can be done using linear regression and a threshold like 0.5 can be set\n",
    "- but this might be susceptible to outliers\n",
    "- The aim of Log Reg is to find a best fit line which will linearly seperate the groups\n",
    "- equation is same as regression y = mx+c\n",
    "- The idea is to convert multidimensional points into just 0 or 1\n",
    "- This is done using sigmoid function\n",
    "- <img src = \"../../images/sigmoid.png\">\n",
    "- Instead of equation from Lin reg, if there is a sigmoid of that equation, the predicted value will always be b/w 0,1\n",
    "- This also gives the proabability given the value x, what are the chances the point belongs in either classes\n",
    "- Where the predicted value will be perpendicular distance of the point from the line\n",
    "- The unknown variable is the wieghts that needs to be assigned to each feature\n",
    "- <img src =\"../../images/log_weights.png\">\n",
    "- Here w is the weights, which laong with x which is the input, goes through summation and sigmoid non linear function to get predictions.\n",
    "- Which is then used to find the error or the loss function which then is partially derivated using gradient descent to find the global minima\n",
    "- <img src = \"../../images/log_chain.png\">\n",
    "- The cost function is\n",
    "- <img src = \"../../images/log_cost.png\">\n",
    "- This needs to be partially differentiated \n",
    "- <img src = \"../../images/log_diff.png\">\n",
    "- Difference between lir and log reg is that\n",
    "    - the yhat is sigmoid in log\n",
    "    - the cost function is not convex so there are many local minima and the initial parameters of weights determine where the local minima will be found after grad desct (no guarantee of global minima)\n",
    "- the Logit/log odds function\n",
    "- <img src = \"../../images/logit.png\">\n",
    "- Regularization also can be done\n",
    "- <img src =\"../../images/log_regularization.png\">\n",
    "\n",
    "> Code\n",
    "```python\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "    clf.predict(X[:2, :])\n",
    "    clf.predict_proba(X[:2, :])\n",
    "    clf.score(X, y)\n",
    "    # parameters : penalty, C, solver, \n",
    "```\n",
    "\n",
    "> Pros and cons\n",
    "+ Simple to build and explain\n",
    "+ less overfitting in lower dimensions\n",
    "- creates linear boundaries, we won't obtain better results when dealing with complex or non-linear data\n",
    "- cant be used if the number of observations is fewer than the number of features; otherwise, it may result in overfitting.\n",
    "- affected by outliers\n",
    "- has to qualify the assumptions like in linear regression\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/z9XAXXGwUzM)\n",
    "[youtube](https://youtu.be/xXvgkILaFT4?list=PLyqSpQzTE6M-SISTunGRBRiZk7opYBf_K)\n",
    "[blog](https://www.analyticsvidhya.com/blog/2021/10/building-an-end-to-end-logistic-regression-model/)\n",
    "\n",
    "### - Decision Tree\n",
    "> Assumptions\n",
    "- If the values are continuous then they are discretized prior to building the model\n",
    "- \n",
    "> Working\n",
    "- Works for bth regression and classification\n",
    "- Its multiple linear lines, not exactly non linear but better than just one linear line\n",
    "- hierarchical tree structure consisting of a root node, branches, internal nodes, and leaf nodes\n",
    "- algorithmic model utilizes conditional control statements and is non-parametric\n",
    "- The tree is built by splitting on each feature and the hierarchy of feature is decided based on the purity of the feature in predicting the target\n",
    "- The purity is based on\n",
    "    - Entropy and information gain\n",
    "        - <img src=\"../../images/decision_entropy.png\">\n",
    "        - <img src=\"../../images/decision_entropy_ex.png\">\n",
    "        - entropy before the split and after the split is compared\n",
    "        - <img src=\"../../images/decision_entropy_ex2.png\">\n",
    "        - Entropy is found for all features and which ever has the mx change is taken\n",
    "        - I.e. higher the Entropy, the lower will be the purity\n",
    "        - Which ever feature gives most information gain is used as node as thats the most purest\n",
    "        - Steps\n",
    "            - First entropy of the whole target is calulated\n",
    "            - Entropy of each feature is calculated for each class in it\n",
    "            - Information gain of each feature is found, which is total entropy minus (individual class entropy of the feature into its proability)\n",
    "            - Which feature has the highest IG becomes the root node with brances as its classes\n",
    "            - Now for each of the branches, its takes subset by filtering onlny for those value and then IG is calculated again to find the next split\n",
    "            - This continues, till there are leaf nodes that are pure split\n",
    "    - Gini index\n",
    "        - Unlike IG, minimum GI is taken as root node as its 1-(probability)\n",
    "        - <img src=\"../../images/decision_gini.png\">\n",
    "        - Steps\n",
    "            - Almost the same just that lowest GI will be used as root and for splitting\n",
    "    - <img src=\"../../images/decision_gi_IG.png\">\n",
    "    - [youtube](https://youtu.be/5aIFgrrTqOw)\n",
    "    - Entropy ranges from 0-1\n",
    "    - GI ranges from 0-0.5\n",
    "    - GI doesn have log and hence is faster\n",
    "- The nodes have threshold in case of numerical values\n",
    "    - [youtube](https://youtu.be/5O8HvA9pMew?list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe)\n",
    "    - Sorts the tables in ascending manner\n",
    "    - Each number is taken and IG or GI is calculated\n",
    "    - Time consuming for large dataset\n",
    "- For regression tree \n",
    "    - TODO\n",
    "- Pruning : When we remove sub-nodes of a decision node, cutting down of branches\n",
    "- Early stopping :  not allowing the layes to go beyond a point\n",
    "> Code\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(criterion ='entropy').fit(X_train,y_train)\n",
    "prediction = tree.predict(X_test)\n",
    "tree.score(test_features,test_targets)\n",
    "# parameters : max_depth, min_samples_split, criterion, min_samples_leaf, max_features, \n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "cross_val_score(regressor, X, y, cv=10)\n",
    "```\n",
    "> Pros and cons\n",
    "+ Highly explainable\n",
    "+ resistant to outliers\n",
    "- Generally overfits, hence high variance\n",
    "- parametric, hence new data point if added, needs retraining\n",
    "- not suitable for large datasets as its just one tree\n",
    "- not for imbalanced dataset\n",
    "\n",
    "Source \n",
    "[youtube](https://youtu.be/uPwZ8DhQNjg?list=PLyqSpQzTE6M-SISTunGRBRiZk7opYBf_K)\n",
    "[youtube](https://youtu.be/CWzpomtLqqs)\n",
    "[blog]()\n",
    "### - Random Forest\n",
    "> Working\n",
    "- DT are highly sensitive to training data and can overfit without generalizing\n",
    "- First multiple datsets are generated using bootstraping\n",
    "- Then for these randomly selected rows, randomly selected features are selected \n",
    "    - The usual number for feature # is log or sqr root of total # of features\n",
    "- These two add diversity to the datasets and reduces chances of overfitting\n",
    "- Then for each dataset DTs are created\n",
    "- For predicting, the data point is passed through all the trees and the output is generated\n",
    "- Majority wins (this is aggregation)\n",
    "- bootstrapping + aggregation = bagging\n",
    "- In case of regression, a mean or median can be considered as the output\n",
    "\n",
    "> Code\n",
    "    ```python\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf.fit(X, y)\n",
    "    clf.predict([[0, 0, 0, 0]])\n",
    "    clf.feature_importances_\n",
    "\n",
    "    # parameters : (n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "    regr.fit(X, y)\n",
    "    regr.predict([[0, 0, 0, 0]])\n",
    "\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- Complex computation, slower\n",
    "- interpretation is difficult\n",
    "\n",
    "+ Scaling is not really required\n",
    "+ Doesn care about unwanted features\n",
    "+ Generalized\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/v6VJ2RO66Ag)\n",
    "\n",
    "### - Ada Boost\n",
    "> Working\n",
    "- Boosting algorithms improve the prediction power by converting a number of weak learners to strong learners.\n",
    "- In Adaptive boosting, the weights are re-assigned to each instance, with higher weights assigned to incorrectly classified instances\n",
    "- Overview\n",
    "    - To begin with  it builds a model and gives equal weights to all the data points\n",
    "    - then assigns higher weights to points that are wrongly classified\n",
    "    - Now all the points with higher weights are given more importance in the next model\n",
    "    - keep training models until and unless a lower error is received.\n",
    "- Base learner\n",
    "    - The inital models are based on shallow trees which are just stumps based on gini index for each feature. Which ever has the least entropy or impurity or gini index will be selected as a base learner\n",
    "    - Once the stump is made and the predictions are ready, total erro will be calculated by summing up all the weights which were misclassified \n",
    "    - Performance of the model will be calculated using\n",
    "    - <img src =\"../../images/ada_boost_1.png\">\n",
    "    - Now to update the weights, first for the misclassified ones with positive performance in exp\n",
    "    - <img src =\"../../images/ada_boost_2.png\">\n",
    "    - Then for the properly classified ones, with negative performance in exp\n",
    "    - This new updated weights might not add up to 1 unlike the inital weights \n",
    "    - So the new weights are divided by the sum of all new weights to get fractions that will add upto 1\n",
    "    - This will be the normalized weights which will be used to create new dataset for boosting\n",
    "- Boosted dataset\n",
    "    - This will be created by again sampling from orginal dataset\n",
    "    - A random number is selected and based on which bucket it falls on the cumilative normalized weight, that data point is inserted in the dataset\n",
    "    - Since, the weight is high for the misclassified data point, it will get selected more often based on central limit theorem ???\n",
    "    - There by creating a new dataset with empahsis on misclassified data point with the same # of rows as the original\n",
    "    - This is then again used to create base learner like earlier with equal weightage\n",
    "    - The process then repeates till # of base learner = # specified in parameter\n",
    "- For predicting, all the stumps are used and which ever class gets the stumps with high summed up performance will win\n",
    "> Code   \n",
    "    ```python\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    clf.fit(X, y)\n",
    "    clf.predict([[0, 0, 0, 0]])\n",
    "    clf.score(X, y)\n",
    "\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "    regr.fit(X, y)\n",
    "    regr.predict([[0, 0, 0, 0]])\n",
    "    regr.score(X, y)\n",
    "    # Parameters : estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated'\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- Adaboost is that it needs a quality dataset, outliers might disturb it\n",
    "- Slower than XGboost\n",
    "- Computationally expensive\n",
    "- Not good for imbalanced\n",
    "\n",
    "+ Flexible mode\n",
    "+ Adaboost is less prone to overfitting as the input parameters are not jointly optimized\n",
    "+ extended beyond binary classification and has found use cases in text and image classification as well\n",
    "\n",
    "Source \n",
    "[blog](https://www.analyticsvidhya.com/blog/2021/09 adaboost-algorithm-a-complete-guide-for-beginners/)\n",
    "[Youtube](https://youtu.be/LsK-xG1cLYA)\n",
    "\n",
    "### - Gradient Boost\n",
    "> Working\n",
    "- Overview\n",
    "    - Unlike stumps, here small trees with 8-32 leafs are built\n",
    "    - \n",
    "- Steps (regression)\n",
    "    - First an average of the target is taken and is used as predicted\n",
    "    - Next difference b/w the avg and actual target is calulated as a new column (residuals)\n",
    "    - The new column now becomes the target and a small decision tree is built\n",
    "    - If a leaf has two residuals, its averaged\n",
    "    - Now the residuals are predicted and added with the average which might overfit (new target)\n",
    "    - hence a parameter is multiplied with the residual before adding which is the learning rate and is a hyperparameter (0-1)\n",
    "    - Lots of small steps in the right direction by keeping the learning rate small leads to better accuracy\n",
    "    - Now the new target is used to find new residuals again to repeat the process\n",
    "    - Now the average target + lr * tree 1's prediction + lr * tree 2's prediction ...\n",
    "    - Till the difference between the prediction and actual is small or the # of trees has reached specified limit\n",
    "- Intution behind Gradient Descent Regression\n",
    "    - This needs a loss function thats differentiable\n",
    "    - which can be anything as its flexible. Usually a simple regression loss function\n",
    "    0.5(actual - predicted)^2 is taken\n",
    "    - 0.5 is for differentiablity using the chain rule\n",
    "    1. To use the average of the target as a constant value\n",
    "        <img src=\"../../images/grad_boost_1.png>\n",
    "    2. make the first tree which is just a leaf with the average value of the target\n",
    "        - There will be M trees 1/1 where M is a hyperparameter specified, for which the following will be repeated\n",
    "        - <img src=\"../../images/grad_boost_2.png\">\n",
    "        - Find derivative of the loss function (which is just the residual for all the rows of dataset)\n",
    "        - <img src=\"../../images/grad_boost_3.png>\n",
    "        - Now using this residual build a tree to predict new residuals\n",
    "        - Find the avg. values of last leaves of the tree if multiple values in it\n",
    "        - <img src=\"../../images/grad_boost_4.png>\n",
    "        - Add original average of the target with new residuals after multiplying it with the learnign rate and update the target\n",
    "        - <img src=\"../../images/grad_boost_5.png>\n",
    "    3. Output of  step 2 till residual is minimum or the # of trees are reached\n",
    "- Classification overview\n",
    "    - In this since the target is a class and not a numerical value, the log(odds) of the target is taken for each point\n",
    "        -  log(# of class 1/ #of class 2)\n",
    "    -  Logistic function of the log(odds) is also calculated\n",
    "        - <img src=\"../../images/grad_boost_6.png>\n",
    "    - This proability is then set against threshold of 0.5 to find new target constant values which will be 1 of prob is >=0.5 else 0\n",
    "    - New residuals will be calcuated = (observed/the initial constant probability - predicted)\n",
    "    - Build a tree to predict residuals\n",
    "    - In regression, if a leaf contains two values it can be averaged while in classification since it is a log odd's proability, it will be transformed using the following for each leaf\n",
    "    - <img src=\"../../images/grad_boost_7.png>\n",
    "    - once all the values for leaves are calculated, the constant prediction of single leaf will be updated using product of new * learning rate and adding it to the constant value\n",
    "    - This gives a new log(odds) which is used to get new probability using logistic function for each data point\n",
    "    - Now the residuals is calculated (observed - residuals)\n",
    "- Intution behind Gradient Descent Classification\n",
    "    - SKIPPING, too much to handle\n",
    "> Code   \n",
    "    ```python\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "    clf.score\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    reg = GradientBoostingRegressor(random_state=0)\n",
    "    reg.fit(X_train, y_train)\n",
    "    reg.predict(X_test[1:2])\n",
    "    reg.score(X_test, y_test)\n",
    "\n",
    "\n",
    "    # parameters : loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0\n",
    "\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- models can be computationally expensive and take a long time to train, especially on CPUs;\n",
    "- hard to interpret the final models.\n",
    "- overemphasize outliers and cause overfitting\n",
    "- large grid search during hyper tuning.\n",
    "- Less interpretative in nature\n",
    "\n",
    "+ Lots of flexibility - can optimize on different loss functions and provides several hyper parameter tuning options that make the function fit very flexible.\n",
    "+ train faster especially on larger datasets,\n",
    "+ most of them provide support handling categorical features, so no pre-processing required\n",
    "+ some of them handle missing values natively so imputation taken care of\n",
    "\n",
    "Source\n",
    "[youtube](https://youtu.be/3CC4N4z3GJc)\n",
    "[youtube](https://youtu.be/2xudPOBz-vs)\n",
    "[youtube](https://youtu.be/StWY5QWMXCw)\n",
    "### - Light Gradient boost\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Xtream Gradient\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source \n",
    "[youtube](https://www.youtube.com/watch?v=OtD8wVaFm6E)\n",
    "### - Catboost\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Isolation tree\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Isolation tree\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - SVM\n",
    "> Working\n",
    "- Overview\n",
    "    - Unlike in linear regression hwere the line is drawn considering all the points, here its only based on the points that are at the edges of the classes and closest to each other (support vectors)\n",
    "    - Margin : The distance between the support vectors from the hyper plane\n",
    "    - The aim is to have a hyperplane which has the largest margin from both the extreme points\n",
    "    - This uses maximum margin classifiers, which are suseptible to outliers\n",
    "    - Outliers must not affect the hyperplance so misclassification must be allowed to get the largest margins\n",
    "    - This is the bias variance tradeoff\n",
    "    - Using CV of these margins, the best margin is found and since its misclassifying, its called soft margins\n",
    "    - The points within the soft margin and on the edge are called Support Vector\n",
    "    - But when they fail like in case where class 1 is surrounded by class 2, support vector classifiers are useless\n",
    "    - Then comes support vector machines, which moves the data to higher dimensions\n",
    "        - This adds new dimensions to data which then can be used to find the hyperplane that seperates the seemingly unclassifyable data\n",
    "        - This adding of new dimension can be like squaring of data or root or cube or exp etc has to be considered based on the data\n",
    "        - This moving to higher dimension by transforming data is done using kernel functions\n",
    "        - Kernel : ‘linear’, ‘poly’, radial basis function, ‘sigmoid’, ‘precomputed’\n",
    "        - rbf : finds plane in infinite dimensions bt in 1 d acts as weighted nearest neighbour\n",
    "        - The kernel trick : not transforming the data but acting as if its transformed to find the hyperplane\n",
    "        - This makes computation easy\n",
    "    - Polynomial kernel\n",
    "        - (a x b + r)^d\n",
    "        - a and b are the data points its comparing, r is coefficient of polynomial and d is the polynomial degree passed as parameter\n",
    "        - by passing r and d, new coordinates of the points can be calculated, which are determined using cross validation\n",
    "    - RBF\n",
    "        - e^(-y(a-b)^2)\n",
    "        - y is to be found using CV\n",
    "        - can be expanded to infinity using taylor series\n",
    "> Code   \n",
    "    ```python\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "    clf.fit(X, y)\n",
    "    # parameters :  C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- not suitable for large data sets.\n",
    "- does not perform very well when the data set has more noise\n",
    "- underperforms when number of features for each data point exceeds the number of training data samples\n",
    "- here is no probabilistic explanation for the classification.\n",
    "- Choosing a “good” kernel function is not easy\n",
    "- time consuming\n",
    "- Difficult to understand and interpret the final model, variable weights and individual impact.\n",
    "- not that easy to fine-tune these hyper-parameters. It is hard to visualize their impact\n",
    "\n",
    "+ kernel trick is real strength of SVM for complex hyperplane\n",
    "+ SVM is not solved for local optima.\n",
    "+ the risk of over-fitting is less in SVM.\n",
    "+ works relatively well when there is a clear margin of separation \n",
    "+ more effective in high dimensional spaces.\n",
    "+ effective in cases where the number of dimensions is greater than the number of samples.\n",
    "+ relatively memory efficient\n",
    "\n",
    "Source : \n",
    "[youtube](https://www.youtube.com/watch?v=efR1C6CvhmE&pp=ygUNc3ZtIGV4cGxhaW5lZA%3D%3D)\n",
    "### - Validations\n",
    "> K fold\n",
    "- .\n",
    "> OOO\n",
    "- .\n",
    "> .\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Imbalanced \n",
    "> SMOTE\n",
    "- .\n",
    "> Imbalanced-learn\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Metrics\n",
    "> Confusion Matrix\n",
    "- <img src =\"../../images/conf_mat1.webp\">\n",
    "- <img src =\"../../images/conf_mat.webp\">\n",
    " - Type 1 (False Positive rate) = fp/(fp+tn)\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    false_negative_rate = fn / (tp + fn)\n",
    "    ```\n",
    " - Type 2 (False Negative Rate) = fn/(tp+fn)\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    false_positive_rate = fp / (fp + tn)\n",
    "    ```\n",
    "\n",
    "> Accuracy\n",
    "- (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "> Precision (Positive predict value)\n",
    "- how many of the correctly predicted cases actually turned out to be positive.\n",
    "- False Positive is a higher concern than False Negatives.\n",
    "- ex : music or video recommendation systems, e-commerce websites, etc. Wrong results could lead to customer churn and be harmful to the business.\n",
    "- ex : raising false alerts is costly, \n",
    "- <img src =\"../../images/pre_recall.png\">\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix, precision_score\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    precision = precision_score(y_true, y_pred_class) # or optionally tp/ (tp + fp)\n",
    "    ```\n",
    "\n",
    "> Recall\n",
    "- how many of the actual positive cases we were able to predict correctly with our model.\n",
    "- ex : really care about catching all fraudulent transactions even at a cost of false alerts\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix, recall_score\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    recall = recall_score(y_true, y_pred_class) # or optionally tp / (tp + fn)\n",
    "    ```\n",
    "> False Discovery Rate\n",
    "- measures how many predictions out of all positive predictions were incorrect. (1- precision)\n",
    "- When raising false alerts is costly and when you want all the positive predictions to be worth looking at you should optimize for precision.\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    false_discovery_rate = fp/ (tp + fp)\n",
    "    ```\n",
    "\n",
    "> F1, Threshold\n",
    "- harmonic mean of Precision and Recall, \n",
    "- Used when there is imbalance\n",
    "- Threshold can be found which will give the highest F1\n",
    "- It is maximum when Precision is equal to Recall. Not interpretable\n",
    "- <img src =\"../../images/f1_1.png\">\n",
    "- <img src =\"../../images/f1_2.png\">\n",
    "    - F1 beta\n",
    "        - Higher the beta higher regards to recall\n",
    "        - if beta =1 then its normal f1 score, beta=2 is f2 score and so on\n",
    "        - <img src =\"../../images/f1_3.png\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import fbeta_score\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    f1= f1_score(y_true, y_pred_class)\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    f2 = fbeta_score(y_true, y_pred_class, beta = 2)\n",
    "    ```\n",
    "\n",
    "> Specificity (True negative rate)\n",
    "- the number of correct negative predictions divided by the total number of negatives\n",
    "- where False Negative trumps False Positive.\n",
    "- ex :  important in medical cases where it doesn’t matter whether we raise a false alarm, but the actual positive cases should not go undetected!\n",
    "- ex : really want to be sure that you are right when you say something is safe\n",
    "- <img src =\"../../images/specificity.jpg\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_class).ravel()\n",
    "    true_negative_rate = tn / (tn + fp)\n",
    "    ```\n",
    "\n",
    "> Sensitivity also recall also True Positive rate\n",
    "- the number of correct positive predictions divided by the total number of positives\n",
    "- ex : really care about catching all fraudulent transactions even at a cost of false alerts\n",
    "- <img src =\"../../images/sensitivity.jpg\">\n",
    "\n",
    "    ```python\n",
    "    # in recall below\n",
    "    ``` \n",
    "\n",
    "> AUC-ROC\n",
    "- visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart.\n",
    "- ROC score tells us how good our curve is, we can calculate the Area Under the ROC Curve, or ROC AUC score.\n",
    "    ```python\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from scikitplot.metrics import plot_roc\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_roc(y_true, y_pred, ax=ax)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_pos)\n",
    "    ``` \n",
    "\n",
    "> Cohen Kappa\n",
    "- Cohen Kappa tells you how much better is your model over the random classifier that predicts based on class frequencies.\n",
    "- mostly used when imbalance exists\n",
    "- <img src =\"../../images/cohen_kappa.webp\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "    cohen_kappa = cohen_kappa_score(y_true, y_pred_class)\n",
    "    ```\n",
    "\n",
    "> Matthews Correlation Coefficient \n",
    "- Imbalanced dataset and easy to interpret\n",
    "- <img src =\"../../images/mcc_eq.webp\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "    y_pred_class = y_pred_pos > threshold\n",
    "    matthews_corr = matthews_corrcoef(y_true, y_pred_class)\n",
    "    ```\n",
    "\n",
    "> Precision-Recall Curve\n",
    "- For every threshold, you calculate PPV and TPR and plot it.\n",
    "    ```python\n",
    "    \n",
    "    from scikitplot.metrics import plot_precision_recall\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_precision_recall(y_true, y_pred, ax=ax)\n",
    "\n",
    "    from sklearn.metrics import average_precision_score\n",
    "\n",
    "    avg_precision = average_precision_score(y_true, y_pred_pos)\n",
    "    ``` \n",
    "> Log loss\n",
    "- objective function that is optimized under the hood and can be used as metric\n",
    "- <img src =\"../../images/log_loss.webp\">\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import log_loss\n",
    "\n",
    "    loss = log_loss(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "> Kolmogorov-Smirnov plot\n",
    "- assess the separation between prediction distributions for positive and negative classes\n",
    "- problem is about sorting/prioritizing the most relevant observations and you care equally about positive and negative classes.\n",
    "- Good addition to roc score\n",
    "    ```python\n",
    "    from scikitplot.metrics import plot_ks_statistic\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_ks_statistic(y_true, y_pred, ax=ax)\n",
    "    ```\n",
    "\n",
    "Source\n",
    "[blog](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/#Precision_vs._Recall)\n",
    "[blog](https://neptune.ai/blog/evaluation-metrics-binary-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source\n",
    "[youtube]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "> Working\n",
    "- Aim is to convert words in to numbers to be fed into NN\n",
    "- Random number assignment can be done but that doesn signify the relation between two words like synonyms or opposites\n",
    "- Bag of words : \n",
    "- Skip gram : \n",
    "> Types\n",
    "- \n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source\n",
    "[youtube](https://youtu.be/viZrOnJclY0)\n",
    "[youtube]()\n",
    "[blog]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
