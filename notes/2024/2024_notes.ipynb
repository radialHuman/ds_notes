{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md' target='_blank'>../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md</a><br>"
      ],
      "text/plain": [
       "/home/radial/Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [youtube](https://youtu.be/ZTt9gsGcdDo)\n",
    "# Essential Matrix Algebra for Neural Networks, Clearly Explained\n",
    "FileLink('../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Regression?\n",
    "To predict value of something (target) based on values of multiple factors (features).\n",
    "Since it is supervised, it must have training data with values of both target and features to get a sum of products (equation) which would have be the best among all possible equations describing the relations between the two by reducing the amount of distance between the actual value and the value predicted by the equation's line.\n",
    "Depending on the nature of the factors it will be linear (straight line/plane/hyperplane) or non linear (curved).\n",
    "\n",
    "Linear : target = A*feature1 + B*feature2 + C*Feature3 + x\n",
    "Non Linear : target = A*feature1^4 + B*feature2^2 + C*Feature3 + x (order is 4, as thats the highest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Assumptions\n",
    "    1. Linearity: The relationship between the dependent and independent variables is linear and additive. Must have linear betas.\n",
    "        > A linear relationship suggests that a change in response Y due to one unit change in X¹ is constant, regardless of the value of X¹.\n",
    "        > An additive relationship suggests that the effect of X¹ on Y is independent of other variables.\n",
    "        > If this is ignored, then the line will be off the actual value by a lot and the model will not accurately represent the data\n",
    "\n",
    "        > How to check if this is met before modeling: Plot scatter plot of each feature against the target and see the pattern. If its like a stright line then fine\n",
    "\n",
    "        > How to check after modeling : Plot residual vs predicted values and if there is any pattern and lack of randomness across the middle like there a non linear relationship was involved\n",
    "\n",
    "        > How to fix : If its not then polynomial regression might be needed on the variable in the equation by applying transformations like log or exp or root or power etc depending on the relation between that variable and the target\n",
    "\n",
    "\n",
    "    2. Independence (No Autocorrelation in residuals/errors): The observations are independent of each other.\n",
    "        > This means that a residual of an observation should not predict the next observation.\n",
    "        > Usually an issue in time series but if features are not choosen wisely, if can occur in tabular too\n",
    "        > Might be caused by missing variable\n",
    "\n",
    "        > How to check before modeling : We can conduct durbinWatsonTest on our model for an output with a p-value, which will determine whether the assumption is met or not. \n",
    "        The null hypothesis states that the errors are not auto-correlated with themselves (they are independent). Thus, if we achieve a p-value > 0.05, we would fail to reject the null hypothesis.\n",
    "        It must lie between 0 and 4. If DW = 2, implies no autocorrelation, 0 < DW < 2 implies positive autocorrelation while 2 < DW < 4 indicates negative autocorrelation.\n",
    "\n",
    "        > How to check after modeling : Draw the line over the points and look at the residual plot for patterns. If exists then there is no independence. Must be random.\n",
    "        Residual vs time plot and look for the seasonal or correlated pattern in residual values.\n",
    "\n",
    "        > How to fix :\n",
    "        Do more domain searching to add missing variables\n",
    "        Depending on the equation, or the relation between x and y, add polynomial or exp or log\n",
    "        <img src=\"../../images/autocorr_correction.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "\n",
    "    3. Homoscedasticity: The variance of the errors is constant across all levels of the independent variables.\n",
    "        > The variance of the residuals should not vary as the target is increased in residual plot\n",
    "        > Generally, non-constant variance arises in presence of outliers or extreme large values. \n",
    "        > If a feature has a wide range of numbers then this can happen as the feature might not be a good predictor\n",
    "\n",
    "        > How to check before modeling: Plot a variable against the target, find best fit and look at residual plot. If there is a funnel/dual cone/reverse funnel pattern then it might be heteroscedastic. \n",
    "\n",
    "        > How to check after modeling : residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern. Wreusch-Pagan / Cook – Weisberg test or White general test or Goldfledt-Quant test to detect this phenomenon\n",
    "\n",
    "        > How to fix it : There might be two/multiple regression liens that can explain if divided based on pattern. Log variables might help.\n",
    "        <img src=\"../../images/hetero_example.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "    4. Normality: The errors follow a normal distribution.\n",
    "        > The spread of the errors at a given point, should be normally distributed\n",
    "        > prediction errors/residuals need to be normally distributed\n",
    "        > Sometimes the error distribution is \"skewed\" by the presence of a few large outliers.\n",
    "        > Calculation of confidence intervals and various significance tests for coefficients are all based on the assumptions of normally distributed errors.\n",
    "        > If the error distribution is significantly non-normal, confidence intervals may be too wide or too narrow. \n",
    "        > The dependent and independent variables in a regression model do not need to be normally distributed by themselves   \n",
    "        > the normal error assumption is usually justified by appeal to the central limit theorem, which holds in the case where many random variations are added together.\n",
    "\n",
    "        > How to check before modeling: plot a histogram on your residuals or employ a Q-Q plot, which also helps us visually determine if our residuals follow a normal distribution. if there is a skew in your residuals. A log transformation on your dependent variable may help.\n",
    "\n",
    "        > How to check after modeling : The Kolmogorov-Smirnov test, the Shapiro-Wilk test, the Jarque-Bera test, and Anderson-Darling test.\n",
    "\n",
    "        > How to fix it : check the extreme values of the variable\n",
    "\n",
    "    5. No multicollinearity: The independent variables are not highly correlated with each other.\n",
    "        > independent variables show moderate to high correlation\n",
    "        > tough task to figure out the true relationship of a predictors with response variable\n",
    "        > difficult to find out which variable is actually contributing to predict the response variable.\n",
    "        > the standard errors tend to increase and the confidence interval becomes wider leading to less precise estimates of slope parameters\n",
    "        > two variables vary very similarly and contain the same kind of information.\n",
    "        > only the complexity of the model increase, and no new information or pattern is learned by the model.\n",
    "        > \n",
    "\n",
    "        > How to check before modeling:  use scatter plot/heat map to visualize correlation effect among variables\n",
    "\n",
    "        > How to check after modeling : VIF factor. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity.\n",
    "        VIF = 1/(1-R2) for each variable\n",
    "\n",
    "        > How to fix it : drop and check vif\n",
    "\n",
    "    6. No endogeneity: There is no relationship between the errors and the independent variables.\n",
    "        > Causes of endogeneity include omitted variables, simultaneity, and sample selection in the estimated equation. \n",
    "        > OLS estimator will be inconsistent and biased,\n",
    "\n",
    "        > How to check before modeling:  \n",
    "\n",
    "        > How to check after modeling : Durbin-Wu-Hausman test, which compares the OLS estimator with an alternative estimator that is robust to endogeneity, such as the instrumental variables (IV) estimator.\n",
    "\n",
    "        > How to fix it : reduce the sources of endogeneity by including more relevant variables, improving the data quality, or using a different research design that avoids the causal ambiguity.\n",
    "        instrumental variables (IV) estimator, which uses a variable that is correlated with the endogenous explanatory variable but not with the error term\n",
    "\n",
    "Source : \n",
    "[youtube](https://www.youtube.com/playlist?list=PLTNMv857s9WUI1Nz4SssXDKAELESXz-bi)\n",
    "[blog 0](https://medium.com/@andrewhnberry/checking-your-linear-regression-assumptions-and-how-to-check-them-338f770acb57)\n",
    "[blog 1](https://godatadrive.com/blog/basic-guide-to-test-assumptions-of-linear-regression-in-r)\n",
    "[blog 2](https://people.duke.edu/~rnau/testing.htm)\n",
    "[blog 3](https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/)\n",
    "[blog 4](https://www.geeksforgeeks.org/assumptions-of-linear-regression/)\n",
    "[blog 5](https://www.jmp.com/en_in/statistics-knowledge-portal/what-is-regression/simple-linear-regression-assumptions.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Working\n",
    " * The aim is to find BLUE (best linear unbiased estimate) or the best fit line which explains all the variance across the data\n",
    " * This is usually an equation involving target on one side and linear additive combination of features on the other which are not in polynomial or expoenential or logarithmic form\n",
    " * y = coefficient + ax1 + bx2 + cx3 + dx4 | ... | + error\n",
    " * where a,b,c,d ... are slope values and coefficient is the intercept in multi dimensional plane\n",
    " * here coeeficient is the value when features are 0\n",
    " * and slope is the unit change in target wrt. unit change in feature\n",
    "\n",
    " * Cost function : to reduce overall distance between each point in dataset and predcited value by the line\n",
    " * in the below equation is one way to find the distance b/w predicted and actual, this can be replaced with absolute also. where m is total number of values\n",
    "\n",
    "  <img src=\"../../images/linear_cost.png\">\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/1-OGRohmH2s)\n",
    "\n",
    "* Gradient Descent\n",
    "\n",
    "  - Infinite number of lines can be drawn and then the cost function can be calculated for all but thats computationally expensive\n",
    "  - So by keeping coefficent constant, and varying the slope values, for all the variables values, cost function is calculated.\n",
    "  - Then a grpah is drawn between all possible m values and cost function's output to find the lowest point (gloabl minima) on the curve which will give the best m for line \n",
    "  - This finding of global minima (slope = 0) is using gradient descent which is based on convergence theorem which moves towards minima using derivative values of slope and a learning rate\n",
    "  - To know which way is global minima, all the sides needs to be considered before moving in any direction\n",
    "\n",
    "* LR using Ordinary Least square (OLR)\n",
    "- TODO  https://youtu.be/KZ1mWboXE6g\n",
    " \n",
    "* Multiple LR using Matrix \n",
    "  - In this image, the a is all the beta values including coefficient\n",
    "  - <img src=\"../../images/multi_reg_matrix.png\"> \n",
    "    \n",
    "  - Using direct formula : (Xt.X)^-1.Xt.Y\n",
    "  - Using single valur decomposition\n",
    "  - Using QR decomposition\n",
    "\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/_OyKjstWe80)\n",
    "[blog](https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0, b1, b2 = [-2.40119363  1.62732095 -0.29177719]\n",
      "Predictions = [1. 2. 3.]\n",
      "Error = [-0. -0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# MLR using Matrix\n",
    "import numpy as np\n",
    "x = np.array([[100, 212, 356], [4.25, 10.99, 16.2],  [-234, -465, -678]], np.int32)\n",
    "y = np.array([1,2,3])   \n",
    "# formula : (Xt.X)^-1.Xt.Y\n",
    "betas = np.dot(np.dot(np.linalg.inv(np.dot((x.T), x)),(x.T)),y)\n",
    "np.set_printoptions(suppress=False)\n",
    "print(\"b0, b1, b2 = {}\".format(betas))\n",
    "preds = np.dot(x,betas)\n",
    "print(\"Predictions = {}\".format(preds))\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"Error = {}\".format(preds-y))\n",
    "\n",
    "#source : https://cmdlinetips.com/2020/03/linear-regression-using-matrix-multiplication-in-python-using-numpy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Code\n",
    "\n",
    "source : [blog](https://www.freecodecamp.org/news/data-science-with-python-8-ways-to-do-linear-regression-and-measure-their-speed-b5577d75f8b/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 1.0\n",
      "Betas : [ 0.00863046  0.01082108 -0.01180701]\n",
      "Beta0 : 2.046179467416178\n",
      "Predictions : [2.11436926]\n"
     ]
    }
   ],
   "source": [
    "# MLR using scikit learn\n",
    "# !pip install scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(x, y)\n",
    "print(\"Score : {}\".format(reg.score(x, y)))\n",
    "print(\"Betas : {}\\nBeta0 : {}\".format(reg.coef_, reg.intercept_))\n",
    "print(\"Predictions : {}\".format(reg.predict(np.array([[3, 5, 1]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betas : [-2.40119363  1.62732095 -0.29177719]\n",
      "Model : \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                    nan\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Tue, 02 Jan 2024   Prob (F-statistic):                nan\n",
      "Time:                        17:16:02   Log-Likelihood:                 81.846\n",
      "No. Observations:                   3   AIC:                            -157.7\n",
      "Df Residuals:                       0   BIC:                            -160.4\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -2.4012        inf         -0        nan         nan         nan\n",
      "x2             1.6273        inf          0        nan         nan         nan\n",
      "x3            -0.2918        inf         -0        nan         nan         nan\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   1.012\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.488\n",
      "Skew:                           0.643   Prob(JB):                        0.784\n",
      "Kurtosis:                       1.500   Cond. No.                     1.42e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.42e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 3 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1795: RuntimeWarning: divide by zero encountered in divide\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1795: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1717: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n"
     ]
    }
   ],
   "source": [
    "# MLR using statsmodel\n",
    "# !pip install statsmodels\n",
    "import statsmodels.api as sm\n",
    "model = sm.OLS(y,x)\n",
    "results = model.fit()\n",
    "print(\"Betas : {}\".format(results.params))\n",
    "print(\"Model : \\n{}\".format(results.summary()))\n",
    "# source  : [blog](https://datatofish.com/statsmodels-linear-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Output analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Metrics\n",
    "> MSE : Mean Squared error\n",
    "- Considers square of distance between actual and predicted value\n",
    "- Since its a quadratic in nature, the curve is differentiable at all points to find minima\n",
    "- one local and global minima as its a paraobla\n",
    "- Can tackle outliers as the distance is squared.\n",
    "- Since its a square the units are not the same for easy explanation\n",
    "\n",
    "> RSME\n",
    "- This is root of mse\n",
    "- suffers from same outlier impact\n",
    "- but has the same unit\n",
    "- also is differentiable at all points\n",
    "\n",
    "> RSLME\n",
    "- log of RSME to slows down the scale of error\n",
    "\n",
    "> MAE : Mean Absolute error\n",
    "- absolute difference between predicted and actual value\n",
    "- so the nature of the functionis like a V and is not differentiable in the origin\n",
    "- but is robust as outliers dont affect it as much as mse\n",
    "- Also since its |y-yhat| the unit is same for explainability\n",
    "- but its optimization takes time and is complicated as sub gradients needs to be calculated\n",
    "\n",
    "> MAPE\n",
    "- percentage of MAE, i.e. (MAE*100)/N\n",
    "\n",
    "> R2\n",
    "- 1-((square of sum of actual minus predicted)/(square of sum of actual minus mean))\n",
    "- 0.8-0.90 is good depending on the case\n",
    "- 1 might indicate overfitting\n",
    "- always increases if more features are added and might over fit so must be penalized\n",
    "    \n",
    "> Adjuested R2\n",
    "- To prevent increasing of R2 by adding useless features\n",
    "- if the feautre is not good, it will reduce the adjusted R2\n",
    "- 1-((1-R2)(N-1))/(N-P-1)\n",
    "- N is # of points\n",
    "- P is # of variables\n",
    "\n",
    "Source \n",
    "[youtube](https://youtu.be/BGlEv2CTfeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Regularization\n",
    "- When model is overfitting, i.e. good on training and bad on val and test, it might be learning the noise and not understanding the pattern\n",
    "- Overfitting models have high variance\n",
    "- When it doesn perform well even in the training then its underfitting, can be solved by making the model more complex or adding new features\n",
    "- Simple model which underfits is highly biased\n",
    "- Regularization helps in bias variance trade off by penalizing models for overfitting\n",
    "<img src = \"../../images/under_over_fit.png\">\n",
    "<img src  = \"../../images/regularization.webp\">\n",
    "\n",
    "\n",
    "> Lasso - Least absolute shrinkage and selection operator regression (L1)\n",
    "- The cost function used in regression where distance is squared, a product of lambda and absolute slope is added to penalize.\n",
    "- If there are features in a model which doesn contribute much, LASSO makes its weight to 0 there by acting as feature selector\n",
    "- <img src = \"../../images/lasso.png\">\n",
    "- m is # of features, n is # of samples, w is the slope of the feature, yi is actual and yhat is predicted\n",
    "- lambda is the hyperparameter, selected using cross valdidation (0-infinity)\n",
    "- When the value of lambda is very high, it will make the weights of slopes of features which dont contribute much go to 0\n",
    "- since the absolute graph is like a diamond which has 0\n",
    "\n",
    "    ```python\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    lasso=Lasso()\n",
    "    parameters={ 'alpha' :[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,28, 38,35,55,100]}\n",
    "    lasso_regressor=GridSearchCV( lasso, parameters, scoring='neg_mean_error' ,cv-5)\n",
    "    lasso_regressor. fit(X,y)\n",
    "    print(lasso_regressor.best_params_)\n",
    "    print(lasso_regressor.best_score_)\n",
    "    ```\n",
    "\n",
    "> Ridge - (L2)\n",
    "- The cost function is modified by adding a squared value of the slope\n",
    "- <img src = \"../../images/ridge.png\">\n",
    "- This reduces overfitting\n",
    "- lambda is the hyperparameter (0-infinity)\n",
    "- high lambda will reduce the weight but not make it 0\n",
    "- since square graph is like a circle and doesn go to 0\n",
    "\n",
    "    ```python\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.model_ selection import GridSearchcv\n",
    "    ridge=Ridge() \n",
    "    paraneters={ \"alpha\" :[1e-15,le-10,1e-8,1e-3,1e-2,1,5,16,20, 38, 35,46,45,58,55,168]}\n",
    "    ridge_regressor=Gridsearchcv( ridge, parameters, scoring='neg_mean_squared_error',cv=5)\n",
    "    ridge_regressor.fit(x,y)\n",
    "    ```\n",
    "\n",
    "> Elastic net (L1 and L2)\n",
    "- A combination of both Lasso and Ridge\n",
    "- Cost function has both absolute and squared values of slope added\n",
    "- <img src = \"../../images/elastic.png\">\n",
    "- this has two hyper parameters alpha and lambda\n",
    "\n",
    "    Source  :    \n",
    "    [youtube](https://youtu.be/2bx9Os2gPu4)\n",
    "    [youtube](https://www.youtube.com/watch?v=9lRv01HDU0s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "### - Basics\n",
    "> Parametric vs non parametric\n",
    "-\n",
    "> Grid vs random search CV\n",
    "-\n",
    "> class_weight\n",
    "- parameter to tell the model if the dataset target is balanced or not \n",
    "> Bootstrapping\n",
    "- Process of randomly selecting rows of data with replacement to generate mode datasets\n",
    "- Used usually in ensemble models like Random Forests \n",
    "> Bagging\n",
    "- Combination of bootstrapping and aggregation (majority wins) this is a paralell process as all the different sets are independent of each other\n",
    "> Boosting\n",
    "- This is a sequential process to work on weak learners by boosting the values of misclassified data points\n",
    "- \n",
    "> Stacking\n",
    "- \n",
    "> Blending\n",
    "- \n",
    "> Optimizer\n",
    "- \n",
    "### - Logistic\n",
    "> Working\n",
    "- For binary classification, things can be done using linear regression and a threshold like 0.5 can be set\n",
    "- but this might be susceptible to outliers\n",
    "- The aim of Log Reg is to find a best fit line which will linearly seperate the groups\n",
    "- equation is same as regression y = mx+c\n",
    "- The idea is to convert multidimensional points into just 0 or 1\n",
    "- This is done using sigmoid function\n",
    "- <img src = \"../../images/sigmoid.png\">\n",
    "- Instead of equation from Lin reg, if there is a sigmoid of that equation, the predicted value will always be b/w 0,1\n",
    "- This also gives the proabability given the value x, what are the chances the point belongs in either classes\n",
    "- Where the predicted value will be perpendicular distance of the point from the line\n",
    "- The unknown variable is the wieghts that needs to be assigned to each feature\n",
    "- <img src =\"../../images/log_weights.png\">\n",
    "- Here w is the weights, which laong with x which is the input, goes through summation and sigmoid non linear function to get predictions.\n",
    "- Which is then used to find the error or the loss function which then is partially derivated using gradient descent to find the global minima\n",
    "- <img src = \"../../images/log_chain.png\">\n",
    "- The cost function is\n",
    "- <img src = \"../../images/log_cost.png\">\n",
    "- This needs to be partially differentiated \n",
    "- <img src = \"../../images/log_diff.png\">\n",
    "- Difference between lir and log reg is that\n",
    "    - the yhat is sigmoid in log\n",
    "    - the cost function is not convex so there are many local minima and the initial parameters of weights determine where the local minima will be found after grad desct (no guarantee of global minima)\n",
    "- the Logit/log odds function\n",
    "- <img src = \"../../images/logit.png\">\n",
    "- Regularization also can be done\n",
    "- <img src =\"../../images/log_regularization.png\">\n",
    "\n",
    "> Code\n",
    "```python\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "    clf.predict(X[:2, :])\n",
    "    clf.predict_proba(X[:2, :])\n",
    "    clf.score(X, y)\n",
    "    # parameters : penalty, C, solver, \n",
    "```\n",
    "\n",
    "> Pros and cons\n",
    "+ Simple to build and explain\n",
    "+ less overfitting in lower dimensions\n",
    "- creates linear boundaries, we won't obtain better results when dealing with complex or non-linear data\n",
    "- cant be used if the number of observations is fewer than the number of features; otherwise, it may result in overfitting.\n",
    "- affected by outliers\n",
    "- has to qualify the assumptions like in linear regression\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/z9XAXXGwUzM)\n",
    "[youtube](https://youtu.be/xXvgkILaFT4?list=PLyqSpQzTE6M-SISTunGRBRiZk7opYBf_K)\n",
    "[blog](https://www.analyticsvidhya.com/blog/2021/10/building-an-end-to-end-logistic-regression-model/)\n",
    "\n",
    "### - Decision Tree\n",
    "> Assumptions\n",
    "- If the values are continuous then they are discretized prior to building the model\n",
    "- \n",
    "> Working\n",
    "- Works for bth regression and classification\n",
    "- Its multiple linear lines, not exactly non linear but better than just one linear line\n",
    "- hierarchical tree structure consisting of a root node, branches, internal nodes, and leaf nodes\n",
    "- algorithmic model utilizes conditional control statements and is non-parametric\n",
    "- The tree is built by splitting on each feature and the hierarchy of feature is decided based on the purity of the feature in predicting the target\n",
    "- The purity is based on\n",
    "    - Entropy and information gain\n",
    "        - <img src=\"../../images/decision_entropy.png\">\n",
    "        - <img src=\"../../images/decision_entropy_ex.png\">\n",
    "        - entropy before the split and after the split is compared\n",
    "        - <img src=\"../../images/decision_entropy_ex2.png\">\n",
    "        - Entropy is found for all features and which ever has the mx change is taken\n",
    "        - I.e. higher the Entropy, the lower will be the purity\n",
    "        - Which ever feature gives most information gain is used as node as thats the most purest\n",
    "        - Steps\n",
    "            - First entropy of the whole target is calulated\n",
    "            - Entropy of each feature is calculated for each class in it\n",
    "            - Information gain of each feature is found, which is total entropy minus (individual class entropy of the feature into its proability)\n",
    "            - Which feature has the highest IG becomes the root node with brances as its classes\n",
    "            - Now for each of the branches, its takes subset by filtering onlny for those value and then IG is calculated again to find the next split\n",
    "            - This continues, till there are leaf nodes that are pure split\n",
    "    - Gini index\n",
    "        - Unlike IG, minimum GI is taken as root node as its 1-(probability)\n",
    "        - <img src=\"../../images/decision_gini.png\">\n",
    "        - Steps\n",
    "            - Almost the same just that lowest GI will be used as root and for splitting\n",
    "    - <img src=\"../../images/decision_gi_IG.png\">\n",
    "    - [youtube](https://youtu.be/5aIFgrrTqOw)\n",
    "    - Entropy ranges from 0-1\n",
    "    - GI ranges from 0-0.5\n",
    "    - GI doesn have log and hence is faster\n",
    "- The nodes have threshold in case of numerical values\n",
    "    - [youtube](https://youtu.be/5O8HvA9pMew?list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe)\n",
    "    - Sorts the tables in ascending manner\n",
    "    - Each number is taken and IG or GI is calculated\n",
    "    - Time consuming for large dataset\n",
    "- For regression tree \n",
    "    - TODO\n",
    "- Pruning : When we remove sub-nodes of a decision node, cutting down of branches\n",
    "- Early stopping :  not allowing the layes to go beyond a point\n",
    "> Code\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(criterion ='entropy').fit(X_train,y_train)\n",
    "prediction = tree.predict(X_test)\n",
    "tree.score(test_features,test_targets)\n",
    "# parameters : max_depth, min_samples_split, criterion, min_samples_leaf, max_features, \n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "cross_val_score(regressor, X, y, cv=10)\n",
    "```\n",
    "> Pros and cons\n",
    "+ Highly explainable\n",
    "+ resistant to outliers\n",
    "- Generally overfits, hence high variance\n",
    "- parametric, hence new data point if added, needs retraining\n",
    "- not suitable for large datasets as its just one tree\n",
    "- not for imbalanced dataset\n",
    "\n",
    "Source \n",
    "[youtube](https://youtu.be/uPwZ8DhQNjg?list=PLyqSpQzTE6M-SISTunGRBRiZk7opYBf_K)\n",
    "[youtube](https://youtu.be/CWzpomtLqqs)\n",
    "[blog]()\n",
    "### - Random Forest\n",
    "> Working\n",
    "- DT are highly sensitive to training data and can overfit without generalizing\n",
    "- First multiple datsets are generated using bootstraping\n",
    "- Then for these randomly selected rows, randomly selected features are selected \n",
    "    - The usual number for feature # is log or sqr root of total # of features\n",
    "- These two add diversity to the datasets and reduces chances of overfitting\n",
    "- Then for each dataset DTs are created\n",
    "- For predicting, the data point is passed through all the trees and the output is generated\n",
    "- Majority wins (this is aggregation)\n",
    "- bootstrapping + aggregation = bagging\n",
    "- In case of regression, a mean or median can be considered as the output\n",
    "\n",
    "> Code\n",
    "    ```python\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf.fit(X, y)\n",
    "    clf.predict([[0, 0, 0, 0]])\n",
    "    clf.feature_importances_\n",
    "\n",
    "    # parameters : (n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "    regr.fit(X, y)\n",
    "    regr.predict([[0, 0, 0, 0]])\n",
    "\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- Complex computation, slower\n",
    "- interpretation is difficult\n",
    "\n",
    "+ Scaling is not really required\n",
    "+ Doesn care about unwanted features\n",
    "+ Generalized\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/v6VJ2RO66Ag)\n",
    "\n",
    "### - Ada Boost\n",
    "> Working\n",
    "- \n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "\n",
    "### - Gradient Boost\n",
    "> Working\n",
    "- \n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Light Gradient boost\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Xtream Gradient\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Catboost\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Isolation tree\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Isolation tree\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - SVM\n",
    "> Working\n",
    "- .\n",
    "> Code   \n",
    "    ```python\n",
    "    ```\n",
    "\n",
    "> Pros and cons\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Validations\n",
    "> K fold\n",
    "- .\n",
    "> OOO\n",
    "- .\n",
    "> .\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Imbalanced \n",
    "> SMOTE\n",
    "- .\n",
    "> Imbalanced-learn\n",
    "- .\n",
    "\n",
    "Source : \n",
    "### - Metrics\n",
    "> Confusion Matrix\n",
    "- <img src =\"../../images/conf_mat1.webp\">\n",
    "- <img src =\"../../images/conf_mat.webp\">\n",
    " - Type 1 (False Positive rate) = fp/(fp+tn)\n",
    " - Type 2 (False Negative Rate) = fn/(tp+fn)\n",
    "\n",
    "> Accuracy\n",
    "- (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "> Precision (Positive predict value)\n",
    "- how many of the correctly predicted cases actually turned out to be positive.\n",
    "- False Positive is a higher concern than False Negatives.\n",
    "- ex : music or video recommendation systems, e-commerce websites, etc. Wrong results could lead to customer churn and be harmful to the business.\n",
    "- ex : raising false alerts is costly, \n",
    "- <img src =\"../../images/pre_recall.png\">\n",
    "\n",
    "> Recall\n",
    "- how many of the actual positive cases we were able to predict correctly with our model.\n",
    "- ex : really care about catching all fraudulent transactions even at a cost of false alerts\n",
    "\n",
    "> F1, Threshold\n",
    "- harmonic mean of Precision and Recall, \n",
    "- Used when there is imbalance\n",
    "- Threshold can be found which will give the highest F1\n",
    "- It is maximum when Precision is equal to Recall. Not interpretable\n",
    "- <img src =\"../../images/f1_1.png\">\n",
    "- <img src =\"../../images/f1_2.png\">\n",
    "    - F1 beta\n",
    "        - Higher the beta higher regards to recall\n",
    "        - if beta =1 then its normal f1 score, beta=2 is f2 score and so on\n",
    "        - <img src =\"../../images/f1_3.png\">\n",
    "\n",
    "> Specificity (True negative rate)\n",
    "- the number of correct negative predictions divided by the total number of negatives\n",
    "- where False Negative trumps False Positive.\n",
    "- ex :  important in medical cases where it doesn’t matter whether we raise a false alarm, but the actual positive cases should not go undetected!\n",
    "- ex : really want to be sure that you are right when you say something is safe\n",
    "- <img src =\"../../images/specificity.jpg\">\n",
    "\n",
    "> Sensitivity also recall also True Positive rate\n",
    "- the number of correct positive predictions divided by the total number of positives\n",
    "- ex : \n",
    "- <img src =\"../../images/sensitivity.jpg\">\n",
    "    ```python\n",
    "\n",
    "    ``` \n",
    "\n",
    "> AUC-ROC\n",
    "- visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart.\n",
    "- ROC score tells us how good our curve is, we can calculate the Area Under the ROC Curve, or ROC AUC score.\n",
    "    ```python\n",
    "\n",
    "    ``` \n",
    "\n",
    "> Cohen Kappa\n",
    "- Cohen Kappa tells you how much better is your model over the random classifier that predicts based on class frequencies.\n",
    "- mostly used when imbalance exists\n",
    "- <img src =\"../../images/cohen_kappa.webp\">\n",
    "\n",
    "> Matthews Correlation Coefficient \n",
    "- Imbalanced dataset and easy to interpret\n",
    "- <img src =\"../../images/mcc_eq.webp\">\n",
    "\n",
    "> Precision-Recall Curve\n",
    "- For every threshold, you calculate PPV and TPR and plot it.\n",
    "    ```python\n",
    "\n",
    "    ``` \n",
    "> Log loss\n",
    "- TODO\n",
    "\n",
    "> Kolmogorov-Smirnov plot\n",
    "- \n",
    "\n",
    "Source\n",
    "[blog](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/#Precision_vs._Recall)\n",
    "[blog](https://neptune.ai/blog/evaluation-metrics-binary-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
