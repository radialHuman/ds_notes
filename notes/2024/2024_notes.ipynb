{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md' target='_blank'>../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md</a><br>"
      ],
      "text/plain": [
       "/home/radial/Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [youtube](https://youtu.be/ZTt9gsGcdDo)\n",
    "# Essential Matrix Algebra for Neural Networks, Clearly Explained\n",
    "FileLink('../../../../../../../Documents/Retrain DS/DS/Youtube/Essential Matrix Algebra for Neural Networks, Clearly Explained.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Regression?\n",
    "To predict value of something (target) based on values of multiple factors (features).\n",
    "Since it is supervised, it must have training data with values of both target and features to get a sum of products (equation) which would have be the best among all possible equations describing the relations between the two by reducing the amount of distance between the actual value and the value predicted by the equation's line.\n",
    "Depending on the nature of the factors it will be linear (straight line/plane/hyperplane) or non linear (curved).\n",
    "\n",
    "Linear : target = A*feature1 + B*feature2 + C*Feature3 + x\n",
    "Non Linear : target = A*feature1^4 + B*feature2^2 + C*Feature3 + x (order is 4, as thats the highest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assumptions\n",
    "    1. Linearity: The relationship between the dependent and independent variables is linear and additive. Must have linear betas.\n",
    "        > A linear relationship suggests that a change in response Y due to one unit change in X¹ is constant, regardless of the value of X¹.\n",
    "        > An additive relationship suggests that the effect of X¹ on Y is independent of other variables.\n",
    "        > If this is ignored, then the line will be off the actual value by a lot and the model will not accurately represent the data\n",
    "\n",
    "        > How to check if this is met before modeling: Plot scatter plot of each feature against the target and see the pattern. If its like a stright line then fine\n",
    "\n",
    "        > How to check after modeling : Plot residual vs predicted values and if there is any pattern and lack of randomness across the middle like there a non linear relationship was involved\n",
    "\n",
    "        > How to fix : If its not then polynomial regression might be needed on the variable in the equation by applying transformations like log or exp or root or power etc depending on the relation between that variable and the target\n",
    "\n",
    "\n",
    "    2. Independence (No Autocorrelation in residuals/errors): The observations are independent of each other.\n",
    "        > This means that a residual of an observation should not predict the next observation.\n",
    "        > Usually an issue in time series but if features are not choosen wisely, if can occur in tabular too\n",
    "        > Might be caused by missing variable\n",
    "\n",
    "        > How to check before modeling : We can conduct durbinWatsonTest on our model for an output with a p-value, which will determine whether the assumption is met or not. \n",
    "        The null hypothesis states that the errors are not auto-correlated with themselves (they are independent). Thus, if we achieve a p-value > 0.05, we would fail to reject the null hypothesis.\n",
    "        It must lie between 0 and 4. If DW = 2, implies no autocorrelation, 0 < DW < 2 implies positive autocorrelation while 2 < DW < 4 indicates negative autocorrelation.\n",
    "\n",
    "        > How to check after modeling : Draw the line over the points and look at the residual plot for patterns. If exists then there is no independence. Must be random.\n",
    "        Residual vs time plot and look for the seasonal or correlated pattern in residual values.\n",
    "\n",
    "        > How to fix :\n",
    "        Do more domain searching to add missing variables\n",
    "        Depending on the equation, or the relation between x and y, add polynomial or exp or log\n",
    "        <img src=\"../../images/autocorr_correction.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "\n",
    "    3. Homoscedasticity: The variance of the errors is constant across all levels of the independent variables.\n",
    "        > The variance of the residuals should not vary as the target is increased in residual plot\n",
    "        > Generally, non-constant variance arises in presence of outliers or extreme large values. \n",
    "        > If a feature has a wide range of numbers then this can happen as the feature might not be a good predictor\n",
    "\n",
    "        > How to check before modeling: Plot a variable against the target, find best fit and look at residual plot. If there is a funnel/dual cone/reverse funnel pattern then it might be heteroscedastic. \n",
    "\n",
    "        > How to check after modeling : residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern. Wreusch-Pagan / Cook – Weisberg test or White general test or Goldfledt-Quant test to detect this phenomenon\n",
    "\n",
    "        > How to fix it : There might be two/multiple regression liens that can explain if divided based on pattern. Log variables might help.\n",
    "        <img src=\"../../images/hetero_example.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "    4. Normality: The errors follow a normal distribution.\n",
    "        > The spread of the errors at a given point, should be normally distributed\n",
    "        > prediction errors/residuals need to be normally distributed\n",
    "        > Sometimes the error distribution is \"skewed\" by the presence of a few large outliers.\n",
    "        > Calculation of confidence intervals and various significance tests for coefficients are all based on the assumptions of normally distributed errors.\n",
    "        > If the error distribution is significantly non-normal, confidence intervals may be too wide or too narrow. \n",
    "        > The dependent and independent variables in a regression model do not need to be normally distributed by themselves   \n",
    "        > the normal error assumption is usually justified by appeal to the central limit theorem, which holds in the case where many random variations are added together.\n",
    "\n",
    "        > How to check before modeling: plot a histogram on your residuals or employ a Q-Q plot, which also helps us visually determine if our residuals follow a normal distribution. if there is a skew in your residuals. A log transformation on your dependent variable may help.\n",
    "\n",
    "        > How to check after modeling : The Kolmogorov-Smirnov test, the Shapiro-Wilk test, the Jarque-Bera test, and Anderson-Darling test.\n",
    "\n",
    "        > How to fix it : check the extreme values of the variable\n",
    "\n",
    "    5. No multicollinearity: The independent variables are not highly correlated with each other.\n",
    "        > independent variables show moderate to high correlation\n",
    "        > tough task to figure out the true relationship of a predictors with response variable\n",
    "        > difficult to find out which variable is actually contributing to predict the response variable.\n",
    "        > the standard errors tend to increase and the confidence interval becomes wider leading to less precise estimates of slope parameters\n",
    "        > two variables vary very similarly and contain the same kind of information.\n",
    "        > only the complexity of the model increase, and no new information or pattern is learned by the model.\n",
    "        > \n",
    "\n",
    "        > How to check before modeling:  use scatter plot/heat map to visualize correlation effect among variables\n",
    "\n",
    "        > How to check after modeling : VIF factor. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity.\n",
    "        VIF = 1/(1-R2) for each variable\n",
    "\n",
    "        > How to fix it : drop and check vif\n",
    "\n",
    "    6. No endogeneity: There is no relationship between the errors and the independent variables.\n",
    "        > Causes of endogeneity include omitted variables, simultaneity, and sample selection in the estimated equation. \n",
    "        > OLS estimator will be inconsistent and biased,\n",
    "\n",
    "        > How to check before modeling:  \n",
    "\n",
    "        > How to check after modeling : Durbin-Wu-Hausman test, which compares the OLS estimator with an alternative estimator that is robust to endogeneity, such as the instrumental variables (IV) estimator.\n",
    "\n",
    "        > How to fix it : reduce the sources of endogeneity by including more relevant variables, improving the data quality, or using a different research design that avoids the causal ambiguity.\n",
    "        instrumental variables (IV) estimator, which uses a variable that is correlated with the endogenous explanatory variable but not with the error term\n",
    "\n",
    "Source : \n",
    "[youtube](https://www.youtube.com/playlist?list=PLTNMv857s9WUI1Nz4SssXDKAELESXz-bi)\n",
    "[blog 0](https://medium.com/@andrewhnberry/checking-your-linear-regression-assumptions-and-how-to-check-them-338f770acb57)\n",
    "[blog 1](https://godatadrive.com/blog/basic-guide-to-test-assumptions-of-linear-regression-in-r)\n",
    "[blog 2](https://people.duke.edu/~rnau/testing.htm)\n",
    "[blog 3](https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/)\n",
    "[blog 4](https://www.geeksforgeeks.org/assumptions-of-linear-regression/)\n",
    "[blog 5](https://www.jmp.com/en_in/statistics-knowledge-portal/what-is-regression/simple-linear-regression-assumptions.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Working\n",
    " > The aim is to find BLUE (best linear unbiased estimate) or the best fit line which explains all the variance across the data\n",
    " > This is sually an equation involving target on one side and linear additive combination of features on the other which are not in polynomial or expoenential or logarithmic form\n",
    " > y = coefficient + ax1 + bx2 + cx3 + dx4 | ... | + error\n",
    " > where a,b,c,d ... are slope values and coefficient is the intercept in multi dimensional plane\n",
    " > here coeeficient is the value when features are 0\n",
    " > and slope is the unit change in target wrt. unit change in feature\n",
    "\n",
    " > Cost function : to reduce overall distance between each point in dataset and predcited value by the line\n",
    " > in the below equation is one way to find the distance b/w predicted and actual, this can be replaced with absolute also. where m is total number of values\n",
    "\n",
    "  <img src=\"../../images/linear_cost.png\">\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/1-OGRohmH2s)\n",
    "\n",
    "> Gradient Descent\n",
    "\n",
    "  - Infinite number of lines can be drawn and then the cost function can be calculated for all but thats computationally expensive\n",
    "  - So by keeping coefficent constant, and varying the slope values, for all the variables values, cost function is calculated.\n",
    "  - Then a grpah is drawn between all possible m values and cost function's output to find the lowest point (gloabl minima) on the curve which will give the best m for line \n",
    "  - This finding of global minima (slope = 0) is using gradient descent which is based on convergence theorem which moves towards minima using derivative values of slope and a learning rate\n",
    "  - To know which way is global minima, all the sides needs to be considered before moving in any direction\n",
    "\n",
    "> LR using Ordinary Least square (OLR)\n",
    "- TODO  https://youtu.be/KZ1mWboXE6g\n",
    " \n",
    "> Multiple LR using Matrix \n",
    "  - In this image, the a is all the beta values including coefficient\n",
    "  - <img src=\"../../images/multi_reg_matrix.png\"> \n",
    "    \n",
    "  - Using direct formula : (Xt.X)^-1.Xt.Y\n",
    "  - Using single valur decomposition\n",
    "  - Using QR decomposition\n",
    "\n",
    "\n",
    "Source : \n",
    "[youtube](https://youtu.be/_OyKjstWe80)\n",
    "[blog](https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0, b1, b2 = [-2.40119363  1.62732095 -0.29177719]\n",
      "Predictions = [1. 2. 3.]\n",
      "Error = [-0. -0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# MLR using Matrix\n",
    "import numpy as np\n",
    "x = np.array([[100, 212, 356], [4.25, 10.99, 16.2],  [-234, -465, -678]], np.int32)\n",
    "y = np.array([1,2,3])   \n",
    "# formula : (Xt.X)^-1.Xt.Y\n",
    "betas = np.dot(np.dot(np.linalg.inv(np.dot((x.T), x)),(x.T)),y)\n",
    "np.set_printoptions(suppress=False)\n",
    "print(\"b0, b1, b2 = {}\".format(betas))\n",
    "preds = np.dot(x,betas)\n",
    "print(\"Predictions = {}\".format(preds))\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"Error = {}\".format(preds-y))\n",
    "\n",
    "#source : https://cmdlinetips.com/2020/03/linear-regression-using-matrix-multiplication-in-python-using-numpy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code\n",
    "\n",
    "source : [blog](https://www.freecodecamp.org/news/data-science-with-python-8-ways-to-do-linear-regression-and-measure-their-speed-b5577d75f8b/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 1.0\n",
      "Betas : [ 0.00863046  0.01082108 -0.01180701]\n",
      "Beta0 : 2.046179467416178\n",
      "Predictions : [2.11436926]\n"
     ]
    }
   ],
   "source": [
    "# MLR using scikit learn\n",
    "# !pip install scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(x, y)\n",
    "print(\"Score : {}\".format(reg.score(x, y)))\n",
    "print(\"Betas : {}\\nBeta0 : {}\".format(reg.coef_, reg.intercept_))\n",
    "print(\"Predictions : {}\".format(reg.predict(np.array([[3, 5, 1]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betas : [-2.40119363  1.62732095 -0.29177719]\n",
      "Model : \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                    nan\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Tue, 02 Jan 2024   Prob (F-statistic):                nan\n",
      "Time:                        17:16:02   Log-Likelihood:                 81.846\n",
      "No. Observations:                   3   AIC:                            -157.7\n",
      "Df Residuals:                       0   BIC:                            -160.4\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -2.4012        inf         -0        nan         nan         nan\n",
      "x2             1.6273        inf          0        nan         nan         nan\n",
      "x3            -0.2918        inf         -0        nan         nan         nan\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   1.012\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.488\n",
      "Skew:                           0.643   Prob(JB):                        0.784\n",
      "Kurtosis:                       1.500   Cond. No.                     1.42e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.42e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 3 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1795: RuntimeWarning: divide by zero encountered in divide\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1795: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/home/radial/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1717: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n"
     ]
    }
   ],
   "source": [
    "# MLR using statsmodel\n",
    "# !pip install statsmodels\n",
    "import statsmodels.api as sm\n",
    "model = sm.OLS(y,x)\n",
    "results = model.fit()\n",
    "print(\"Betas : {}\".format(results.params))\n",
    "print(\"Model : \\n{}\".format(results.summary()))\n",
    "# source  : [blog](https://datatofish.com/statsmodels-linear-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pros and cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso ridge : https://www.youtube.com/watch?v=9lRv01HDU0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
