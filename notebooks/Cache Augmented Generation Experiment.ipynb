{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223e36f2-60c0-4e18-b4aa-8c6fb5b41f49",
   "metadata": {},
   "source": [
    "# 1. Generate Chunks & Summaries\n",
    "\n",
    "First step is to take input data in pdf (Harry Potter books in our case) and generate the **chunks** and **summaries** for it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc619af-f841-4e86-98d5-cfe7924fcc9f",
   "metadata": {},
   "source": [
    "## a). Import required modules\n",
    "\n",
    "We will import all the required modules for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f28a34d0-326f-469c-9948-6a74b283f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import json\n",
    "from config import *\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "import torch\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from time import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache, OffloadedCache, QuantizedCache\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87167ffc-acba-45af-b3c7-610ff374f9e2",
   "metadata": {},
   "source": [
    "## b). Initalize llm and prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f216d8-aa93-4555-9371-2de990993e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7956/31763356.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(api_key=OPEN_AI_KEY)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0, openai_api_key = OPEN_AI_KEY, model_name = MODEL_NAME_PARENT)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=OPEN_AI_KEY)\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "                    {text}\n",
    "                    CONSCISE SUMMARY:\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70aab8e-1523-48a0-9b83-add0ce540e2c",
   "metadata": {},
   "source": [
    "## c). Load Data\n",
    "\n",
    "We will be loading the harry potter pdf using pymupdf module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf2e5a-034f-4541-a013-5b82d434d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: str) -> pymupdf.Document:\n",
    "    \"\"\"\n",
    "        Reading a PDF Document using PyMuPdf.\n",
    "    \"\"\"\n",
    "    doc = pymupdf.open(data_path)\n",
    "    return doc\n",
    "\n",
    "data = load_data(\"./harrypotter.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa926dff-fbb1-463b-92ff-1d234e0a705c",
   "metadata": {},
   "source": [
    "## d). Generating Chunks\n",
    "\n",
    "We will be breaking the document into chunks of around 10k tokens and store it as a dictionary. We will also persist it as a json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef10ea-ef07-4102-8ad6-58a4abaa61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunks(data: pymupdf.Document, chunk_size: int = 10000) -> dict:\n",
    "    \"\"\"\n",
    "        Creating chunks of fixed size and store as a json.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    chunk = 0\n",
    "    final_dict = {}\n",
    "    for page in data:\n",
    "        text = text + \"\\n\" +  page.get_text()\n",
    "        if len(text.split()) > chunk_size:\n",
    "            final_dict[f\"chunk_{chunk}\"] = text\n",
    "            chunk +=1\n",
    "            text = \"\"\n",
    "    \n",
    "    with open(\"./chunks.json\", 'w') as file:  \n",
    "        json.dump(final_dict, file)\n",
    "        \n",
    "    return final_dict\n",
    "\n",
    "chunk_dictionary = generate_chunks(data, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093094e-de04-4e68-8509-59f47fedbb7f",
   "metadata": {},
   "source": [
    "## e). Generate Summaries\n",
    "\n",
    "Now for each chunk we will be generating its summaries and storing it as dictionary. We will also persist it as json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf4b27-becf-43c5-a0a3-95480cff82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_summaries(chunk_dictionary: dict, prompt: str) -> dict:\n",
    "    \"\"\"\n",
    "        For each chunk, generate summary and store as a json.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    summary_dict = {}\n",
    "    \n",
    "    for i in range(len(chunk_dictionary)):\n",
    "        summary = chain.run(chunk_dictionary[f\"chunk_{i}\"])\n",
    "        summary_dict[f\"chunk_{i}\"] = summary\n",
    "\n",
    "    with open(\"./summary.json\", 'w') as file:  \n",
    "        json.dump(summary_dict, file)\n",
    "\n",
    "    return summary_dict\n",
    "\n",
    "summary_dictionary = generate_chunk_summaries(chunk_dictionary, prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a110091-ec4b-491a-b368-e8c9e92ac5c6",
   "metadata": {},
   "source": [
    "# 2. Generate K-V Cache\n",
    "\n",
    "Once we have the dictionaries ready, we will use the **chunks** (not the summaries) to create their **K-V Cache**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ecb37-c7f4-41b0-bd41-644d3f7e1360",
   "metadata": {},
   "source": [
    "## a). Initialize the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b60aae1-ad0d-4ce7-bedb-9c0b0237e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_CHILD, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME_CHILD,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            token=HF_TOKEN\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaffd216-a68a-4bde-b9fa-c89f2953d4ed",
   "metadata": {},
   "source": [
    "## b). Load the chunks dictionary\n",
    "\n",
    "We will load the chunks dictionary that we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0f095f-7509-46a5-bc2f-a1ba2aaf134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(data_path: str) -> dict:\n",
    "    \"\"\"\n",
    "        funtion to load a json based on the string path provided\n",
    "    \"\"\"\n",
    "    with open(data_path, 'r') as file:  \n",
    "        final_dict = json.load(file)\n",
    "    return final_dict\n",
    "\n",
    "chunks_dictionary = load_json_data(\"./chunks.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a74ee-815b-4b2f-92da-968166d22153",
   "metadata": {},
   "source": [
    "## c). Iterate to create Dynamic Cache\n",
    "\n",
    "Next, we will iterate through the chunk dictionary one by one and generate its k-v cache. For each chunk we will do following things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babffa74-3390-4dc0-af43-1706df42d86a",
   "metadata": {},
   "source": [
    "We will first create a **prompt instruction**. Here we will pass the chunk in a structured prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b8177-12db-4c3f-8a6e-6b2aebf5b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_instruction = \"Answer the question with a super short answer.\"\n",
    "\n",
    "# We will take the very first chunk for example purpose.\n",
    "chunk = chunks_dictionary[list(chunks_dictionary.keys())[0]]\n",
    "chunk_name = list(chunks_dictionary.keys())[0]\n",
    "knowledges = f\"\"\"\n",
    "                <|begin_of_text|>\n",
    "                <|start_header_id|>system<|end_header_id|>\n",
    "                You are an assistant for giving short answers based on given context.<|eot_id|>\n",
    "                <|start_header_id|>user<|end_header_id|>\n",
    "                Context information is bellow.\n",
    "                ------------------------------------------------\n",
    "                {chunk}\n",
    "                ------------------------------------------------\n",
    "                {answer_instruction}\n",
    "                Question:\n",
    "                Summarize the entire document while keeping all the keypoints intact.\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886149d-3726-4b77-b5de-a31b87e8ecd5",
   "metadata": {},
   "source": [
    "We will then use the **model** we selected to create the **k-v cache** for this chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af5a65-8c64-4c37-b6ae-1f7d07ec1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device\n",
    "embed_device = model.model.embed_tokens.weight.device\n",
    "print(f\"device selected - {embed_device}\")\n",
    "\n",
    "input_ids = tokenizer.encode(knowledges, return_tensors=\"pt\").to(embed_device)\n",
    "past_key_values = OffloadedCache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        use_cache=True,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ddcc89-c8d0-4be8-9188-0ad2664d8c2f",
   "metadata": {},
   "source": [
    "Finally, we will **save the k-v cache in disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f669d044-fafb-4e4e-a97e-d0d9a0063275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the cache from model\n",
    "kv = outputs.past_key_values\n",
    "\n",
    "# Extract Keys\n",
    "key_cache = kv.key_cache\n",
    "\n",
    "# Extract Values\n",
    "value_cache = kv.value_cache\n",
    "\n",
    "# Extract the device on which training done\n",
    "original_device = kv.original_device\n",
    "\n",
    "# Save everything\n",
    "torch.save(key_cache, f\"{chunk_name}_key.pt\")\n",
    "torch.save(value_cache, f\"{chunk_name}_value.pt\")\n",
    "torch.save(original_device, f\"{chunk_name}_od.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5a80e-fbbb-4064-beca-600e0ad23122",
   "metadata": {},
   "source": [
    "Now that we know, how individual chunk cache is being generated and stored, lets see how to **iterate over all the chunks** and create its Dynamic Cache. Remember, this step will take some amount of disk memory. So make sure to have **atleast 50 GB** extra free space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4a9f2-80b1-4c4d-b950-3a8e2031fd97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_gpu_allocation():\n",
    "    \"\"\"\n",
    "        Function to find the gpu usage\n",
    "    \"\"\"\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "    print(f\"Memory Allocated: {allocated}, Memory Reserved: {reserved}\")\n",
    "\n",
    "def preprocess_knowledge(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompt: str,\n",
    ") -> DynamicCache:\n",
    "    \"\"\"\n",
    "    Prepare knowledge kv cache for CAG.\n",
    "    Args:\n",
    "        model: HuggingFace model with automatic device mapping\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        prompt: The knowledge to preprocess, which is basically a prompt\n",
    "\n",
    "    Returns:\n",
    "        DynamicCache: KV Cache\n",
    "    \"\"\"\n",
    "    print(\"Before Embedding Step:\")\n",
    "    find_gpu_allocation()\n",
    "    embed_device = model.model.embed_tokens.weight.device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(embed_device)\n",
    "    past_key_values = OffloadedCache()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "    print(\"After Caching Step:\")\n",
    "    find_gpu_allocation()\n",
    "    result = outputs.past_key_values\n",
    "    \n",
    "    # Follow below steps to clean the GPU memory\n",
    "    outputs.past_key_values = None\n",
    "    del outputs.past_key_values\n",
    "    del outputs\n",
    "    del input_ids\n",
    "    del embed_device\n",
    "    del model\n",
    "    del past_key_values\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"After Deletion of Everything Step:\")\n",
    "    find_gpu_allocation()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def write_kv_cache(kv: DynamicCache, chunk):\n",
    "    \"\"\"\n",
    "    Write the KV Cache to a file.\n",
    "    \"\"\"\n",
    "    key_cache = kv.key_cache\n",
    "    value_cache = kv.value_cache\n",
    "    original_device = kv.original_device\n",
    "    torch.save(key_cache, f\"./chunk_caches/{chunk}_key.pt\")\n",
    "    torch.save(value_cache, f\"./chunk_caches/{chunk}_value.pt\")\n",
    "    torch.save(original_device, f\"./chunk_caches/{chunk}_od.pt\")\n",
    "    # torch.save(kv, f\"./chunk_caches/test.pt\")\n",
    "\n",
    "\n",
    "def prepare_kvcache(documents, answer_instruction: str = None, chunk = None):\n",
    "    # Prepare the knowledges kvcache\n",
    "\n",
    "    if answer_instruction is None:\n",
    "        answer_instruction = \"Answer the question with a super short answer.\"\n",
    "    knowledges = f\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are an assistant for giving short answers based on given context.<|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Context information is bellow.\n",
    "    ------------------------------------------------\n",
    "    {documents}\n",
    "    ------------------------------------------------\n",
    "    {answer_instruction}\n",
    "    Question:\n",
    "    Summarize the entire document while keeping all the keypoints intact.\n",
    "    \"\"\"\n",
    "    # Get the knowledge cache\n",
    "    t1 = time()\n",
    "    kv = preprocess_knowledge(model, tokenizer, knowledges)\n",
    "    print(\"kvlen: \", kv.key_cache[0].shape[-2])\n",
    "    write_kv_cache(kv, chunk)\n",
    "    t2 = time()\n",
    "    return kv, t2 - t1\n",
    "\n",
    "def dynamic_cache_creator(knowledges, chunk):\n",
    "    answer_instruction = None\n",
    "    knowledge_cache, prepare_time = prepare_kvcache(knowledges, answer_instruction=answer_instruction, chunk=chunk)\n",
    "    kv_len = knowledge_cache.key_cache[0].shape[-2]\n",
    "    print(f\"KVcache prepared in {prepare_time} seconds\")\n",
    "    return knowledge_cache, prepare_time, kv_len\n",
    "\n",
    "dynamic_cache_dict = {}\n",
    "\n",
    "for i, (chunk, content) in enumerate(chunks_dictionary.items()):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "    print(\"*********\")\n",
    "    print(f\"iteration - {i}\")\n",
    "    print(\"token length: \", len(content.split()))\n",
    "    knowledge_cache, prepare_time, kv_len = dynamic_cache_creator(content, chunk)\n",
    "\n",
    "print(\"KV cache generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a34d9c-1251-442a-b7d7-b5c9836d0a47",
   "metadata": {},
   "source": [
    "# 3. Generate Embeddings\n",
    "\n",
    "Now that we have created the K-V Cache, another aspect that will be used for answer generation is the Embeddings of the Chunk Summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96243643-b6da-4be5-ad47-2d784740554c",
   "metadata": {},
   "source": [
    "## a). Load Chunk Summaries\n",
    "\n",
    "We will load the chunk summaries that we persisted as a json in step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef88847-4cbc-4005-a073-9d70ed933732",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dictionary = load_json_data(\"./summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453cb21a-2c51-40c8-98bc-09f3bb25a844",
   "metadata": {},
   "source": [
    "## b). Create a Vector Store\n",
    "\n",
    "Based on the summaries, we will **create the vectorstore** in **Chroma DB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "464574a5-d029-4821-bf9b-dc9e88f9f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_initialize_retriever(summary_dict):\n",
    "    id_key = \"doc_id\"\n",
    "    doc_ids = list(summary_dict.keys())\n",
    "    summary_texts = [Document(page_content=s, metadata={id_key: doc_ids[i]}) for i, s in enumerate(summary_dict.values())]\n",
    "    vectorstore = Chroma.from_documents(documents=summary_texts, embedding=embeddings)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "vectorstore = create_and_initialize_retriever(summary_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001b021-0da6-4534-ac78-dca5aee27397",
   "metadata": {},
   "source": [
    "# 4. Generate Answers\n",
    "\n",
    "Now that we have all the things ready - the cache and the vector store, we will move on to the answer generator step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca73a5f-694f-44d5-b361-24562c278e82",
   "metadata": {},
   "source": [
    "## a). Fetch Correct Chunks\n",
    "\n",
    "Based on the query of the user, we will search the Vector DB to find the **most relevant chunk** which has the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58adf35-3c87-4672-9b9b-c1fb9fb71740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted chunk is - chunk_99\n"
     ]
    }
   ],
   "source": [
    "def fetch_correct_chunk(query, vectorstore):\n",
    "\n",
    "    embedding_vector = embeddings.embed_query(query)\n",
    "    docs = vectorstore.similarity_search_by_vector(embedding_vector)\n",
    "\n",
    "    chunk = docs[0].metadata[\"doc_id\"]\n",
    "\n",
    "    return chunk\n",
    "\n",
    "query = \"Who is Nagini?\"\n",
    "\n",
    "chunk_name = fetch_correct_chunk(query, vectorstore)\n",
    "\n",
    "print(f\"The extracted chunk is - {chunk_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe28a02-9abd-4ab6-8fab-f6724c410a7f",
   "metadata": {},
   "source": [
    "## b). Extract the correct k-v cache\n",
    "\n",
    "Based on the chunk name, we will extract its **Dynamic Cache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b41500e1-0f24-41df-a12b-ceee8b059c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_knowledge_cache(chunk):\n",
    "    knowledge_cache = OffloadedCache()\n",
    "    knowledge_cache.key_cache = torch.load(f\"./chunk_caches/{chunk}_key.pt\", weights_only=False)\n",
    "    knowledge_cache.value_cache = torch.load(f\"./chunk_caches/{chunk}_value.pt\", weights_only=False)\n",
    "    knowledge_cache.prefetch_stream = torch.cuda.Stream()\n",
    "    knowledge_cache.original_device = torch.load(f\"./chunk_caches/{chunk}_od.pt\", weights_only=False)\n",
    "    return knowledge_cache\n",
    "\n",
    "knowledge_cache = initialize_knowledge_cache(chunk_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86167659-c9aa-425b-8b0b-8009c1b5f3f1",
   "metadata": {},
   "source": [
    "## c). Use KV Cache to generate answer embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47d4d8-5cae-4ccf-b582-8e6769422b52",
   "metadata": {},
   "source": [
    "First we will create a **structured prompt** for user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbab2590-147a-4734-a845-aca2dec51b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query we have already defined in the extraction of embeddings phase\n",
    "generation_prompt = f\"\"\"\n",
    "    {query}\n",
    "    Give very concise answer. In max one sentence\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42133a05-acc8-487c-aeac-575024f04235",
   "metadata": {},
   "source": [
    "Next, we will **convert the prompt to its tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6786e0c-9c6a-4f89-859c-00e163d4a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(generation_prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751bc36-0d52-4af4-a9d4-589b8502d6d7",
   "metadata": {},
   "source": [
    "Now, we will generate an **embeddings reponse**, from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74a763ef-2520-42f4-9a43-f4d9f35c7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    input_ids: torch.Tensor,\n",
    "    past_key_values,\n",
    "    max_new_tokens: int = 300\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate text with greedy decoding.\n",
    "\n",
    "    Args:\n",
    "        model: HuggingFace model with automatic device mapping\n",
    "        input_ids: Input token ids\n",
    "        past_key_values: KV Cache for knowledge\n",
    "        max_new_tokens: Maximum new tokens to generate\n",
    "    \"\"\"\n",
    "\n",
    "    embed_device = model.model.embed_tokens.weight.device\n",
    "\n",
    "    origin_ids = input_ids\n",
    "    input_ids = input_ids.to(embed_device)\n",
    "\n",
    "    output_ids = input_ids.clone()\n",
    "    next_token = input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(\n",
    "                input_ids=next_token, \n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = next_token_logits.argmax(dim=-1).unsqueeze(-1)\n",
    "            next_token = next_token.to(embed_device)\n",
    "\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            output_ids = torch.cat([output_ids, next_token], dim=1)\n",
    "\n",
    "            if next_token.item() in model.config.eos_token_id:\n",
    "                break\n",
    "    return output_ids[:, origin_ids.shape[-1]:]\n",
    "\n",
    "answer_embeddings = generate(model, input_ids, knowledge_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad40f75-bbb5-42b4-bf30-fe292fd2563b",
   "metadata": {},
   "source": [
    "Lastly, we will **decode this reponse back to text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68931ed0-4da3-4afa-8c4d-f72f39aa7704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nagini is a snake that was once a Horcrux created by Albus Dumbledore to protect his son Ariana, but was later used by Gellert Grindelwald to gain power, and was later used by Voldemort to kill Ariana Dumbledore.\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(answer_embeddings[0], skip_special_tokens=True, temperature=0.5)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3abbe5a-c5b3-48cf-a223-394e76c37a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CAG)",
   "language": "python",
   "name": "cag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
