{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c60d1752-406b-4361-a213-efcdc032b611",
      "metadata": {
        "tags": [],
        "id": "c60d1752-406b-4361-a213-efcdc032b611"
      },
      "outputs": [],
      "source": [
        "from pypdf import PdfReader\n",
        "from litellm import completion\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "920f0e32-7a19-431e-aa65-0646e80e46d8",
      "metadata": {
        "tags": [],
        "id": "920f0e32-7a19-431e-aa65-0646e80e46d8",
        "outputId": "c5d0bf40-91c7-4004-87bd-09ab94437a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of pages: 12\n"
          ]
        }
      ],
      "source": [
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "FILE_PATH = \"learning_context.pdf\"\n",
        "reader = PdfReader(FILE_PATH)\n",
        "num_of_pages = len(reader.pages)\n",
        "print(f\"Number of pages: {num_of_pages}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7894d44e-6dc1-473d-a8f0-4ceeb550bddf",
      "metadata": {
        "id": "7894d44e-6dc1-473d-a8f0-4ceeb550bddf",
        "outputId": "79e15c10-a1c9-448d-ec35-9d981bd2984c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================== Page Summary ================== \n",
            "The paper proposes a method called FILCO to improve context filtering for retrieval-augmented generation in knowledge-intensive tasks such as question answering and fact verification. FILCO enhances context quality by identifying useful context and training filtering models to improve generation outputs. The method outperforms existing approaches in various tasks and effectively filters out irrelevant content, enabling generators to predict correct answers more accurately. Imperfect retrieval systems often provide distracting or irrelevant content, leading to issues like hallucination in generated outputs. Existing approaches focus on optimizing content at the passage level or selectively retrieving relevant passages. Enhancing context quality is crucial as proper grounding on supporting content results in better model performance. Overall, FILCO addresses the challenges of over-reliance on imperfect context and enhances the accuracy of generation models in knowledge-intensive language tasks.\n"
          ]
        }
      ],
      "source": [
        "def create_message(content:str, role:str) -> dict:\n",
        "    return {\"content\": content, \"role\": role}\n",
        "\n",
        "user_prompt_template = \"\"\"Summarize the given text below. Summary MUST be less than 200 words.\n",
        "\n",
        "text: {text}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "# Read first page of the paper\n",
        "page = reader.pages[0]\n",
        "text = page.extract_text()\n",
        "\n",
        "system_message = create_message(\"You are an expert in summarizing research papers.\", \"system\")\n",
        "user_message = create_message(content=user_prompt_template.format(text=text), role=\"user\")\n",
        "\n",
        "messages = []\n",
        "messages.append(system_message)\n",
        "messages.append(user_message)\n",
        "\n",
        "# Summarize first page\n",
        "response = completion(model=\"gpt-3.5-turbo-0125\", messages=messages)\n",
        "print(\"================== Page Summary ================== \")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563d1894-fa81-4c9d-93c5-beedbe9036d3",
      "metadata": {
        "tags": [],
        "id": "563d1894-fa81-4c9d-93c5-beedbe9036d3",
        "outputId": "4de989b5-b405-464b-d462-2635f2644fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================== Page Summary START ================== \n",
            "The research paper proposes FILCO, a method that enhances context filtering for retrieval-augmented generation tasks. By identifying useful context and training context filtering models, FILCO improves the quality of context provided to generation models. Experimental results show that FILCO outperforms existing approaches in tasks such as extractive question answering, fact verification, and dialog generation. The paper addresses the challenges of imperfect retrieval systems providing irrelevant content, leading to issues such as hallucinations in generated output. Current methods focus on optimizing context at the passage level, such as reranking relevant passages or selecting only evidential ones. Improving the quality of context is crucial for guiding generation models to produce accurate outputs.\n",
            "================== Page Summary END ================== \n",
            "\n",
            "================== Page Summary START ================== \n",
            "The paper introduces a method called FILCO for fine-grained context filtering in text generation tasks, training on content selected based on three measures: whether passages contain the generation output, lexical overlap, and conditional cross-mutual information. Experimental results on six language datasets show that FILCO outperforms baseline methods in question answering, fact verification, and dialog generation tasks. The method reduces the length of prompts across tasks and improves generation with positive and negative passages. Comparing filtering methods, STRINC, LEXICAL, and CXMI-based filtering are found to be optimal for different types of tasks. FILCO maintains its advantage over baseline methods even in complex multi-passage settings. The paper outlines notation, oracle filtering strategies, training context filtering models, and generating with filtered contexts in retrieval-augmented generation tasks.\n",
            "================== Page Summary END ================== \n",
            "\n",
            "================== Page Summary START ================== \n",
            "The text introduces three approaches for filtering potentially useful content from retrieved passages: String Inclusion, Lexical Overlap, and Conditional Cross-Mutual Information (CXMI). String Inclusion determines if a text span contains the output exactly, while Lexical Overlap calculates the unigram overlap between examples and candidate text spans. CXMI measures the probability difference in generating the expected output with and without context augmentation. These filtering methods aim to select relevant text spans for tasks like question answering and dialog generation. The study discusses training context filtering models and using them at test time when the answer is unknown. Filtering methods are applied to create training data and predict filtered contexts at test time, which are then used in conjunction with a generation model to predict the output. Each filtering method has its strengths and limitations in effectively selecting relevant information for different types of tasks.\n",
            "================== Page Summary END ================== \n",
            "\n",
            "================== Page Summary START ================== \n",
            "The text discusses a method for knowledge-intensive language tasks that involve using retrieved passages for generation. It outlines the process of preparing context for inference by adding filtered context to training examples and providing filtered context during inference. The study experiments on tasks such as open-domain question answering, multi-hop question answering, long-form question answering, fact verification, and knowledge-grounded dialog generation using datasets built from Wikipedia articles. Evaluation metrics like EM, F1 score, and accuracy are used to assess model performance. The text also emphasizes the importance of selected text retrieval to reduce computational costs in training and inference. Additionally, the quality of retrieved passages for generation is evaluated to ensure effective performance. Table 1 provides statistics and evaluation metrics for the tasks, with a focus on the dataset characteristics and evaluation results.\n",
            "================== Page Summary END ================== \n",
            "\n",
            "================== Page Summary START ================== \n",
            "The text discusses the use of an adversarial Dense Passage Retriever (DPR) to retrieve Wikipedia passages for various tasks. The performance of top 1 and top 5 retrieved passages is evaluated across different datasets, with varying recall and precision results. It is noted that while involving more passages increases document coverage, it also leads to increased computation. The presence of redundant content in retrieved passages could potentially impact model performance. The text then outlines experiments and analysis conducted using FLAN-T5 and LLAMA 2 models for context filtering and end generation tasks. Both models are fine-tuned for specific tasks, with implementation details provided. Experimental methods include baseline approaches, such as concatenating all passages into the input, and proposed approaches like filtering sufficient context. The goal is to enhance generation efficiency by selecting precise and relevant information from retrieved passages.\n",
            "================== Page Summary END ================== \n",
            "\n",
            "================== Page Summary START ================== \n",
            "In this study, the authors propose a method for context filtering in generating answers for various question answering tasks. They compare their method with baseline approaches and evaluate its effectiveness using different models. The results show that applying context filtering significantly enhances performance across different datasets. The proposed method achieves notable improvements in extractive QA tasks, with increased EM scores. For more complex tasks like HotpotQA and ELI5, improvements are less significant due to task difficulty. However, for abstractive generation tasks, the method brings substantial accuracy and F1 score increases. The study demonstrates that filtering irrelevant content improves the model's ability to focus on relevant knowledge for generating accurate answers. Additionally, the method performs effectively for both positive and negative passages, particularly benefiting abstractive generation tasks.\n",
            "================== Page Summary END ================== \n",
            "\n",
            "================== Page Summary START ================== \n",
            "The research paper evaluates the impact of context filtering on improving the accuracy of outputs in text generation models. The study finds that removing distracting content from positive and negative passages leads to more correct outputs. By filtering contexts at the sentence level, input length can be reduced substantially, with a corresponding increase in answer precision. The study compares different context filtering strategies and finds that the FILCO method effectively reduces input length by 44-64% while maintaining high precision in output generation. Specifically, the FILCO method shows improvements in precision compared to other strategies such as SILVER and PSG baseline for tasks like HotpotQA and WoW. The research highlights the importance of context filtering in improving the performance of text generation models.\n",
            "================== Page Summary END ================== \n",
            "\n",
            "================== Page Summary START ================== \n",
            "The study compared the performance of different context filtering measures on various tasks using the FLAN-T5 and LLAMA 2 models. Results showed that tasks like NQ and TQA benefited most from STRINC filtering, while FEVER, HOTPOTQA, and ELI5 performed best with CXMI. LEXICAL filtering was found to be suitable for tasks like WoW. The study found that extractive tasks performed better with STRINC, while abstractive tasks were more suited to LEXICAL filtering. For more complex tasks like fact verification, CXMI was the most effective measure. Integrating multiple passages as context input was beneficial, particularly for tasks like multi-hop QA. The FLAN-T5 model showed consistent performance across tasks when using multiple passages, with the study comparing different methods and reporting results against various baselines.\n",
            "================== Page Summary END ================== \n",
            "\n",
            "================== Page Summary START ================== \n",
            "The research introduces a method called FILCO for providing context to improve model generations in various knowledge-intensive tasks. FILCO outperforms existing baselines by a large margin and shows improved performance when aggregating multiple top-ranked passages. The study highlights the importance of optimizing retrieval for augmentation and filtering out distracting content to enhance model performance. FILCO brings an average increase of 2.8 to 3.0 points across different datasets and tasks. The method is effective in filtering context for various tasks but may have limitations in certain data domains and under specific evaluation metrics. Overall, FILCO shows promise in facilitating more accurate and faithful model generations in diverse scenarios.\n",
            "================== Page Summary END ================== \n",
            "\n",
            "================== Page Summary START ================== \n",
            "The text discusses a method for evaluating model retrieval, filtering, and generation performance using metrics like Exact Match and Unigram F1. The method requires training models to filter context and generate output, requiring computational resources based on model architecture and size. Despite this, the method is more computationally efficient than traditional full-passage augmentation. The authors recommend readers verify the effectiveness of the method before applying it to special-domain datasets. They also highlight the potential for neural- or human-based evaluations alongside automatic metrics due to potential inaccuracies with complex tasks and models. The work was supported by a grant from Bosch, and the study encourages further exploration and application of the method beyond the field of computational linguistics.\n",
            "================== Page Summary END ================== \n",
            "\n",
            "================== Page Summary START ================== \n",
            "The summary reflects an overview of various research papers related to computational linguistics. These papers cover topics like active retrieval augmented generation, large-scale challenge datasets for reading comprehension, open-domain question answering, passage retrieval for question answering, nearest neighbor machine translation, dialogue generation, natural language processing tasks, and knowledge-intensive tasks. The papers discuss approaches such as retrieval-augmented generation, latent retrieval for question answering, reader-guided passage reranking, and augmented language models. The researchers explore the effectiveness of different memory types, benchmarking knowledge-intensive language tasks, and utilizing BERT for passage re-ranking. The studies represent advancements and challenges in the field of computational linguistics through the development of models and datasets for various NLP tasks.\n",
            "================== Page Summary END ================== \n",
            "\n",
            "================== Page Summary START ================== \n",
            "The text provides a list of research papers related to natural language processing and question answering systems. Various studies explore topics such as stress testing question answering models, distractions in language models, retrieval augmentation in conversations, fact extraction and verification datasets, open-domain question answering, machine comprehension datasets, multi-hop question answering challenges, and deep learning for answer sentence selection. These papers aim to improve the performance and capabilities of language models through different techniques and datasets.\n",
            "================== Page Summary END ================== \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Summarize entire paper page by page\n",
        "for page_num in range(num_of_pages):\n",
        "    page = reader.pages[page_num]\n",
        "    text = page.extract_text()\n",
        "\n",
        "    system_message = create_message(\"You are an expert in summarizing research papers.\", \"system\")\n",
        "    user_message = create_message(content=user_prompt_template.format(text=text), role=\"user\")\n",
        "\n",
        "    messages = []\n",
        "    messages.append(system_message)\n",
        "    messages.append(user_message)\n",
        "\n",
        "    # Summarize first page\n",
        "    response = completion(model=\"gpt-3.5-turbo-0125\", messages=messages)\n",
        "    print(\"================== Page Summary START ================== \")\n",
        "    print(response.choices[0].message.content)\n",
        "    print(\"================== Page Summary END ================== \\n\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86fd36ab-8fff-4046-aaf3-8ac05118a09e",
      "metadata": {
        "id": "86fd36ab-8fff-4046-aaf3-8ac05118a09e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}