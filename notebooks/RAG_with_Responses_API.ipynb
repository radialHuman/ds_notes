{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "kkBYvb2GsaXC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt_C6kIesKG0"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2 pandas tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.66.3"
      ],
      "metadata": {
        "id": "Jvo_glS8wKzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import concurrent\n",
        "import PyPDF2\n",
        "import os\n",
        "import pandas as pd\n",
        "import base64"
      ],
      "metadata": {
        "id": "8PbFSnTLsczh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "zL57HNxIteWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=userdata.get('openai'))\n",
        "dir_pdfs = 'openai_blog_pdfs' # have those PDFs stored locally here\n",
        "pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]"
      ],
      "metadata": {
        "id": "svZIFFmWsgwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdf_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeYbifNLtnqD",
        "outputId": "d6dc1ec8-31b1-4335-f485-f40bf0c79cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['openai_blog_pdfs/The court rejects Elon’s latest attempt to slow OpenAI down _ OpenAI.pdf', 'openai_blog_pdfs/Introducing deep research _ OpenAI.pdf', 'openai_blog_pdfs/New tools for building agents _ OpenAI.pdf', 'openai_blog_pdfs/OpenAI GPT-4.5 System Card _ OpenAI.pdf', 'openai_blog_pdfs/Introducing Operator _ OpenAI.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Vector Store with our PDFs\n",
        "\n",
        "- Create a Vector Store on OpenAI's servers\n",
        "- Upload files to the vector store"
      ],
      "metadata": {
        "id": "uz9NME2Ytx5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(store_name: str) -> dict:\n",
        "    try:\n",
        "        vector_store = client.vector_stores.create(name=store_name)\n",
        "        details = {\n",
        "            \"id\": vector_store.id,\n",
        "            \"name\": vector_store.name,\n",
        "            \"created_at\": vector_store.created_at,\n",
        "            \"file_count\": vector_store.file_counts.completed\n",
        "        }\n",
        "        print(\"Vector store created:\", details)\n",
        "        return details\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating vector store: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "mg8ZDBmeuawD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_name = \"openai_blog_store\"\n",
        "vector_store_details = create_vector_store(store_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPYX8c9Muhs1",
        "outputId": "1a959901-38e5-4384-f64b-aef0edcf73ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created: {'id': 'vs_67d5f672b158819189c250187c7b5eb2', 'name': 'openai_blog_store', 'created_at': 1742075506, 'file_count': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_single_pdf(file_path: str, vector_store_id: str):\n",
        "    file_name = os.path.basename(file_path)\n",
        "    try:\n",
        "        file_response = client.files.create(file=open(file_path, 'rb'), purpose=\"assistants\")\n",
        "        attach_response = client.vector_stores.files.create(\n",
        "            vector_store_id=vector_store_id,\n",
        "            file_id=file_response.id\n",
        "        )\n",
        "        return {\"file\": file_name, \"status\": \"success\"}\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {file_name}: {str(e)}\")\n",
        "        return {\"file\": file_name, \"status\": \"failed\", \"error\": str(e)}"
      ],
      "metadata": {
        "id": "EBgAxeVGtso3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_pdf_files_to_vector_store(vector_store_id: str):\n",
        "    pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]\n",
        "    stats = {\"total_files\": len(pdf_files), \"successful_uploads\": 0, \"failed_uploads\": 0, \"errors\": []}\n",
        "\n",
        "    print(f\"{len(pdf_files)} PDF files to process. Uploading in parallel...\")\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(upload_single_pdf, file_path, vector_store_id): file_path for file_path in pdf_files}\n",
        "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(pdf_files)):\n",
        "            result = future.result()\n",
        "            if result[\"status\"] == \"success\":\n",
        "                stats[\"successful_uploads\"] += 1\n",
        "            else:\n",
        "                stats[\"failed_uploads\"] += 1\n",
        "                stats[\"errors\"].append(result)\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "JolLrtB1t8XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload all files to the vector-store\n",
        "upload_pdf_files_to_vector_store(vector_store_details[\"id\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyC70S3JuBXf",
        "outputId": "7216e512-5537-40fb-c35a-9fce692e8201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 PDF files to process. Uploading in parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  2.81it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total_files': 5, 'successful_uploads': 5, 'failed_uploads': 0, 'errors': []}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standalone vector search\n",
        "\n",
        "Now that our vector store is ready, we are able to query the Vector Store directly and retrieve relevant content for a specific query. Using the new vector search API, we're able to find relevant items from our knowledge base without necessarily integrating it in an LLM query."
      ],
      "metadata": {
        "id": "CVPV74amy_Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's Deep Research?\"\n",
        "\n",
        "search_results = client.vector_stores.search(\n",
        "    vector_store_id=vector_store_details['id'],\n",
        "    query=query\n",
        ")"
      ],
      "metadata": {
        "id": "nm5ctLCduE9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for result in search_results.data:\n",
        "    print(str(len(result.content[0].text)) + ' of character of content from ' + result.filename + ' with a relevant score of ' + str(result.score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5IzLSykzKsQ",
        "outputId": "a2576fb3-8f02-487b-bc81-f337fac6664e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3484 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.9770935017954946\n",
            "3516 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.9590194714345182\n",
            "3260 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.9427399909585098\n",
            "3620 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.9249127384831187\n",
            "3332 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.890895622820891\n",
            "3474 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.8835218476424277\n",
            "3376 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.8135896906162389\n",
            "2772 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.7170967693481712\n",
            "3183 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.7150089987770629\n",
            "3387 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.7098273776863951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(search_results.data[0].content[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubwrPqEyzZN8",
        "outputId": "2db058ec-69cc-4d10-da2b-4d1c14f04237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introducing deep research | OpenAI\n",
            "\n",
            "\n",
            "February 2, 2025 Release\n",
            "\n",
            "Introducing deep research\n",
            "An agent that uses reasoning to synthesize large amounts of\n",
            "online information and complete multi-step research tasks\n",
            "for you. Available to Pro users today, Plus and Team next.\n",
            "\n",
            "Try on ChatGPT\n",
            "\n",
            "Listen to article 8:56 Share\n",
            "\n",
            "3/15/25, 2:25 PM Introducing deep research | OpenAI\n",
            "\n",
            "https://openai.com/index/introducing-deep-research/ 1/38\n",
            "\n",
            "https://openai.com/research/index/release/\n",
            "https://chatgpt.com/\n",
            "https://openai.com/\n",
            "\n",
            "\n",
            "Today we’re launching deep research in ChatGPT, a new agentic capability that conducts\n",
            "multi-step research on the internet for complex tasks. It accomplishes in tens of minutes\n",
            "what would take a human many hours.\n",
            "\n",
            "Deep research is OpenAI's next agent that can do work for you independently—you give it\n",
            "a prompt, and ChatGPT will find, analyze, and synthesize hundreds of online sources to\n",
            "create a comprehensive report at the level of a research analyst. Powered by a version of\n",
            "the upcoming OpenAI o3 model that’s optimized for web browsing and data analysis, it\n",
            "leverages reasoning to search, interpret, and analyze massive amounts of text, images,\n",
            "and PDFs on the internet, pivoting as needed in reaction to information it encounters.\n",
            "\n",
            "The ability to synthesize knowledge is a prerequisite for creating new knowledge. For this\n",
            "reason, deep research marks a significant step toward our broader goal of developing\n",
            "AGI, which we have long envisioned as capable of producing novel scientific research.\n",
            "\n",
            "Why we built deep research\n",
            "\n",
            "Deep research is built for people who do intensive knowledge work in areas like finance,\n",
            "science, policy, and engineering and need thorough, precise, and reliable research. It can\n",
            "be equally useful for discerning shoppers looking for hyper-personalized\n",
            "recommendations on purchases that typically require careful research, like cars,\n",
            "appliances, and furniture. Every output is fully documented, with clear citations and a\n",
            "summary of its thinking, making it easy to reference and verify the information. It is\n",
            "particularly effective at finding niche, non-intuitive information that would require\n",
            "browsing numerous websites. Deep research frees up valuable time by allowing you to\n",
            "offload and expedite complex, time-intensive web research with just one query.\n",
            "\n",
            "Deep research independently discovers, reasons about, and consolidates insights from\n",
            "across the web. To accomplish this, it was trained on real-world tasks requiring browser\n",
            "\n",
            "3/15/25, 2:25 PM Introducing deep research | OpenAI\n",
            "\n",
            "https://openai.com/index/introducing-deep-research/ 2/38\n",
            "\n",
            "https://openai.com/\n",
            "\n",
            "\n",
            "and Python tool use, using the same reinforcement learning methods behind OpenAI o1,\n",
            "our first reasoning model. While o1 demonstrates impressive capabilities in coding, math,\n",
            "and other technical domains, many real-world challenges demand extensive context and\n",
            "information gathering from diverse online sources. Deep research builds on these\n",
            "reasoning capabilities to bridge that gap, allowing it to take on the types of problems\n",
            "people face in work and everyday life.\n",
            "\n",
            "How to use deep research\n",
            "\n",
            "In ChatGPT, select ‘deep research’ in the message composer and enter your query. Tell\n",
            "ChatGPT what you need—whether it’s a competitive analysis on streaming platforms or a\n",
            "personalized report on the best commuter bike. You can attach files or spreadsheets to\n",
            "add context to your question. Once it starts running, a sidebar appears with a summary of\n",
            "the steps taken and sources used.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did the court say about Elon's case?\"\n",
        "\n",
        "search_results = client.vector_stores.search(\n",
        "    vector_store_id=vector_store_details['id'],\n",
        "    query=query\n",
        ")"
      ],
      "metadata": {
        "id": "xw_qWqZyzMgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for result in search_results.data:\n",
        "    print(str(len(result.content[0].text)) + ' of character of content from ' + result.filename + ' with a relevant score of ' + str(result.score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYnMAdK3zqPD",
        "outputId": "58e44818-6aa4-4327-c3ca-f506297ea523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3065 of character of content from The court rejects Elon’s latest attempt to slow OpenAI down _ OpenAI.pdf with a relevant score of 0.9919459146628226\n",
            "2340 of character of content from The court rejects Elon’s latest attempt to slow OpenAI down _ OpenAI.pdf with a relevant score of 0.9354843499078509\n",
            "2624 of character of content from The court rejects Elon’s latest attempt to slow OpenAI down _ OpenAI.pdf with a relevant score of 0.8562886886419987\n",
            "2919 of character of content from OpenAI GPT-4.5 System Card _ OpenAI.pdf with a relevant score of 0.018677348264400928\n",
            "3183 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.004278511378690768\n",
            "2739 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.004035565093116385\n",
            "2673 of character of content from Introducing Operator _ OpenAI.pdf with a relevant score of 0.003124790134669153\n",
            "2601 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.00285750359112232\n",
            "3112 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.002674340123721771\n",
            "3376 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.0025426917750144273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integrating search results with LLM in a single API call"
      ],
      "metadata": {
        "id": "zPKaOip4z1cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's Deep Research?\"\n",
        "response = client.responses.create(\n",
        "    input= query,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    tools=[{\n",
        "        \"type\": \"file_search\",\n",
        "        \"vector_store_ids\": [vector_store_details['id']],\n",
        "    }]\n",
        ")\n"
      ],
      "metadata": {
        "id": "EDtzZDybztCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract annotations from the response\n",
        "annotations = response.output[1].content[0].annotations"
      ],
      "metadata": {
        "id": "7LmlrliN0WUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNMwxl2Q0dvZ",
        "outputId": "c5dc6f4c-5a95-40e8-e881-8c8f7aae58d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AnnotationFileCitation(file_id='file-F6LDdDbrP3ydpS12qYtNZM', index=512, type='file_citation', filename='Introducing deep research _ OpenAI.pdf'),\n",
              " AnnotationFileCitation(file_id='file-F6LDdDbrP3ydpS12qYtNZM', index=911, type='file_citation', filename='Introducing deep research _ OpenAI.pdf'),\n",
              " AnnotationFileCitation(file_id='file-F6LDdDbrP3ydpS12qYtNZM', index=1080, type='file_citation', filename='Introducing deep research _ OpenAI.pdf'),\n",
              " AnnotationFileCitation(file_id='file-F6LDdDbrP3ydpS12qYtNZM', index=1080, type='file_citation', filename='Introducing deep research _ OpenAI.pdf'),\n",
              " AnnotationFileCitation(file_id='file-F6LDdDbrP3ydpS12qYtNZM', index=1248, type='file_citation', filename='Introducing deep research _ OpenAI.pdf'),\n",
              " AnnotationFileCitation(file_id='file-F6LDdDbrP3ydpS12qYtNZM', index=1248, type='file_citation', filename='Introducing deep research _ OpenAI.pdf'),\n",
              " AnnotationFileCitation(file_id='file-F6LDdDbrP3ydpS12qYtNZM', index=1431, type='file_citation', filename='Introducing deep research _ OpenAI.pdf')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get top-k retrieved filenames\n",
        "retrieved_files = set([result.filename for result in annotations])\n",
        "\n",
        "print(f'Files used: {retrieved_files}')\n",
        "print('Response:')\n",
        "print(response.output[1].content[0].text) # 0 being the filesearch call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thggdGkZ0fU6",
        "outputId": "58e1d579-4917-432d-c471-c2353b4dd8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files used: {'Introducing deep research _ OpenAI.pdf'}\n",
            "Response:\n",
            "Deep Research is a newly introduced feature by OpenAI that enables users to conduct complex, multi-step research tasks on the internet through ChatGPT. It synthesizes information from various online sources to create comprehensive reports, effectively operating at the level of a research analyst. This capability is especially targeted toward professionals engaged in intensive knowledge work, such as those in finance, science, policy, and engineering, as well as consumers seeking personalized recommendations.\n",
            "\n",
            "The core functionalities of Deep Research include:\n",
            "\n",
            "1. **Multi-Step Reasoning**: It can find, analyze, and synthesize large amounts of data quickly, taking significantly less time than a human would require.\n",
            "2. **Documented Outputs**: Every report generated includes citations and a summary of the reasoning behind the conclusions, making it easy for users to verify and reference the information.\n",
            "3. **Customization and Context**: Users can attach files or spreadsheets to enhance the context of their queries, and it can access user-uploaded files during research.\n",
            "4. **Future Developments**: Plans include expanding access to more specialized data sources and enhancing output features like embedded images and data visualizations.\n",
            "\n",
            "Overall, Deep Research aims to facilitate thorough, reliable, and efficient research, marking a step toward more advanced AI capabilities in generating novel knowledge and insights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response_from_vectorsotre(query: str):\n",
        "  response = client.responses.create(\n",
        "      input= query,\n",
        "      model=\"gpt-4o-mini\",\n",
        "      tools=[{\n",
        "          \"type\": \"file_search\",\n",
        "          \"vector_store_ids\": [vector_store_details['id']],\n",
        "      }]\n",
        "  )\n",
        "\n",
        "  # Extract annotations from the response\n",
        "  annotations = response.output[1].content[0].annotations\n",
        "\n",
        "  # Get top-k retrieved filenames\n",
        "  retrieved_files = set([result.filename for result in annotations])\n",
        "\n",
        "  print(f'Files used: {retrieved_files}')\n",
        "  print('Response:')\n",
        "  print(response.output[1].content[0].text) # 0 being the filesearch call\n"
      ],
      "metadata": {
        "id": "s_ruOpsU0jQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What is Deep Researc and What was the court's verdict on Elon's case?\"\n",
        "get_response_from_vectorsotre(query=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq5FffUL07oC",
        "outputId": "477eee7e-b1fd-44b8-fb65-df998f3d4067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files used: {'The court rejects Elon’s latest attempt to slow OpenAI down _ OpenAI.pdf', 'Introducing deep research _ OpenAI.pdf'}\n",
            "Response:\n",
            "### Deep Research\n",
            "Deep Research is a capability launched by OpenAI that enables users to conduct multi-step research tasks by synthesizing large amounts of information from the web. It is designed to perform complex inquiries much faster than a human could, effectively acting like a research analyst. The system uses advanced reasoning to gather, analyze, and represent data in a comprehensive report, including citations for easy reference. It particularly excels in industries such as finance, science, and policy, providing detailed outputs that are fully documented.\n",
            "\n",
            "### Court's Verdict on Elon Musk's Case\n",
            "In the case involving Elon Musk and OpenAI, the court rejected Musk's request for a preliminary injunction, finding that he had not demonstrated a likelihood of success on the merits of his claims. Furthermore, the court dismissed several of Musk's claims entirely, marking a significant step toward resolving the lawsuit filed by him against OpenAI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating performance"
      ],
      "metadata": {
        "id": "VsgdrIQd1dvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating questions"
      ],
      "metadata": {
        "id": "rjWRuk_51hCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "def generate_questions(pdf_path):\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    prompt = (\n",
        "        \"Can you generate a question that can only be answered from this document?:\\n\"\n",
        "        f\"{text}\\n\\n\"\n",
        "    )\n",
        "\n",
        "    response = client.responses.create(\n",
        "        input=prompt,\n",
        "        model=\"gpt-4o\",\n",
        "    )\n",
        "\n",
        "    question = response.output[0].content[0].text\n",
        "\n",
        "    return question"
      ],
      "metadata": {
        "id": "ULZUH_MZ1Ors"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_questions(pdf_files[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hsU-b8Bj1keN",
        "outputId": "c95e7418-51bd-4c77-8aa9-40e327088b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What was the outcome of Elon Musk's request for a preliminary injunction against OpenAI, as mentioned in the document?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate questions for each PDF and store in a dictionary\n",
        "questions_dict = {}\n",
        "for pdf_path in pdf_files:\n",
        "    questions = generate_questions(pdf_path)\n",
        "    questions_dict[os.path.basename(pdf_path)] = questions"
      ],
      "metadata": {
        "id": "ho0PL28c1pUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ifmoLft1uLh",
        "outputId": "fa1c909e-8dd4-4da4-d994-ef0ea4152f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'The court rejects Elon’s latest attempt to slow OpenAI down _ OpenAI.pdf': \"What was the court's decision regarding Elon Musk's request for a preliminary injunction against OpenAI, as mentioned in the document?\",\n",
              " 'Introducing deep research _ OpenAI.pdf': \"What percentage accuracy did the model powering Deep Research achieve on Humanity's Last Exam, and how does this compare to other models mentioned?\",\n",
              " 'New tools for building agents _ OpenAI.pdf': 'What is the purpose of the new Responses API introduced by OpenAI, and how does it differ from the previous Chat Completions and Assistants APIs?',\n",
              " 'OpenAI GPT-4.5 System Card _ OpenAI.pdf': 'What is the preparedness score for cybersecurity mentioned in the GPT-4.5 system card?',\n",
              " 'Introducing Operator _ OpenAI.pdf': \"Certainly! Here's a question that can only be answered using the document provided:\\n\\nWhat is the name of the new model that powers the Operator and combines vision capabilities with advanced reasoning?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for filename, query in questions_dict.items():\n",
        "    rows.append({\"query\": query, \"_id\": filename.replace(\".pdf\", \"\")})\n",
        "\n",
        "# Metrics evaluation parameters\n",
        "k = 5\n",
        "total_queries = len(rows)\n",
        "correct_retrievals_at_k = 0\n",
        "reciprocal_ranks = []\n",
        "average_precisions = []\n",
        "\n",
        "def process_query(row):\n",
        "    query = row['query']\n",
        "    expected_filename = row['_id'] + '.pdf'\n",
        "    # Call file_search via Responses API\n",
        "    response = client.responses.create(\n",
        "        input=query,\n",
        "        model=\"gpt-4o-mini\",\n",
        "        tools=[{\n",
        "            \"type\": \"file_search\",\n",
        "            \"vector_store_ids\": [vector_store_details['id']],\n",
        "            \"max_num_results\": k,\n",
        "        }],\n",
        "        tool_choice=\"required\" # it will force the file_search, while not necessary, it's better to enforce it as this is what we're testing\n",
        "    )\n",
        "    # Extract annotations from the response\n",
        "    annotations = None\n",
        "    if hasattr(response.output[1], 'content') and response.output[1].content:\n",
        "        annotations = response.output[1].content[0].annotations\n",
        "    elif hasattr(response.output[1], 'annotations'):\n",
        "        annotations = response.output[1].annotations\n",
        "\n",
        "    if annotations is None:\n",
        "        print(f\"No annotations for query: {query}\")\n",
        "        return False, 0, 0\n",
        "\n",
        "    # Get top-k retrieved filenames\n",
        "    retrieved_files = [result.filename for result in annotations[:k]]\n",
        "    if expected_filename in retrieved_files:\n",
        "        rank = retrieved_files.index(expected_filename) + 1\n",
        "        rr = 1 / rank\n",
        "        correct = True\n",
        "    else:\n",
        "        rr = 0\n",
        "        correct = False\n",
        "\n",
        "    # Calculate Average Precision\n",
        "    precisions = []\n",
        "    num_relevant = 0\n",
        "    for i, fname in enumerate(retrieved_files):\n",
        "        if fname == expected_filename:\n",
        "            num_relevant += 1\n",
        "            precisions.append(num_relevant / (i + 1))\n",
        "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
        "\n",
        "    if expected_filename not in retrieved_files:\n",
        "        print(\"Expected file NOT found in the retrieved files!\")\n",
        "\n",
        "    if retrieved_files and retrieved_files[0] != expected_filename:\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Expected file: {expected_filename}\")\n",
        "        print(f\"First retrieved file: {retrieved_files[0]}\")\n",
        "        print(f\"Retrieved files: {retrieved_files}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "    return correct, rr, avg_precision"
      ],
      "metadata": {
        "id": "QYs8J8Ao1yA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_query(rows[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy53e2_B19_Z",
        "outputId": "28d9dc08-659e-4b48-f437-1cd1ffb151de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 1.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with ThreadPoolExecutor() as executor:\n",
        "    results = list(tqdm(executor.map(process_query, rows), total=total_queries))\n",
        "\n",
        "correct_retrievals_at_k = 0\n",
        "reciprocal_ranks = []\n",
        "average_precisions = []\n",
        "\n",
        "for correct, rr, avg_precision in results:\n",
        "    if correct:\n",
        "        correct_retrievals_at_k += 1\n",
        "    reciprocal_ranks.append(rr)\n",
        "    average_precisions.append(avg_precision)\n",
        "\n",
        "recall_at_k = correct_retrievals_at_k / total_queries\n",
        "precision_at_k = recall_at_k  # In this context, same as recall\n",
        "mrr = sum(reciprocal_ranks) / total_queries\n",
        "map_score = sum(average_precisions) / total_queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpCa5SQR2Aqj",
        "outputId": "150317a9-26f4-474f-d272-d1b2898c2823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:08<00:00,  1.64s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the metrics with k\n",
        "print(f\"Metrics at k={k}:\")\n",
        "print(f\"Recall@{k}: {recall_at_k:.4f}\")\n",
        "print(f\"Precision@{k}: {precision_at_k:.4f}\")\n",
        "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
        "print(f\"Mean Average Precision (MAP): {map_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc823FdH2FQ2",
        "outputId": "284e83b6-2930-42d0-a490-3c8ae0efcdc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics at k=5:\n",
            "Recall@5: 1.0000\n",
            "Precision@5: 1.0000\n",
            "Mean Reciprocal Rank (MRR): 1.0000\n",
            "Mean Average Precision (MAP): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MK8abBns2Jdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}