{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IIL9QJl6fE9n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### senta : issue pythonversion"
      ],
      "metadata": {
        "id": "U6x1XoAqfLKK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vYxLpcxeZmZn"
      },
      "outputs": [],
      "source": [
        "# https://github.com/baidu/Senta/tree/master"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version\n",
        "!sudo update-alternatives --config python3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMHbNto0bMSz",
        "outputId": "bb5ff92d-2295-4104-9679-4db5414cb60a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "update-alternatives: error: no alternatives for python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/baidu/Senta.git\n",
        "# change version type of sentencepience to 0.1.95 in setup.py\n",
        "!python -m pip install Senta/."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlfw4KM0ZpkX",
        "outputId": "4ef52cde-9455-44a8-9dee-ed2799cefc8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./Senta\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nltk==3.4.5 (from Senta==2.0.0)\n",
            "  Using cached nltk-3.4.5.zip (1.5 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.14.5 (from Senta==2.0.0)\n",
            "  Using cached numpy-1.14.5.zip (4.9 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting six==1.11.0 (from Senta==2.0.0)\n",
            "  Using cached six-1.11.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting scikit-learn==0.20.4 (from Senta==2.0.0)\n",
            "  Using cached scikit-learn-0.20.4.tar.gz (11.7 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece==0.1.95 (from Senta==2.0.0)\n",
            "  Downloading sentencepiece-0.1.95.tar.gz (508 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.20.4->Senta==2.0.0) (1.13.1)\n",
            "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scipy>=0.13.3 (from scikit-learn==0.20.4->Senta==2.0.0)\n",
            "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "  Downloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "  Downloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "  Downloading scipy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (59 kB)\n",
            "INFO: pip is still looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (59 kB)\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "  Downloading scipy-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading scipy-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "  Downloading scipy-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "  Downloading scipy-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "  Downloading scipy-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "  Downloading scipy-1.6.1.tar.gz (27.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/27.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading scipy-1.6.0.tar.gz (27.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/27.3 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading scipy-1.5.4.tar.gz (25.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.2/25.2 MB\u001b[0m \u001b[31m130.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from senta import Senta\n",
        "\n",
        "my_senta = Senta()\n",
        "\n",
        "# 获取目前支持的情感预训练模型, 我们开放了以ERNIE 1.0 large(中文)、ERNIE 2.0 large(英文)和RoBERTa large(英文)作为初始化的SKEP模型\n",
        "print(my_senta.get_support_model()) # [\"ernie_1.0_skep_large_ch\", \"ernie_2.0_skep_large_en\", \"roberta_skep_large_en\"]\n"
      ],
      "metadata": {
        "id": "XM65fxHGZy1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vIPYBI36fRyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Erlangshen-Roberta-110 : 500mb\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GVRMHV4ZfSz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "ORDmldLwjtR3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text='今天心情不好'\n",
        "\n",
        "# output=model(torch.tensor([tokenizer.encode(text)]))\n",
        "# print(torch.nn.functional.softmax(output.logits,dim=-1))\n"
      ],
      "metadata": {
        "id": "cxpTust9fWMw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text={\"嗨，你好吗\" : \"How you doing\",\n",
        "      \"今天，我的浏览器崩溃了好几次，真是让人很烦人。\" :  \"Today, my browser crashed several times, it's really annoying.\",\n",
        "\"我不喜欢 Brave 浏览器的字体太小 now.\" :  \"I don't like the font size in the Brave browser.\",\n",
        "\"有时候， Brave 浏览器很慢，需要优化。\" :  \"Sometimes, the Brave browser is slow and needs optimization.\",\n",
        "\"我的网速很快，但是 Brave 浏览器加载页面很慢。\" :  \"My internet speed is fast, but the Brave browser loads pages slowly.\",\n",
        "\"我不确定 Brave 浏览器是否安全，需要更多的信息。\" :  \"I'm not sure if the Brave browser is safe, I need more information.\",\n",
        "\"我喜欢 Brave 浏览器的私人 browsing 功能。\" :  \"I like the private browsing feature of the Brave browser.\",\n",
        "\"Brave 浏览器的设计非常好，但功能有点 complexity.\" :  \"The design of the Brave browser is great, but the features are a bit complex.\",\n",
        "\"我希望 Brave 浏览器可以有更多的语言支持。\" : \"I hope the Brave browser can support more languages.\",\n",
        "\"我不能在 Brave 浏览器中找到某些网站，这很 frustrating.\" :  \"I can't find some websites in the Brave browser, it's frustrating.\",\n",
        "\"Brave 浏览器的广告过多，希望可以减少。\" :  \"There are too many ads in the Brave browser, I hope they can reduce.\",\n",
        "\"我不确定 Brave 浏览器的 cookie 政策，需要 clarification.\" :  \"I need clarification on the cookie policy of the Brave browser.\",\n",
        "\"Brave 浏览器的下载速度很慢，需要 improve.\" :  \"The download speed in the Brave browser is slow, it needs to be improved.\",\n",
        "\"我喜欢 Brave 浏览器的快速搜索功能。\" :  \"I like the quick search feature of the Brave browser.\",\n",
        "\"Brave 浏览器的翻译功能不够 accurate，需要 improve.\" :  \"The translation function in the Brave browser is not accurate enough, it needs to be improved.\",\n",
        "}\n",
        "def get_output(sentences : list[str] , loaded_tokeinser, loaded_model) -> None:\n",
        "  neg_threshold = 0.3\n",
        "  for n,(i,j) in enumerate(sentences.items()):\n",
        "    print(\"{}\".format(n) , end=\" : \")\n",
        "    output=model(torch.tensor([tokenizer.encode(i)]))\n",
        "    a, b = torch.nn.functional.softmax(output.logits,dim=-1).tolist()[0]\n",
        "    print(\"{:.2f}, {:.2f}\".format(a,b) , end = \" | \")\n",
        "    print(\"Complaint | {}\".format(j)) if a > neg_threshold else print(\"Not| {}\".format(j))"
      ],
      "metadata": {
        "id": "HopCS2eDfYQs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment')\n",
        "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment')"
      ],
      "metadata": {
        "id": "qbHkiq6bjzuc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "get_output(text, tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLwg7zJyfmC7",
        "outputId": "33d56e8c-98fd-47df-c39b-a2c54d840600"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : 0.10, 0.90 | Not| How you doing\n",
            "1 : 0.99, 0.01 | Complaint | Today, my browser crashed several times, it's really annoying.\n",
            "2 : 0.94, 0.06 | Complaint | I don't like the font size in the Brave browser.\n",
            "3 : 0.24, 0.76 | Not| Sometimes, the Brave browser is slow and needs optimization.\n",
            "4 : 0.63, 0.37 | Complaint | My internet speed is fast, but the Brave browser loads pages slowly.\n",
            "5 : 0.71, 0.29 | Complaint | I'm not sure if the Brave browser is safe, I need more information.\n",
            "6 : 0.03, 0.97 | Not| I like the private browsing feature of the Brave browser.\n",
            "7 : 0.18, 0.82 | Not| The design of the Brave browser is great, but the features are a bit complex.\n",
            "8 : 0.08, 0.92 | Not| I hope the Brave browser can support more languages.\n",
            "9 : 0.91, 0.09 | Complaint | I can't find some websites in the Brave browser, it's frustrating.\n",
            "10 : 0.20, 0.80 | Not| There are too many ads in the Brave browser, I hope they can reduce.\n",
            "11 : 0.39, 0.61 | Complaint | I need clarification on the cookie policy of the Brave browser.\n",
            "12 : 0.86, 0.14 | Complaint | The download speed in the Brave browser is slow, it needs to be improved.\n",
            "13 : 0.02, 0.98 | Not| I like the quick search feature of the Brave browser.\n",
            "14 : 0.37, 0.63 | Complaint | The translation function in the Brave browser is not accurate enough, it needs to be improved.\n",
            "CPU times: user 3.35 s, sys: 78.7 ms, total: 3.43 s\n",
            "Wall time: 3.68 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGgTvxkHhJ0O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Erlangshen-Roberta-330 : 1.3GB"
      ],
      "metadata": {
        "id": "naN4nF8MjRPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-330M-Sentiment')\n",
        "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-330M-Sentiment')"
      ],
      "metadata": {
        "id": "X2rzfc-xjVW2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "get_output(text, tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBcyVzaNjZ9u",
        "outputId": "3b0317c4-df30-4af7-ef67-4b935efd3d76"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : 0.00, 1.00 | Not| How you doing\n",
            "1 : 0.99, 0.01 | Complaint | Today, my browser crashed several times, it's really annoying.\n",
            "2 : 1.00, 0.00 | Complaint | I don't like the font size in the Brave browser.\n",
            "3 : 1.00, 0.00 | Complaint | Sometimes, the Brave browser is slow and needs optimization.\n",
            "4 : 1.00, 0.00 | Complaint | My internet speed is fast, but the Brave browser loads pages slowly.\n",
            "5 : 1.00, 0.00 | Complaint | I'm not sure if the Brave browser is safe, I need more information.\n",
            "6 : 0.00, 1.00 | Not| I like the private browsing feature of the Brave browser.\n",
            "7 : 1.00, 0.00 | Complaint | The design of the Brave browser is great, but the features are a bit complex.\n",
            "8 : 0.92, 0.08 | Complaint | I hope the Brave browser can support more languages.\n",
            "9 : 1.00, 0.00 | Complaint | I can't find some websites in the Brave browser, it's frustrating.\n",
            "10 : 1.00, 0.00 | Complaint | There are too many ads in the Brave browser, I hope they can reduce.\n",
            "11 : 1.00, 0.00 | Complaint | I need clarification on the cookie policy of the Brave browser.\n",
            "12 : 1.00, 0.00 | Complaint | The download speed in the Brave browser is slow, it needs to be improved.\n",
            "13 : 0.06, 0.94 | Not| I like the quick search feature of the Brave browser.\n",
            "14 : 1.00, 0.00 | Complaint | The translation function in the Brave browser is not accurate enough, it needs to be improved.\n",
            "CPU times: user 7.56 s, sys: 220 ms, total: 7.78 s\n",
            "Wall time: 7.94 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KzvGCpluj4Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing with extrernal data"
      ],
      "metadata": {
        "id": "sHzRDU6ukgTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/zxjwudi/One-of-a-kind-Complaint-Handling-Dataset/blob/main/dataset_customer_complaint.xlsx\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "1AoMInPqkixd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"dataset_customer_complaint.xlsx\", usecols=\"I\")\n",
        "df.columns = [\"complaints\"]\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMsgDlK3knt9",
        "outputId": "89202502-f34d-46e8-823b-840e17ddefa1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1070, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output(sentences : list[str] , loaded_tokeinser, loaded_model) -> list[int]:\n",
        "  neg_threshold = 0.3\n",
        "  result = []\n",
        "  from tqdm import tqdm\n",
        "  for i in tqdm(sentences):\n",
        "    # print(\"{}\".format(n) , end=\" : \")\n",
        "    output = model(torch.tensor([tokenizer.encode(i)]))\n",
        "    a, b = torch.nn.functional.softmax(output.logits,dim=-1).tolist()[0]\n",
        "    # print(\"{:.2f}, {:.2f}\".format(a,b) , end = \" | \")\n",
        "    # print(\"Complaint | {}\".format(j)) if a > neg_threshold else print(\"Not| {}\".format(j))\n",
        "    result.append(1 if a> neg_threshold else 0)\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "jkxguBL-lbCd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = get_output(df.complaints.values, tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV8tsZLXkwoM",
        "outputId": "3dfb3c9e-7325-4ce7-eaba-e9b5085871b9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1070/1070 [14:29<00:00,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy : 100.0\n",
            "CPU times: user 13min 37s, sys: 3 s, total: 13min 40s\n",
            "Wall time: 14min 29s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"accuracy : {}\".format(len(result)/df.shape[0]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-8KvMAKoWch",
        "outputId": "5b1d2617-675d-462f-8be7-a6c866dd1dd6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy : 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# modelscope"
      ],
      "metadata": {
        "id": "2H4NEMWMoXaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install addict datasets oss2 modelscope  simplejson"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqWr7lOtqaNs",
        "outputId": "d5c1174c-d31d-43a4-aca4-a3c0d8c259ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: oss2 in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: modelscope in /usr/local/lib/python3.10/dist-packages (1.19.2)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.10/dist-packages (from oss2) (1.7)\n",
            "Requirement already satisfied: pycryptodome>=3.4.7 in /usr/local/lib/python3.10/dist-packages (from oss2) (3.21.0)\n",
            "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from oss2) (2.16.5)\n",
            "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /usr/local/lib/python3.10/dist-packages (from oss2) (2.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from oss2) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from modelscope) (2.2.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2) (0.10.0)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2) (43.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2) (2.22)\n",
            "Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "Installing collected packages: simplejson\n",
            "Successfully installed simplejson-3.19.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# comment off offlinemodel emableed in 3 places if error\n",
        "from modelscope.pipelines import pipeline\n",
        "from modelscope.utils.constant import Tasks"
      ],
      "metadata": {
        "id": "jEII66bglUyM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "KZaeID2Tpn3I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nlp_structbert_sentiment-classification_chinese-base : 400mb"
      ],
      "metadata": {
        "id": "2X8mJvYzpp-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_cls = pipeline(Tasks.text_classification, 'damo/nlp_structbert_sentiment-classification_chinese-base')\n",
        "semantic_cls(input='启动的时候很大声音，然后就会听到1.2秒的卡察的声音，类似齿轮摩擦的声音')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puyytVFmpnNV",
        "outputId": "6fd92bbc-2abd-433b-b666-9fd6d628f224"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-10-26 07:43:20,391 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
            "2024-10-26 07:43:20,890 - modelscope - INFO - initiate model from /root/.cache/modelscope/hub/damo/nlp_structbert_sentiment-classification_chinese-base\n",
            "2024-10-26 07:43:20,899 - modelscope - INFO - initiate model from location /root/.cache/modelscope/hub/damo/nlp_structbert_sentiment-classification_chinese-base.\n",
            "2024-10-26 07:43:20,913 - modelscope - INFO - initialize model from /root/.cache/modelscope/hub/damo/nlp_structbert_sentiment-classification_chinese-base\n",
            "2024-10-26 07:43:31,630 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
            "/usr/local/lib/python3.10/dist-packages/modelscope/utils/checkpoint.py:550: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(ckpt_file, map_location='cpu')\n",
            "2024-10-26 07:43:36,477 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTextClassification.\n",
            "\n",
            "2024-10-26 07:43:36,481 - modelscope - INFO - All the weights of ModelForTextClassification were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTextClassification for predictions without further training.\n",
            "2024-10-26 07:43:36,551 - modelscope - INFO - The key of sentence1: sentence1, The key of sentence2: None, The key of label: label\n",
            "2024-10-26 07:43:36,593 - modelscope - INFO - cuda is not available, using cpu instead.\n",
            "2024-10-26 07:43:36,600 - modelscope - INFO - The key of sentence1: text, The key of sentence2: None, The key of label: label\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1126: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'scores': [0.9281617403030396, 0.07183829694986343], 'labels': ['负面', '正面']}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output(sentences : list[str] , classifier) -> list[int]:\n",
        "  from tqdm import tqdm\n",
        "  result = [classifier(input = i) for i in tqdm(sentences) ]\n",
        "  return result"
      ],
      "metadata": {
        "id": "vkCV7dC-o2JR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = get_output(df.complaints.values, semantic_cls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57NqaqHUqEkD",
        "outputId": "dab7c93f-37d6-4c66-f463-4ec02b03d1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 11/1070 [00:29<55:25,  3.14s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"accuracy : {}\".format(len(result)/df.shape[0]*100))"
      ],
      "metadata": {
        "id": "vepUIzRlyLRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QvhwmVz6ySHj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}