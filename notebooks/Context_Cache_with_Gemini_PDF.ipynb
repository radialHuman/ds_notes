{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMIwp7AoOAdw"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n"
      ],
      "metadata": {
        "id": "QF-EXpcWOP33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o gemini.pdf https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAcmvM68nuV6",
        "outputId": "a493324f-3913-42ec-9419-bf68a47e3b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 7059k  100 7059k    0     0  11.1M      0 --:--:-- --:--:-- --:--:-- 11.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o base_model.pdf https://arxiv.org/pdf/2312.01552"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scLmhZHynuY_",
        "outputId": "0dc097b3-8cbf-4f42-c66d-8e66968824da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 3656k  100 3656k    0     0  6819k      0 --:--:-- --:--:-- --:--:-- 6833k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sw2lChGLnucb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the file and print a confirmation\n",
        "sample_file = genai.upload_file(path=\"gemini.pdf\",\n",
        "                                display_name=\"Gemini 1.5 PDF\")\n",
        "\n",
        "print(f\"Uploaded file '{sample_file.display_name}' as: {sample_file.uri}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "c4JNhugFnufQ",
        "outputId": "90cf3e09-c85d-4891-a8cf-cba30ec7fb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file 'Gemini 1.5 PDF' as: https://generativelanguage.googleapis.com/v1beta/files/bvdtt8stukk5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify PDF file upload and get metadata\n",
        "You can verify the API successfully stored the uploaded file and get its metadata by calling files.get through the SDK. Only the name (and by extension, the uri) are unique. Use display_name to identify files only if you manage uniqueness yourself."
      ],
      "metadata": {
        "id": "Awjq1IsxotGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = genai.get_file(name=sample_file.name)\n",
        "print(f\"Retrieved file '{file.display_name}' as: {sample_file.uri}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "JI-1W0Vsnuie",
        "outputId": "d018cf41-e55a-4c28-94d8-0f19add86830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved file 'Gemini 1.5 PDF' as: https://generativelanguage.googleapis.com/v1beta/files/bvdtt8stukk5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt the Gemini API with the uploaded documents\n",
        "After uploading the file, you can make GenerateContent requests that reference the File API URI. Select the generative model and provide it with a text prompt and the uploaded document:"
      ],
      "metadata": {
        "id": "m5EmsR30pBO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a Gemini model.\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-pro\")\n",
        "\n",
        "# Prompt the model with text and the previously uploaded image.\n",
        "response = model.generate_content([sample_file, \"Can you summarize this document as a bulleted list?\"])\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "p-HOgbkmo0Mg",
        "outputId": "bd0dba12-3910-4ca1-ecff-3d4aaebb7be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a summary of the document in a bulleted list:\n",
            "\n",
            "* **Introduction:** The document introduces Gemini 1.5 Pro, a new multimodal language model from Google DeepMind. It's the first of its kind capable of recalling and reasoning over 10 million tokens of context, including text, video, and audio. \n",
            "* **Key advancements:**\n",
            "    * **Novel mixture-of-experts architecture:** Improves efficiency, reasoning, and long-context performance.\n",
            "    * **Unprecedented context length:** Processes entire documents, hours of video, and days of audio.\n",
            "    * **Multimodal capabilities:**  Processes and retrieves information across various modalities.\n",
            "* **Long-context evaluation:**\n",
            "    * **Qualitative examples:** Demonstrates capabilities like understanding long codebases, learning a new language from limited context, and answering questions about a full-length movie.\n",
            "    * **Quantitative evaluations:**\n",
            "        * **Diagnostic tests:** Shows near-perfect recall in needle-in-a-haystack tasks for text, video, and audio up to millions of tokens. Also demonstrates improved perplexity over long sequences.\n",
            "        * **Realistic tasks:** Outperforms existing models on long-document QA, long-video QA, and long-context ASR, even when competitors use external retrieval methods.\n",
            "* **Core capability evaluations:**\n",
            "    *  Maintains high performance on tasks like math, science, reasoning, coding, multilinguality, and instruction following, despite its focus on long-context understanding.\n",
            "    *  Outperforms Gemini 1.0 Pro across the board and performs at a comparable level to Gemini 1.0 Ultra, while being more computationally efficient.\n",
            "* **Responsible deployment:**\n",
            "    *  Impact assessment conducted to identify potential societal benefits and risks.\n",
            "    *  Model mitigations implemented to address content safety, representational harms, and memorization.\n",
            "    *  Ongoing safety evaluations are conducted across various modality areas.\n",
            "* **Discussion:** \n",
            "    * Highlights the importance of developing new evaluation methodologies for long-context models, as existing benchmarks are insufficiently challenging. \n",
            "    * Calls for research on more complex reasoning tasks and improved automatic metrics.\n",
            "* **Appendix:**  Includes detailed information on the model architecture, training data, evaluation procedures, and prompt engineering, as well as additional results and examples.\n",
            "\n",
            "**Overall, the document highlights the significant advancements of Gemini 1.5 Pro in long-context understanding, its strong performance on core capabilities, and Google's commitment to responsible AI development.** \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.usage_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iGUMiRro0PX",
        "outputId": "214e98ad-a44b-4ac7-9e7e-931635931868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 78595\n",
            "candidates_token_count: 520\n",
            "total_token_count: 79115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt the model with text and the previously uploaded image.\n",
        "response = model.generate_content([sample_file, \"Can you explain Figure 9 in the paper?\"])\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "1PBy_Ubxo0Sl",
        "outputId": "b667be16-c1c6-4c0c-b420-8b0cc35be62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Figure 9 in the paper illustrates the results of an experiment designed to test the ability of the large language model Gemini 1.5 Pro to understand very long audio sequences. The experiment uses a \"needle-in-a-haystack\" approach, where a short audio clip (the \"needle\") containing a secret keyword is hidden within a much larger audio file (the \"haystack\"). \n",
            "\n",
            "Here's a breakdown of the figure:\n",
            "\n",
            "* **The Task:** The model is presented with an audio file that can be up to 107 hours long (almost 5 days). This audio is constructed by concatenating many shorter audio clips. Hidden somewhere within this long audio is a very short clip where a speaker says \"the secret keyword is needle\". The model is then asked to identify the secret keyword, using a text-based question, meaning it has to perform cross-modal reasoning (audio to text).\n",
            "* **Comparison:** The figure compares the performance of Gemini 1.5 Pro with a combination of two other models: Whisper and GPT-4 Turbo. Whisper is a speech recognition model that transcribes audio into text. GPT-4 Turbo is a text-based language model. Since these models can't handle such long audio inputs natively, they are used in a pipeline: the audio is first broken into 30-second segments, transcribed by Whisper, and then the concatenated text is fed to GPT-4 Turbo to find the keyword.\n",
            "* **The Grids:** The figure contains two main grids, one for each model (or model combination). The x-axis of each grid represents the length of the audio haystack, ranging from 12 minutes to 11 hours for the smaller grids on the left and then extending to 107 hours for the larger grid on the right. The y-axis represents the depth at which the needle is inserted, meaning its relative position within the audio.  \n",
            "* **Color Coding:** The cells in the grids are color-coded:\n",
            "    * **Green:** The model successfully identified the secret keyword.\n",
            "    * **Red:** The model failed to identify the keyword. \n",
            "* **Results:** The figure shows that Gemini 1.5 Pro achieves 100% accuracy on this task, finding the needle in all instances. In contrast, the Whisper + GPT-4 Turbo combination achieves around 94.5% accuracy. This demonstrates the superiority of Gemini 1.5 Pro's long-context audio understanding capabilities compared to existing approaches that rely on breaking down the audio into smaller chunks.\n",
            "\n",
            "**In essence, Figure 9 highlights Gemini 1.5 Pro's ability to process and understand very long audio sequences, opening up possibilities for new applications in areas like audio analysis, transcription, and retrieval.** \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.usage_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49LyhHo1o0VV",
        "outputId": "3861d4cb-1a15-4844-f56f-9cac9029fb1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 78594\n",
            "candidates_token_count: 573\n",
            "total_token_count: 79167\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gHHeANexsbCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt the model with text and the previously uploaded image.\n",
        "response = model.generate_content([sample_file, \"Can you describe the scene in Figure 15 in details? How many people do you see in the image? and what is the caption of the image\"])\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "i9vNYy9XqGFD",
        "outputId": "6b86143e-efb7-4c63-b294-b78294e90de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The scene in Figure 15 appears to be a professional Go match. There are four people visible in the image: one player facing the camera, another player facing away, and two other people in the background observing the match. The caption overlaid on the image reads: \"The secret word is 'needle'\". \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textwrap.wrap(response.text, width=80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11bQKnspsoll",
        "outputId": "66d17f57-7fe6-4822-e696-a9b03e045924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The scene in Figure 15 appears to be a professional Go match. There are four',\n",
              " 'people visible in the image: one player facing the camera, another player facing',\n",
              " 'away, and two other people in the background observing the match. The caption',\n",
              " 'overlaid on the image reads: \"The secret word is \\'needle\\'\".']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with Multiple Files"
      ],
      "metadata": {
        "id": "-u2KtAbOt_24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the file and print a confirmation\n",
        "base_model_file = genai.upload_file(path=\"base_model.pdf\",\n",
        "                                display_name=\"Base Model PDF\")\n",
        "\n",
        "print(f\"Uploaded file '{base_model_file.display_name}' as: {base_model_file.uri}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "7FjtdemZqGIK",
        "outputId": "5430e3fb-6b6b-4bd7-de8f-a7267768225f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file 'Base Model PDF' as: https://generativelanguage.googleapis.com/v1beta/files/r4326pzox1w4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = genai.get_file(name=base_model_file.name)\n",
        "print(f\"Retrieved file '{file.display_name}' as: {file.uri}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "nCT3ez9iqGLk",
        "outputId": "676b8874-4473-44e3-adc0-fc8a88c5c295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved file 'Base Model PDF' as: https://generativelanguage.googleapis.com/v1beta/files/r4326pzox1w4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a Gemini model.\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "\n",
        "prompt = \"Summarize the differences between the thesis statements for these documents.\"\n",
        "\n",
        "response = model.generate_content([prompt, sample_file, base_model_file,])\n",
        "\n"
      ],
      "metadata": {
        "id": "cKXSEFGmqGOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textwrap.wrap(response.text, width=120)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1YdwfIDqGRi",
        "outputId": "9c0993b9-df70-4ae6-9126-1e268d8736ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The thesis statement of the Gemini 1.5 Pro paper is that the new model surpasses previous models in its ability to',\n",
              " 'process extremely long context while maintaining the core capabilities of the model. The thesis statement of the LIMA',\n",
              " 'paper is that alignment tuning is superficial and that base LLMs have already acquired the knowledge required for',\n",
              " 'answering user queries. The thesis statement of the URIAL paper is that base LLMs can be effectively aligned without SFT',\n",
              " 'or RLHF by using a simple, tuning-free alignment method that leverages in-context learning.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Do you think the URIAL approach has validity? Can you give me counter arguments?\"\n",
        "response = model.generate_content([prompt, sample_file, base_model_file,])"
      ],
      "metadata": {
        "id": "rBUPu9n_qGXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textwrap.wrap(response.text, width=120)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgUbeBOqo0YQ",
        "outputId": "2ab09cca-4da8-44d0-fc5c-688f04c86826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The URIAL approach, as described in the paper, has validity in that it demonstrates that in-context learning can be',\n",
              " 'effective in aligning base LLMs without the need for supervised fine-tuning or reinforcement learning.   However, here',\n",
              " 'are some counter arguments:  * **Generalizability:** The study is limited to a specific dataset of instructions and base',\n",
              " 'LLMs. It is unclear whether these findings will generalize to other datasets and LLM architectures.  * **Task',\n",
              " \"Specificity:** URIAL's performance may vary depending on the complexity of the task. It may be less effective for tasks\",\n",
              " 'that require more complex reasoning or factual knowledge.  * **Contextual Limitations:** The effectiveness of URIAL',\n",
              " 'relies on careful selection of in-context examples, which can be time-consuming and requires human effort.  * **Safety',\n",
              " 'and Alignment:** While URIAL achieves some level of alignment in terms of style and engagement, it may not be sufficient',\n",
              " 'to address all safety and alignment concerns, particularly in sensitive domains. * **Real-world Applications:** The',\n",
              " 'paper focuses on research and evaluation settings. It is unclear how URIAL would perform in real-world scenarios with',\n",
              " 'diverse and unpredictable user interactions.   Overall, URIAL offers a promising approach to aligning base LLMs without',\n",
              " 'tuning, but further research is needed to assess its limitations and potential for real-world applications.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxjK-eqjx89S",
        "outputId": "4cb1e48c-9d55-4eb7-ee22-9eaee116fb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The URIAL approach, as described in the paper, has validity in that it demonstrates that in-context learning can be effective in aligning base LLMs without the need for supervised fine-tuning or reinforcement learning. \n",
            "\n",
            "However, here are some counter arguments:\n",
            "\n",
            "* **Generalizability:** The study is limited to a specific dataset of instructions and base LLMs. It is unclear whether these findings will generalize to other datasets and LLM architectures. \n",
            "* **Task Specificity:** URIAL's performance may vary depending on the complexity of the task. It may be less effective for tasks that require more complex reasoning or factual knowledge. \n",
            "* **Contextual Limitations:** The effectiveness of URIAL relies on careful selection of in-context examples, which can be time-consuming and requires human effort. \n",
            "* **Safety and Alignment:** While URIAL achieves some level of alignment in terms of style and engagement, it may not be sufficient to address all safety and alignment concerns, particularly in sensitive domains.\n",
            "* **Real-world Applications:** The paper focuses on research and evaluation settings. It is unclear how URIAL would perform in real-world scenarios with diverse and unpredictable user interactions. \n",
            "\n",
            "Overall, URIAL offers a promising approach to aligning base LLMs without tuning, but further research is needed to assess its limitations and potential for real-world applications. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List files\n",
        "You can list all files uploaded using the File API and their URIs using files.list_files():"
      ],
      "metadata": {
        "id": "U425k52QyijT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files\n",
        "for file in genai.list_files():\n",
        "    print(f\"{file.display_name}, URI: {file.uri}\")"
      ],
      "metadata": {
        "id": "8-XTWoEdPTeX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "3382887e-a4b5-4aaa-c701-134d1bb32689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Model PDF, URI: https://generativelanguage.googleapis.com/v1beta/files/r4326pzox1w4\n",
            "Gemini 1.5 PDF, URI: https://generativelanguage.googleapis.com/v1beta/files/bvdtt8stukk5\n",
            "Base Model PDF, URI: https://generativelanguage.googleapis.com/v1beta/files/fpygzv9hp2c2\n",
            "Base Model PDF, URI: https://generativelanguage.googleapis.com/v1beta/files/5j2rhoqxz7th\n",
            "Base Model PDF, URI: https://generativelanguage.googleapis.com/v1beta/files/xsn3l96pmhoq\n",
            "Gemini 1.5 PDF, URI: https://generativelanguage.googleapis.com/v1beta/files/bdk8holpqvfz\n",
            "Base Model PDF, URI: https://generativelanguage.googleapis.com/v1beta/files/20hkjq8z6ita\n",
            "Gemini 1.5 PDF, URI: https://generativelanguage.googleapis.com/v1beta/files/38k5wg9heefa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Context Cache"
      ],
      "metadata": {
        "id": "YCpZJS6AH--B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.generativeai import caching\n",
        "import datetime\n",
        "import time"
      ],
      "metadata": {
        "id": "Ym5nDUNFJPbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a cache with a 5 minute TTL\n",
        "cache = caching.CachedContent.create(\n",
        "    model='models/gemini-1.5-flash-001',\n",
        "    display_name='PDF-file', # used to identify the cache\n",
        "    system_instruction=(\n",
        "        'You are an expert PDF file analyzer, and your job is to answer '\n",
        "        'the user\\'s query based on the PDF file you have access to.'\n",
        "    ),\n",
        "    contents=[sample_file,],\n",
        "    ttl=datetime.timedelta(minutes=15),\n",
        ")\n"
      ],
      "metadata": {
        "id": "W1Qv5RqGJDqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a GenerativeModel which uses the created cache.\n",
        "model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n",
        "\n",
        "# Query the model\n",
        "response = model.generate_content([(\n",
        "    'What is the title of the paper?'\n",
        "    'Who are the authors? '\n",
        "    'What are the major contributions of the paper accordig to the authors?'\n",
        ")])\n",
        "\n",
        "print(response.usage_metadata)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "5Mfw0joBJQ7L",
        "outputId": "ea943e8e-c346-4df0-b5b5-015e2b97a609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 77914\n",
            "candidates_token_count: 278\n",
            "total_token_count: 78192\n",
            "cached_content_token_count: 77886\n",
            "\n",
            "The title of the paper is \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\".\n",
            "\n",
            "The authors are the Gemini Team at Google. \n",
            "\n",
            "The major contributions of the paper are:\n",
            "- The authors introduce Gemini 1.5 Pro, a new, highly compute-efficient multimodal mixture-of-experts model that can recall and reason over fine-grained information from millions of tokens of context. \n",
            "- They show that Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks.\n",
            "- They show that Gemini 1.5 Pro is able to handle extremely long contexts; it has the ability to recall and reason over fine-grained information from up to at least 10M tokens. This scale is unprecedented among contemporary large language models (LLMs).\n",
            "- They demonstrate the in-context learning capabilities of Gemini 1.5 Pro by showing that the model can learn to translate English to Kalamang, a language with fewer than 200 speakers worldwide, at a similar level to a person who learned from the same content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a GenerativeModel which uses the created cache.\n",
        "model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n",
        "\n",
        "# Query the model\n",
        "response = model.generate_content([(\n",
        "    'What is the title of the paper?'\n",
        "    'Who are the authors? provide a list '\n",
        "    'What are the major contributions of the paper accordig to the authors?'\n",
        ")])\n",
        "\n",
        "print(response.usage_metadata)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "5xVp4b6kMhXu",
        "outputId": "cde25d5b-5f1d-4ea6-d534-a01aa1d7c0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 77917\n",
            "candidates_token_count: 310\n",
            "total_token_count: 78227\n",
            "cached_content_token_count: 77886\n",
            "\n",
            "The title of the paper is \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\". The authors of the paper are the \"Gemini Team, Google\". \n",
            "\n",
            "The major contributions of the paper, according to the authors, are:\n",
            "\n",
            "* **Gemini 1.5 Pro, a new multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context**, including multiple long documents and hours of video and audio.\n",
            "* **Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR**, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks.\n",
            "* **Gemini 1.5 Pro can handle extremely long contexts**, up to at least 10M tokens, which is unprecedented among contemporary large language models (LLMs).\n",
            "* **Gemini 1.5 Pro surpasses Gemini 1.0 Pro**, and performs at a similar level to 1.0 Ultra on a wide array of benchmarks while requiring significantly less compute to train.\n",
            "* **Gemini 1.5 Pro has surprising new capabilities at the frontier**, including the ability to learn to translate English to Kalamang, a language with fewer than 200 speakers worldwide, at a similar level to a person who learned from the same content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for c in caching.CachedContent.list():\n",
        "  print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "q5Ec4WToKqGL",
        "outputId": "19414001-2362-40f7-fa74-7b1014b782a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CachedContent(\n",
            "    name='cachedContents/pbfam6rwcdl5',\n",
            "    model='models/gemini-1.5-flash-001',\n",
            "    display_name='PDF-file',\n",
            "    usage_metadata={\n",
            "        'total_token_count': 77886,\n",
            "    },\n",
            "    create_time=2024-08-21 02:42:15.266216+00:00,\n",
            "    update_time=2024-08-21 02:42:15.266216+00:00,\n",
            "    expire_time=2024-08-21 02:57:13.884981+00:00\n",
            ")\n",
            "CachedContent(\n",
            "    name='cachedContents/l01zlu22q67z',\n",
            "    model='models/gemini-1.5-flash-001',\n",
            "    display_name='PDF-file',\n",
            "    usage_metadata={\n",
            "        'total_token_count': 78613,\n",
            "    },\n",
            "    create_time=2024-08-21 02:37:48.009988+00:00,\n",
            "    update_time=2024-08-21 02:37:48.009988+00:00,\n",
            "    expire_time=2024-08-21 02:52:47.005240+00:00\n",
            ")\n",
            "CachedContent(\n",
            "    name='cachedContents/bkn36tvko8ws',\n",
            "    model='models/gemini-1.5-flash-001',\n",
            "    display_name='PDF-file',\n",
            "    usage_metadata={\n",
            "        'total_token_count': 112395,\n",
            "    },\n",
            "    create_time=2024-08-21 02:35:04.336944+00:00,\n",
            "    update_time=2024-08-21 02:35:04.336944+00:00,\n",
            "    expire_time=2024-08-21 02:50:03.114907+00:00\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bs0j1L7wPS5_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}