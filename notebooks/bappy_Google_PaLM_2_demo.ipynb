{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://ai.google/discover/palm2/"
      ],
      "metadata": {
        "id": "hpi80MDykI5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**01: Install All the Required Packages**"
      ],
      "metadata": {
        "id": "9G1P6onxfohf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ca5hHrRZfEsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c509f94e-7a92-4094-a071-2e32acecfbf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/133.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m133.1/133.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/267.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.9/267.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**02: Import All the Required Libraries**"
      ],
      "metadata": {
        "id": "c7Z7vBYjf20s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as palm"
      ],
      "metadata": {
        "id": "Uh1Wbmp-f1Z8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**03: Pass the PaLM API Key**"
      ],
      "metadata": {
        "id": "m8p7qGvHgokC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palm.configure(api_key = \"AIzaSyCON4Y32JXFTj6NzeSBQJqfTtxoWU4DepM\")"
      ],
      "metadata": {
        "id": "DNhlqQAcf-Ko"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**04: Select a Text Generation Model from the available Model List**"
      ],
      "metadata": {
        "id": "w8XfiWszg3Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in palm.list_models():\n",
        "  print(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjEFCTynmw7B",
        "outputId": "f72bec27-2381-4f53-ebee-dfb0f6f2134f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/chat-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Chat Bison',\n",
            "      description='Chat-optimized generative language model.',\n",
            "      input_token_limit=4096,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
            "      temperature=0.25,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Text Bison',\n",
            "      description='Model targeted for text generation.',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-recitation-off',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Text Bison Recitation Off',\n",
            "      description='Model targeted for text generation with recitation turned off.',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-safety-off',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Text Bison Safety Off',\n",
            "      description='Model targeted for text generation with safety turned off.',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-safety-recitation-off',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Text Bison Safety and Recitation Off',\n",
            "      description='Model targeted for text generation with safety and recitation turned off.',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/embedding-gecko-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Embedding Gecko 002',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText'],\n",
            "      temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    m for m in palm.list_models() if \"generateText\" in m.supported_generation_methods\n",
        "]\n",
        "\n",
        "for m in models:\n",
        "  print(f\"Model Name: {m.name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUGx5gL2gKTs",
        "outputId": "7226272a-adfe-4fcf-a969-8ba44928cabb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Name: models/text-bison-001\n",
            "Model Name: models/text-bison-recitation-off\n",
            "Model Name: models/text-bison-safety-off\n",
            "Model Name: models/text-bison-safety-recitation-off\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[0].name\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WRLcUVOhUIe",
        "outputId": "66e5c6f6-efed-442a-ec10-39a984281a9b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/text-bison-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**05: Input Prompt**"
      ],
      "metadata": {
        "id": "tok4HLjvkKmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "\n",
        "Provide a summary of this paragraph by including all the necessary information.\n",
        "Text: \"Johannes Gutenberg (1398 – 1468) was a German goldsmith and publisher who introduced printing to Europe. His introduction of mechanical movable type printing to Europe started the Printing Revolution and is widely regarded as the most important event of the modern period. It played a key role in the scientific revolution and laid the basis for the modern knowledge-based economy and the spread of learning to the masses.\n",
        "Gutenberg many contributions to printing are: the invention of a process for mass-producing movable type, the use of oil-based ink for printing books, adjustable molds, and the use of a wooden printing press. His truly epochal invention was the combination of these elements into a practical system that allowed the mass production of printed books and was economically viable for printers and readers alike.\n",
        "In Renaissance Europe, the arrival of mechanical movable type printing introduced the era of mass communication which permanently altered the structure of society. The relatively unrestricted circulation of information—including revolutionary ideas—transcended borders, and captured the masses in the Reformation. The sharp increase in literacy broke the monopoly of the literate elite on education and learning and bolstered the emerging middle class.\"\n",
        "\n",
        "Summary:\"The German Johannes Gutenberg introduced printing in Europe. His invention had a decisive contribution in spread of mass-learning and in building the basis of the modern society.\n",
        "Gutenberg major invention was a practical system permitting the mass production of printed books. The printed books allowed open circulation of information, and prepared the evolution of society from to the contemporary knowledge-based economy.\"\n",
        "\n",
        "Text: \"The Covid-19 pandemic necessitated a global shift to online learning. While researchers have examined the impact of remote learning on elementary students' academic performance, less is known about elementary teachers' perceptions of teaching online during the pandemic. This qualitative inquiry used interviews to better understand how elementary teachers experienced remote instruction. The results suggest that teachers need more guidance from administration and resources to manage stress. These findings can inform the development of future distance learning plans that better address teachers' needs\"\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "c_8DIadihcua"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**06: Summary**"
      ],
      "metadata": {
        "id": "C2pEx1mQkPBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0.3,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBL5oWpPjv4u",
        "outputId": "9fe3a33a-2e8d-4627-fdf3-b64a2800c10c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: The Covid-19 pandemic led to a global shift to online learning. This qualitative inquiry interviewed elementary teachers to understand their experiences teaching online during the pandemic. The results suggest that teachers need more guidance from administration and resources to manage stress.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "ETwkpRR6n_-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "\n",
        "Could you please help me to write code to generate multiples of a number from a given list.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZhiDA9uFjzAC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0.3,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "completion.result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "MN3J-yKMn3qb",
        "outputId": "27de9327-da29-42a5-a060-98ee415b4670"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'```python\\ndef generate_multiples(number, list1):\\n  \"\"\"Generates a list of multiples of a given number from a given list.\\n\\n  Args:\\n    number: The number to generate multiples of.\\n    list1: The list to generate multiples from.\\n\\n  Returns:\\n    A list of multiples of the given number from the given list.\\n  \"\"\"\\n\\n  multiples = []\\n  for item in list1:\\n    multiples.append(number * item)\\n  return multiples\\n\\n\\nprint(generate_multiples(3, [1, 2, 3, 4, 5]))\\n# [3, 6, 9, 12, 15]\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HympzbBvoHFo",
        "outputId": "c764a0d4-fbd7-448f-8a4d-1312d7e3c0d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def generate_multiples(number, list1):\n",
            "  \"\"\"Generates a list of multiples of a given number from a given list.\n",
            "\n",
            "  Args:\n",
            "    number: The number to generate multiples of.\n",
            "    list1: The list to generate multiples from.\n",
            "\n",
            "  Returns:\n",
            "    A list of multiples of the given number from the given list.\n",
            "  \"\"\"\n",
            "\n",
            "  multiples = []\n",
            "  for item in list1:\n",
            "    multiples.append(number * item)\n",
            "  return multiples\n",
            "\n",
            "\n",
            "print(generate_multiples(3, [1, 2, 3, 4, 5]))\n",
            "# [3, 6, 9, 12, 15]\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s-hwzGFhoKxC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}