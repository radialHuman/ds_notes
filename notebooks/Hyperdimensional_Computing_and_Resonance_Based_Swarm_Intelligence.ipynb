{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyperdimensional Computing and Resonance-Based Swarm Intelligence**\n",
        "\n",
        "## **Overview**\n",
        "This notebook integrates **Hyperdimensional Computing (HDC)**, **Pyramid-Resonance AI**, and **Queen-Agent Swarm Intelligence** into an optimized **adaptive learning framework**. The system is designed to encode, retrieve, and amplify contextual data while leveraging swarm-based decision-making. Additionally, a **Monte Carlo Optimization** process is implemented to refine encoding strategies based on class-specific adaptation.\n",
        "\n",
        "## **Key Components**\n",
        "### 1. **Hyperdimensional Computing (HDC)**\n",
        "- Implements a **Hyperdimensional Encoder** that converts text into **high-dimensional vector representations**.\n",
        "- Uses **random binary vectors** to represent words and encodes documents based on word distributions.\n",
        "- Computes similarity between encoded vectors using **cosine similarity**.\n",
        "\n",
        "### 2. **Pyramid-Resonance AI**\n",
        "- A **multi-layer resonance model** that applies **resonance transformations** to encoded vectors.\n",
        "- Uses **adaptive scaling** based on entropy levels to **dynamically modulate resonance amplification**.\n",
        "- Applies **nonlinear transformations (tanh)** and **resampling techniques** to refine contextual embedding.\n",
        "\n",
        "### 3. **Queen-Agent Swarm Intelligence**\n",
        "- Implements a **swarm learning framework** where **agents are trained to optimize contextual retrieval**.\n",
        "- Uses **pheromone-based reinforcement learning** to **prioritize high-performing agents**.\n",
        "- Introduces **adaptive stabilization** to prevent **overfitting and stagnation** in the learning process.\n",
        "\n",
        "### 4. **Monte Carlo Optimization for Class-Specific Encoding**\n",
        "- Runs **multiple simulations** to optimize **scaling factors** for encoding based on **document classes**.\n",
        "- Adjusts encoding **scaling parameters per class** to **maximize similarity within categories**.\n",
        "- Computes and visualizes **similarity matrices** to analyze **clustering effects of encoded vectors**.\n",
        "\n",
        "## **Applications**\n",
        "- **Text Retrieval & Classification**: Enhances document searchability using **adaptive swarm-based retrieval**.\n",
        "- **Contextual Intelligence**: Uses **resonance-based amplification** to improve **semantic understanding**.\n",
        "- **Self-Optimizing AI Systems**: Implements **adaptive reinforcement learning** to improve **decision-making over time**.\n",
        "- **Efficient High-Dimensional Representations**: Uses **Monte Carlo search** to **fine-tune encoding strategies** per class.\n",
        "\n",
        "## **Execution Steps**\n",
        "1. **Initialize HDC Encoder** to encode text data into high-dimensional vectors.\n",
        "2. **Train Queen-Agent Swarm** to optimize retrieval and decision-making.\n",
        "3. **Apply Pyramid-Resonance AI** to amplify and refine encoded vectors.\n",
        "4. **Run Monte Carlo Optimization** to fine-tune encoding strategies.\n",
        "5. **Analyze Performance** using similarity matrices and benchmark results."
      ],
      "metadata": {
        "id": "zqn8N1bOh8jc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rij0vshXQ608",
        "outputId": "b1939801-3a20-40ef-ef41-6b597359ba3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Similarity Score (NSA-Killer Model): 0.012334041771775776\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy.signal\n",
        "\n",
        "# -------------------------------\n",
        "# Hyperdimensional Computing (HDC)\n",
        "# -------------------------------\n",
        "class HyperdimensionalEncoder:\n",
        "    def __init__(self, dimension=10000, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.dimension = dimension\n",
        "        self.token_vectors = {}\n",
        "\n",
        "    def generate_random_vector(self):\n",
        "        return np.random.choice([-1, 1], size=(self.dimension,))\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        words = text.split()\n",
        "        encoded_vector = np.zeros(self.dimension)\n",
        "        for word in words:\n",
        "            if word not in self.token_vectors:\n",
        "                self.token_vectors[word] = self.generate_random_vector()\n",
        "            encoded_vector += self.token_vectors[word]\n",
        "        return np.sign(encoded_vector)\n",
        "\n",
        "    def similarity(self, vec1, vec2):\n",
        "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "# -------------------------------\n",
        "# Pyramid-Resonance AI\n",
        "# -------------------------------\n",
        "class PyramidResonanceAI:\n",
        "    def __init__(self, layers=3, dimension=10000):\n",
        "        self.layers = layers\n",
        "        self.dimension = dimension\n",
        "        self.weights = [np.random.randn(dimension) for _ in range(layers)]\n",
        "\n",
        "    def apply_resonance(self, vector):\n",
        "        resonant_vector = vector.copy()\n",
        "        for i in range(self.layers):\n",
        "            resonant_vector = np.tanh(np.multiply(resonant_vector, self.weights[i]))\n",
        "            resonant_vector = scipy.signal.resample(resonant_vector, self.dimension)\n",
        "        return resonant_vector\n",
        "\n",
        "# -------------------------------\n",
        "# Queen-Agent Optimized Swarm Intelligence\n",
        "# -------------------------------\n",
        "class ImprovedQueenAgentSwarm:\n",
        "    def __init__(self, num_agents=10, dimension=10000, learning_rate=0.05, inheritance_factor=0.5):\n",
        "        self.num_agents = num_agents\n",
        "        self.dimension = dimension\n",
        "        self.learning_rate = learning_rate\n",
        "        self.inheritance_factor = inheritance_factor\n",
        "        self.agents = [np.random.randn(dimension) for _ in range(num_agents)]\n",
        "        self.pheromones = np.ones(num_agents)\n",
        "        self.rewards = np.zeros(num_agents)\n",
        "\n",
        "    def select_best_agent(self, query_vector):\n",
        "        similarities = np.array([hdc.similarity(query_vector, self.agents[i]) for i in range(self.num_agents)])\n",
        "        weighted_similarities = similarities * self.pheromones\n",
        "        best_agent = np.argmax(weighted_similarities)\n",
        "        self.rewards[best_agent] += 1\n",
        "        return self.agents[best_agent]\n",
        "\n",
        "    def refine_agents(self):\n",
        "        max_reward = np.max(self.rewards) if np.max(self.rewards) > 0 else 1\n",
        "        self.pheromones = (self.rewards / max_reward) ** 0.5\n",
        "        min_pheromone_index = np.argmin(self.pheromones)\n",
        "        if self.pheromones[min_pheromone_index] < 0.3:\n",
        "            best_agent_index = np.argmax(self.pheromones)\n",
        "            self.agents[min_pheromone_index] = (\n",
        "                self.agents[best_agent_index] * self.inheritance_factor +\n",
        "                self.agents[min_pheromone_index] * (1 - self.inheritance_factor)\n",
        "            )\n",
        "        self.rewards *= (1 - self.learning_rate)\n",
        "\n",
        "    def retrieve_context(self, text):\n",
        "        query_vector = hdc.encode_text(text)\n",
        "        return self.select_best_agent(query_vector)\n",
        "\n",
        "# -------------------------------\n",
        "# Full Integrated NSA-Killer Model\n",
        "# -------------------------------\n",
        "class OptimizedIntegratedModel:\n",
        "    def __init__(self, num_agents=10, dimension=10000, pyramid_layers=3, resonance_scaling=0.5):\n",
        "        self.swarm = ImprovedQueenAgentSwarm(num_agents=num_agents, dimension=dimension)\n",
        "        self.pyramid = PyramidResonanceAI(layers=pyramid_layers, dimension=dimension)\n",
        "        self.resonance_scaling = resonance_scaling\n",
        "\n",
        "    def retrieve_and_amplify(self, text):\n",
        "        swarm_vector = self.swarm.retrieve_context(text)\n",
        "        swarm_vector = swarm_vector / np.linalg.norm(swarm_vector)\n",
        "        resonant_vector = self.pyramid.apply_resonance(swarm_vector)\n",
        "        blended_vector = (1 - self.resonance_scaling) * swarm_vector + self.resonance_scaling * resonant_vector\n",
        "        return blended_vector\n",
        "\n",
        "# -------------------------------\n",
        "# Running the Model\n",
        "# -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    hdc = HyperdimensionalEncoder()\n",
        "    integrated_model = OptimizedIntegratedModel()\n",
        "\n",
        "    text1 = \"The quick brown fox jumps over the lazy dog\"\n",
        "    text2 = \"A fast brown fox leaps above the sleepy canine\"\n",
        "\n",
        "    final_vector1_optimized = integrated_model.retrieve_and_amplify(text1)\n",
        "    final_vector2_optimized = integrated_model.retrieve_and_amplify(text2)\n",
        "\n",
        "    final_similarity_score_optimized = hdc.similarity(final_vector1_optimized, final_vector2_optimized)\n",
        "    print(\"Final Similarity Score (NSA-Killer Model):\", final_similarity_score_optimized)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Refining the Swarm Intelligence: Introduce Soft Adaptation Instead of Hard Fixing\n",
        "\n",
        "class AdaptiveStableQueenAgentSwarm:\n",
        "    def __init__(self, num_agents=10, dimension=10000, learning_rate=0.05, inheritance_factor=0.5, adaptation_threshold=5):\n",
        "        \"\"\"Initialize Queen Agent with soft adaptation to maintain stability while avoiding overfitting.\"\"\"\n",
        "        self.num_agents = num_agents\n",
        "        self.dimension = dimension\n",
        "        self.learning_rate = learning_rate\n",
        "        self.inheritance_factor = inheritance_factor\n",
        "        self.agents = [np.random.randn(dimension) for _ in range(num_agents)]\n",
        "        self.pheromones = np.ones(num_agents)\n",
        "        self.rewards = np.zeros(num_agents)\n",
        "        self.best_agent_index = None  # Track the best agent consistently\n",
        "        self.adaptation_counter = 0\n",
        "        self.adaptation_threshold = adaptation_threshold  # How many rounds before re-evaluating best agent\n",
        "\n",
        "    def select_best_agent(self, query_vector):\n",
        "        \"\"\"Select the best agent but allow soft adaptation every few rounds.\"\"\"\n",
        "        similarities = np.array([hdc.similarity(query_vector, self.agents[i]) for i in range(self.num_agents)])\n",
        "\n",
        "        if self.best_agent_index is None or self.adaptation_counter >= self.adaptation_threshold:\n",
        "            self.best_agent_index = np.argmax(similarities)  # Allow adaptation every few rounds\n",
        "            self.adaptation_counter = 0  # Reset adaptation timer\n",
        "\n",
        "        self.adaptation_counter += 1  # Track iterations before adapting again\n",
        "        self.rewards[self.best_agent_index] += 1\n",
        "        return self.agents[self.best_agent_index]\n",
        "\n",
        "    def refine_agents(self):\n",
        "        \"\"\"Apply reinforcement learning and maintain stability while allowing gradual adaptation.\"\"\"\n",
        "        max_reward = np.max(self.rewards) if np.max(self.rewards) > 0 else 1\n",
        "        self.pheromones = (self.rewards / max_reward) ** 0.5\n",
        "\n",
        "        # Ensure weak agents evolve only when necessary\n",
        "        min_pheromone_index = np.argmin(self.pheromones)\n",
        "        if self.pheromones[min_pheromone_index] < 0.3:\n",
        "            best_agent_index = np.argmax(self.pheromones)\n",
        "            self.agents[min_pheromone_index] = (\n",
        "                self.agents[best_agent_index] * self.inheritance_factor +\n",
        "                self.agents[min_pheromone_index] * (1 - self.inheritance_factor)\n",
        "            )\n",
        "\n",
        "        self.rewards *= (1 - self.learning_rate)\n",
        "\n",
        "    def retrieve_context(self, text):\n",
        "        \"\"\"Retrieve context using the best agent but with soft adaptation over time.\"\"\"\n",
        "        query_vector = hdc.encode_text(text)\n",
        "        return self.select_best_agent(query_vector)\n",
        "\n",
        "# Updating the Integrated Model with Soft Adaptive Swarm Selection\n",
        "class AdaptiveStableOptimizedModel:\n",
        "    def __init__(self, num_agents=10, dimension=10000, pyramid_layers=3):\n",
        "        \"\"\"Integrate Adaptive Stable Queen Swarm Intelligence with dynamically scaled Pyramid-Resonance AI.\"\"\"\n",
        "        self.swarm = AdaptiveStableQueenAgentSwarm(num_agents=num_agents, dimension=dimension)\n",
        "        self.pyramid = RefinedDynamicallyScaledResonanceAI(layers=pyramid_layers, dimension=dimension)\n",
        "\n",
        "    def retrieve_and_amplify(self, text):\n",
        "        \"\"\"Retrieve optimized context using adaptive swarm intelligence and amplify it with resonance.\"\"\"\n",
        "        swarm_vector = self.swarm.retrieve_context(text)\n",
        "        swarm_vector = swarm_vector / np.linalg.norm(swarm_vector)  # Normalize Swarm Vector\n",
        "\n",
        "        complexity = np.std(swarm_vector) / np.mean(np.abs(swarm_vector) + 1e-8)\n",
        "        blending_weight = np.clip(complexity, 0.3, 0.7)  # Adaptive balance between swarm and resonance\n",
        "\n",
        "        resonant_vector = self.pyramid.apply_resonance(swarm_vector)\n",
        "        blended_vector = (1 - blending_weight) * swarm_vector + blending_weight * resonant_vector\n",
        "\n",
        "        return blended_vector\n",
        "\n",
        "# Initialize the Adaptive Optimized Model\n",
        "adaptive_optimized_model = AdaptiveStableOptimizedModel()\n",
        "\n",
        "# Retrieve and amplify context using the adaptive NSA-Killer Framework\n",
        "final_vector1_adaptive = adaptive_optimized_model.retrieve_and_amplify(text1)\n",
        "final_vector2_adaptive = adaptive_optimized_model.retrieve_and_amplify(text2)\n",
        "\n",
        "# Compute Similarity After Adaptive Swarm Selection Implementation\n",
        "final_similarity_score_adaptive = hdc.similarity(final_vector1_adaptive, final_vector2_adaptive)\n",
        "final_similarity_score_adaptive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1P8b40iTOJK",
        "outputId": "026a808e-d823-45dd-9426-2535c9a152a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999999999999"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Small Real-World Task: Document Retrieval Simulation\n",
        "\n",
        "# Simulating a small dataset of documents (e.g., customer support chat history, FAQ system)\n",
        "documents = [\n",
        "    \"How do I reset my password?\",\n",
        "    \"What is the refund policy for digital products?\",\n",
        "    \"Can I change my shipping address after placing an order?\",\n",
        "    \"How do I contact customer support?\",\n",
        "    \"What payment methods do you accept?\",\n",
        "    \"How long does it take for an order to be delivered?\",\n",
        "    \"How do I cancel my subscription?\",\n",
        "    \"What is the warranty period for purchased items?\",\n",
        "    \"Do you offer international shipping?\",\n",
        "    \"How can I track my order status?\"\n",
        "]\n",
        "\n",
        "# Query to match against the documents\n",
        "query = \"I forgot my password, how can I recover it?\"\n",
        "\n",
        "# Retrieve the best-matching document using NSA-Killer Model\n",
        "query_vector = adaptive_optimized_model.retrieve_and_amplify(query)\n",
        "similarity_scores = [hdc.similarity(query_vector, adaptive_optimized_model.retrieve_and_amplify(doc)) for doc in documents]\n",
        "\n",
        "# Identify the best-matching document\n",
        "best_match_index = np.argmax(similarity_scores)\n",
        "best_matching_document = documents[best_match_index]\n",
        "\n",
        "# Benchmark execution time for retrieving the best-matching document\n",
        "retrieval_execution_time = benchmark_model(adaptive_optimized_model, [query])\n",
        "\n",
        "# Output results\n",
        "real_world_task_results = {\n",
        "    \"Query\": query,\n",
        "    \"Best Matching Document\": best_matching_document,\n",
        "    \"Similarity Score\": similarity_scores[best_match_index],\n",
        "    \"Execution Time (Avg per sample)\": retrieval_execution_time\n",
        "}\n",
        "\n",
        "real_world_task_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1YR_1XOVTu3",
        "outputId": "d51edcce-dc1c-446a-a1dd-40ca95940ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Query': 'I forgot my password, how can I recover it?',\n",
              " 'Best Matching Document': 'How do I reset my password?',\n",
              " 'Similarity Score': 0.9999999999999999,\n",
              " 'Execution Time (Avg per sample)': 0.006218409538269043}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running an extended benchmarking test for NSA-Killer Model across varied text complexity\n",
        "\n",
        "# Define additional real-world queries and documents\n",
        "extended_documents = [\n",
        "    \"How do I update my billing information?\",\n",
        "    \"What are the steps to secure my account?\",\n",
        "    \"How can I enable two-factor authentication?\",\n",
        "    \"Where can I check my order history?\",\n",
        "    \"What should I do if my payment fails?\",\n",
        "    \"Can I request a refund after 30 days?\",\n",
        "    \"How do I delete my account permanently?\",\n",
        "    \"What are the benefits of a premium subscription?\",\n",
        "    \"Is there a way to export my account data?\",\n",
        "    \"How can I disable email notifications?\"\n",
        "]\n",
        "\n",
        "# Additional real-world queries\n",
        "extended_queries = [\n",
        "    \"How do I add a new credit card to my account?\",\n",
        "    \"What are the best practices for account security?\",\n",
        "    \"How do I set up 2FA on my phone?\",\n",
        "    \"How can I find past purchases on my account?\",\n",
        "    \"What do I do if my credit card is declined?\",\n",
        "    \"Can I still get a refund if it's been more than a month?\",\n",
        "    \"I want to remove my account, what should I do?\",\n",
        "    \"Is there a premium membership? What does it include?\",\n",
        "    \"How do I download all my account data?\",\n",
        "    \"How do I stop receiving emails from your service?\"\n",
        "]\n",
        "\n",
        "# Running similarity tests for multiple queries across all documents\n",
        "query_vectors = [adaptive_optimized_model.retrieve_and_amplify(q) for q in extended_queries]\n",
        "document_vectors = [adaptive_optimized_model.retrieve_and_amplify(doc) for doc in extended_documents]\n",
        "\n",
        "# Compute similarity scores for all query-document pairs\n",
        "similarity_matrix = np.array([[hdc.similarity(qv, dv) for dv in document_vectors] for qv in query_vectors])\n",
        "\n",
        "# Identify the best-matching document for each query\n",
        "best_match_indices = np.argmax(similarity_matrix, axis=1)\n",
        "best_matches = [extended_documents[idx] for idx in best_match_indices]\n",
        "\n",
        "# Benchmark execution time for full query-document retrieval across all examples\n",
        "extended_retrieval_execution_time = benchmark_model(adaptive_optimized_model, extended_queries)\n",
        "\n",
        "# Output results\n",
        "extended_real_world_results = {\n",
        "    \"Queries\": extended_queries,\n",
        "    \"Best Matches\": best_matches,\n",
        "    \"Similarity Scores\": [similarity_matrix[i, idx] for i, idx in enumerate(best_match_indices)],\n",
        "    \"Execution Time (Avg per sample)\": extended_retrieval_execution_time\n",
        "}\n",
        "\n",
        "extended_real_world_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2QmFvRRWAVM",
        "outputId": "4eb43427-7433-4711-fdb0-97e720b9f007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Queries': ['How do I add a new credit card to my account?',\n",
              "  'What are the best practices for account security?',\n",
              "  'How do I set up 2FA on my phone?',\n",
              "  'How can I find past purchases on my account?',\n",
              "  'What do I do if my credit card is declined?',\n",
              "  \"Can I still get a refund if it's been more than a month?\",\n",
              "  'I want to remove my account, what should I do?',\n",
              "  'Is there a premium membership? What does it include?',\n",
              "  'How do I download all my account data?',\n",
              "  'How do I stop receiving emails from your service?'],\n",
              " 'Best Matches': ['How can I enable two-factor authentication?',\n",
              "  'How can I enable two-factor authentication?',\n",
              "  'How can I enable two-factor authentication?',\n",
              "  'How can I enable two-factor authentication?',\n",
              "  'How can I enable two-factor authentication?',\n",
              "  'How can I enable two-factor authentication?',\n",
              "  'How can I enable two-factor authentication?',\n",
              "  'How do I update my billing information?',\n",
              "  'How do I update my billing information?',\n",
              "  'How do I update my billing information?'],\n",
              " 'Similarity Scores': [0.0180029823725249,\n",
              "  0.0180029823725249,\n",
              "  -0.006521670354438725,\n",
              "  -0.006521670354438725,\n",
              "  -0.006521670354438725,\n",
              "  -0.006521670354438725,\n",
              "  -0.006521670354438725,\n",
              "  0.9999999999999999,\n",
              "  0.9999999999999999,\n",
              "  0.9999999999999999],\n",
              " 'Execution Time (Avg per sample)': 0.003384284973144531}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.signal\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------------------\n",
        "# Hyperdimensional Computing (HDC)\n",
        "# -------------------------------\n",
        "class HyperdimensionalEncoder:\n",
        "    def __init__(self, dimension=10000, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.dimension = dimension\n",
        "        self.token_vectors = {}\n",
        "\n",
        "    def generate_random_vector(self):\n",
        "        return np.random.choice([-1, 1], size=(self.dimension,))\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        words = text.split()\n",
        "        encoded_vector = np.zeros(self.dimension)\n",
        "        for word in words:\n",
        "            if word not in self.token_vectors:\n",
        "                self.token_vectors[word] = self.generate_random_vector()\n",
        "            encoded_vector += self.token_vectors[word]\n",
        "        return np.sign(encoded_vector)\n",
        "\n",
        "    def similarity(self, vec1, vec2):\n",
        "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "# -------------------------------\n",
        "# Pyramid-Resonance AI\n",
        "# -------------------------------\n",
        "class RefinedDynamicallyScaledResonanceAI:\n",
        "    def __init__(self, layers=3, dimension=10000, min_scaling=0.1811, max_scaling=0.8479):\n",
        "        self.layers = layers\n",
        "        self.dimension = dimension\n",
        "        self.weights = [np.random.randn(dimension) for _ in range(layers)]\n",
        "        self.min_scaling = min_scaling\n",
        "        self.max_scaling = max_scaling\n",
        "\n",
        "    def compute_scaling_factor(self, vector):\n",
        "        entropy = np.std(vector) / np.mean(np.abs(vector) + 1e-8)\n",
        "        return np.clip(self.min_scaling + (self.max_scaling - self.min_scaling) * entropy, self.min_scaling, self.max_scaling)\n",
        "\n",
        "    def apply_resonance(self, vector):\n",
        "        scaling_factor = self.compute_scaling_factor(vector)\n",
        "        resonant_vector = vector.copy()\n",
        "        for i in range(self.layers):\n",
        "            resonant_vector = np.tanh(np.multiply(resonant_vector, self.weights[i]) * scaling_factor)\n",
        "            resonant_vector = scipy.signal.resample(resonant_vector, self.dimension)\n",
        "        return resonant_vector\n",
        "\n",
        "# -------------------------------\n",
        "# Enhanced Query-Specific Swarm Intelligence\n",
        "# -------------------------------\n",
        "class StabilizedContextAwareQueenAgentSwarm:\n",
        "    def __init__(self, num_agents=10, dimension=10000, learning_rate=0.0172, inheritance_factor=0.5662, adaptation_threshold=3, penalty_factor=0.3578):\n",
        "        self.num_agents = num_agents\n",
        "        self.dimension = dimension\n",
        "        self.learning_rate = learning_rate\n",
        "        self.inheritance_factor = inheritance_factor\n",
        "        self.adaptation_threshold = adaptation_threshold\n",
        "        self.penalty_factor = penalty_factor\n",
        "        self.agents = [np.random.randn(dimension) for _ in range(num_agents)]\n",
        "        self.pheromones = np.ones(num_agents)\n",
        "        self.rewards = np.zeros(num_agents)\n",
        "        self.best_agents = {}\n",
        "\n",
        "    def select_best_agent(self, query_vector, query_id):\n",
        "        similarities = np.array([hdc.similarity(query_vector, self.agents[i]) for i in range(self.num_agents)])\n",
        "        if query_id not in self.best_agents or self.rewards[self.best_agents[query_id]] < np.max(similarities):\n",
        "            self.best_agents[query_id] = np.argmax(similarities)\n",
        "        self.rewards[self.best_agents[query_id]] += 3\n",
        "        self.rewards -= self.penalty_factor\n",
        "        return self.agents[self.best_agents[query_id]]\n",
        "\n",
        "    def retrieve_context(self, text, query_id):\n",
        "        query_vector = hdc.encode_text(text)\n",
        "        return self.select_best_agent(query_vector, query_id)\n",
        "    def __init__(self, num_agents=10, dimension=10000, learning_rate=0.0172, inheritance_factor=0.5662, adaptation_threshold=3, penalty_factor=0.3578):\n",
        "        self.num_agents = num_agents\n",
        "        self.dimension = dimension\n",
        "        self.learning_rate = learning_rate\n",
        "        self.inheritance_factor = inheritance_factor\n",
        "        self.adaptation_threshold = adaptation_threshold\n",
        "        self.penalty_factor = penalty_factor\n",
        "        self.agents = [np.random.randn(dimension) for _ in range(num_agents)]\n",
        "        self.pheromones = np.ones(num_agents)\n",
        "        self.rewards = np.zeros(num_agents)\n",
        "        self.best_agents = {}\n",
        "\n",
        "    def select_best_agent(self, query_vector, query_id):\n",
        "        similarities = np.array([hdc.similarity(query_vector, self.agents[i]) for i in range(self.num_agents)])\n",
        "        if query_id not in self.best_agents or self.rewards[self.best_agents[query_id]] < np.max(similarities):\n",
        "            self.best_agents[query_id] = np.argmax(similarities)\n",
        "        self.rewards[self.best_agents[query_id]] += 3\n",
        "        self.rewards -= self.penalty_factor\n",
        "        return self.agents[self.best_agents[query_id]]\n",
        "\n",
        "# -------------------------------\n",
        "# Monte Carlo Optimization for NSA-Killer Model\n",
        "# -------------------------------\n",
        "class MonteCarloNSAOptimizer:\n",
        "    def __init__(self, num_simulations=50):\n",
        "        self.num_simulations = num_simulations\n",
        "        self.results = []\n",
        "\n",
        "    def run_simulation(self, learning_rate, inheritance_factor, penalty_factor, resonance_min, resonance_max):\n",
        "        model = StabilizedOptimizedModel()\n",
        "        model.swarm.learning_rate = learning_rate\n",
        "        model.swarm.inheritance_factor = inheritance_factor\n",
        "        model.swarm.rewards -= penalty_factor\n",
        "        model.pyramid.min_scaling = resonance_min\n",
        "        model.pyramid.max_scaling = resonance_max\n",
        "\n",
        "        vector1 = model.retrieve_and_amplify(text1, 0)\n",
        "        vector2 = model.retrieve_and_amplify(text2, 1)\n",
        "        similarity_score = hdc.similarity(vector1, vector2)\n",
        "\n",
        "        self.results.append({\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"inheritance_factor\": inheritance_factor,\n",
        "            \"penalty_factor\": penalty_factor,\n",
        "            \"resonance_min\": resonance_min,\n",
        "            \"resonance_max\": resonance_max,\n",
        "            \"similarity_score\": similarity_score,\n",
        "            \"agent_selection_log\": model.swarm.best_agents.copy()\n",
        "        })\n",
        "\n",
        "    def run_monte_carlo(self):\n",
        "        for _ in range(self.num_simulations):\n",
        "            learning_rate = np.random.uniform(0.01, 0.05)\n",
        "            inheritance_factor = np.random.uniform(0.4, 0.7)\n",
        "            penalty_factor = np.random.uniform(0.1, 1.0)\n",
        "            resonance_min = np.random.uniform(0.15, 0.3)\n",
        "            resonance_max = np.random.uniform(0.6, 0.9)\n",
        "            self.run_simulation(learning_rate, inheritance_factor, penalty_factor, resonance_min, resonance_max)\n",
        "        best_result = max(self.results, key=lambda x: x[\"similarity_score\"])\n",
        "        return best_result, self.results\n",
        "\n",
        "# -------------------------------\n",
        "# Running the Model\n",
        "# -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    hdc = HyperdimensionalEncoder()\n",
        "    monte_carlo_optimizer = MonteCarloNSAOptimizer(num_simulations=50)\n",
        "    best_config, all_results = monte_carlo_optimizer.run_monte_carlo()\n",
        "    df_results = pd.DataFrame(all_results)\n",
        "    print(\"Best Monte Carlo Configuration:\", best_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yw_ThcYWl5w",
        "outputId": "c581be5c-3766-4376-9b6d-641eb8a9a289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Monte Carlo Configuration: {'learning_rate': 0.041508022325720546, 'inheritance_factor': 0.4479876131811957, 'penalty_factor': 0.3235989634396341, 'resonance_min': 0.25006058838074874, 'resonance_max': 0.8929548199402698, 'similarity_score': 1.0000000000000002, 'agent_selection_log': {0: 7, 1: 7}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XndfG-yrbjYc",
        "outputId": "eb0f3d2b-6449-4e31-84a0-5f8a04461e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.signal\n",
        "import time\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# -------------------------------\n",
        "# Hyperdimensional Computing (HDC)\n",
        "# -------------------------------\n",
        "class HyperdimensionalEncoder:\n",
        "    def __init__(self, dimension=10000, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.dimension = dimension\n",
        "        self.token_vectors = {}\n",
        "\n",
        "    def generate_random_vector(self):\n",
        "        return np.random.choice([-1, 1], size=(self.dimension,))\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        words = text.split()\n",
        "        encoded_vector = np.zeros(self.dimension)\n",
        "        for word in words:\n",
        "            if word not in self.token_vectors:\n",
        "                self.token_vectors[word] = self.generate_random_vector()\n",
        "            encoded_vector += self.token_vectors[word]\n",
        "        return np.sign(encoded_vector)\n",
        "\n",
        "    def similarity(self, vec1, vec2):\n",
        "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "# -------------------------------\n",
        "# Loading and Encoding a Real-World Dataset\n",
        "# -------------------------------\n",
        "# Choose a dataset (e.g., 'ag_news' for text classification)\n",
        "dataset = load_dataset(\"ag_news\", split=\"train\")\n",
        "\n",
        "# Extract a subset of texts for encoding (first 100 samples)\n",
        "texts = [example[\"text\"] for example in dataset.select(range(100))]\n",
        "\n",
        "# Initialize NSA-Killer Model for Encoding\n",
        "hdc = HyperdimensionalEncoder()\n",
        "\n",
        "# Encode all texts using NSA-Killer Model\n",
        "encoded_vectors = [hdc.encode_text(text) for text in texts]\n",
        "\n",
        "# Compute similarity matrix for the encoded dataset\n",
        "num_samples = len(encoded_vectors)\n",
        "similarity_matrix = np.zeros((num_samples, num_samples))\n",
        "\n",
        "for i in range(num_samples):\n",
        "    for j in range(i, num_samples):  # Compute only upper triangle (symmetry)\n",
        "        similarity = hdc.similarity(encoded_vectors[i], encoded_vectors[j])\n",
        "        similarity_matrix[i, j] = similarity\n",
        "        similarity_matrix[j, i] = similarity  # Mirror the value\n",
        "\n",
        "# Convert results to a DataFrame for visualization\n",
        "similarity_df = pd.DataFrame(similarity_matrix, columns=[f\"Sample_{i}\" for i in range(num_samples)], index=[f\"Sample_{i}\" for i in range(num_samples)])\n",
        "\n",
        "# Display the similarity matrix\n",
        "print(similarity_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLZtIa9mblm_",
        "outputId": "2188678a-8a80-493f-a714-d3b63b0d7fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Sample_0  Sample_1  Sample_2  Sample_3  Sample_4  Sample_5  \\\n",
            "Sample_0   1.000000  0.127933  0.188860  0.082969  0.042200  0.100987   \n",
            "Sample_1   0.127933  1.000000  0.239341  0.140729  0.052941  0.145976   \n",
            "Sample_2   0.188860  0.239341  1.000000  0.118827  0.154618  0.179992   \n",
            "Sample_3   0.082969  0.140729  0.118827  1.000000  0.085735  0.181728   \n",
            "Sample_4   0.042200  0.052941  0.154618  0.085735  1.000000  0.069536   \n",
            "...             ...       ...       ...       ...       ...       ...   \n",
            "Sample_95  0.148800  0.129457  0.057072  0.147203  0.097482  0.149536   \n",
            "Sample_96  0.202575  0.173435  0.306467  0.097455  0.108580  0.102403   \n",
            "Sample_97  0.076211  0.122522  0.060494  0.133995  0.037401  0.130469   \n",
            "Sample_98  0.089027  0.150077  0.131792  0.126643  0.139291  0.114834   \n",
            "Sample_99  0.041421  0.125760  0.125550  0.134192  0.108493  0.052409   \n",
            "\n",
            "           Sample_6  Sample_7  Sample_8  Sample_9  ...  Sample_90  Sample_91  \\\n",
            "Sample_0   0.110200  0.072800  0.056075  0.699600  ...   0.096400   0.093600   \n",
            "Sample_1   0.157546  0.146254  0.124914  0.106415  ...   0.169050   0.122820   \n",
            "Sample_2   0.276156  0.212534  0.125920  0.171739  ...   0.180617   0.192665   \n",
            "Sample_3   0.096372  0.141899  0.113963  0.051271  ...   0.113391   0.069779   \n",
            "Sample_4   0.086800  0.115800  0.097843  0.038600  ...   0.105400   0.136600   \n",
            "...             ...       ...       ...       ...  ...        ...        ...   \n",
            "Sample_95  0.043588  0.094906  0.121524  0.102636  ...   0.108862   0.107145   \n",
            "Sample_96  0.193284  0.114199  0.114623  0.176862  ...   0.130837   0.184857   \n",
            "Sample_97  0.084233  0.131065  0.137993  0.050193  ...   0.119574   0.030246   \n",
            "Sample_98  0.197648  0.099889  0.136221  0.062617  ...   0.092861   0.210001   \n",
            "Sample_99  0.159376  0.152437  0.150725  0.040159  ...   0.140663   0.142345   \n",
            "\n",
            "           Sample_92  Sample_93  Sample_94  Sample_95  Sample_96  Sample_97  \\\n",
            "Sample_0    0.070000   0.083660   0.099000   0.148800   0.202575   0.076211   \n",
            "Sample_1    0.110889   0.161458   0.147533   0.129457   0.173435   0.122522   \n",
            "Sample_2    0.190128   0.199248   0.190128   0.057072   0.306467   0.060494   \n",
            "Sample_3    0.078714   0.138099   0.114668   0.147203   0.097455   0.133995   \n",
            "Sample_4    0.105400   0.081946   0.069200   0.097482   0.108580   0.037401   \n",
            "...              ...        ...        ...        ...        ...        ...   \n",
            "Sample_95   0.079875   0.067391   0.158463   1.000000   0.119123   0.180864   \n",
            "Sample_96   0.152661   0.177185   0.144666   0.119123   1.000000   0.066409   \n",
            "Sample_97   0.064069   0.074204   0.119357   0.180864   0.066409   1.000000   \n",
            "Sample_98   0.105853   0.159473   0.126512   0.154687   0.179254   0.088316   \n",
            "Sample_99   0.180612   0.115091   0.111227   0.118961   0.115854   0.130608   \n",
            "\n",
            "           Sample_98  Sample_99  \n",
            "Sample_0    0.089027   0.041421  \n",
            "Sample_1    0.150077   0.125760  \n",
            "Sample_2    0.131792   0.125550  \n",
            "Sample_3    0.126643   0.134192  \n",
            "Sample_4    0.139291   0.108493  \n",
            "...              ...        ...  \n",
            "Sample_95   0.154687   0.118961  \n",
            "Sample_96   0.179254   0.115854  \n",
            "Sample_97   0.088316   0.130608  \n",
            "Sample_98   1.000000   0.168938  \n",
            "Sample_99   0.168938   1.000000  \n",
            "\n",
            "[100 rows x 100 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.signal\n",
        "import time\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# -------------------------------\n",
        "# Hyperdimensional Computing (HDC)\n",
        "# -------------------------------\n",
        "class HyperdimensionalEncoder:\n",
        "    def __init__(self, dimension=10000, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.dimension = dimension\n",
        "        self.token_vectors = {}\n",
        "\n",
        "    def generate_random_vector(self):\n",
        "        return np.random.choice([-1, 1], size=(self.dimension,))\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        words = text.split()\n",
        "        encoded_vector = np.zeros(self.dimension)\n",
        "        for word in words:\n",
        "            if word not in self.token_vectors:\n",
        "                self.token_vectors[word] = self.generate_random_vector()\n",
        "            encoded_vector += self.token_vectors[word]\n",
        "        return np.sign(encoded_vector)\n",
        "\n",
        "    def similarity(self, vec1, vec2):\n",
        "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "# -------------------------------\n",
        "# Monte Carlo Optimization for Encoding by Class\n",
        "# -------------------------------\n",
        "class MonteCarloEncodingOptimizer:\n",
        "    def __init__(self, num_simulations=50, dimension=10000):\n",
        "        self.num_simulations = num_simulations\n",
        "        self.dimension = dimension\n",
        "        self.results = {}\n",
        "\n",
        "    def run_simulation(self, scaling_factor, class_texts, label):\n",
        "        hdc = HyperdimensionalEncoder(dimension=self.dimension)\n",
        "        encoded_vectors = [hdc.encode_text(text) * scaling_factor for text in class_texts]\n",
        "        num_samples = len(encoded_vectors)\n",
        "        similarity_matrix = np.zeros((num_samples, num_samples))\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            for j in range(i, num_samples):\n",
        "                similarity = hdc.similarity(encoded_vectors[i], encoded_vectors[j])\n",
        "                similarity_matrix[i, j] = similarity\n",
        "                similarity_matrix[j, i] = similarity\n",
        "\n",
        "        avg_similarity = np.mean(similarity_matrix)\n",
        "        if label not in self.results or self.results[label][\"avg_similarity\"] < avg_similarity:\n",
        "            self.results[label] = {\"scaling_factor\": scaling_factor, \"avg_similarity\": avg_similarity}\n",
        "\n",
        "    def run_monte_carlo(self, class_data):\n",
        "        for label, class_texts in class_data.items():\n",
        "            for _ in range(self.num_simulations):\n",
        "                scaling_factor = np.random.uniform(0.5, 2.0)\n",
        "                self.run_simulation(scaling_factor, class_texts, label)\n",
        "        return self.results\n",
        "\n",
        "# -------------------------------\n",
        "# Loading and Encoding a Real-World Dataset by Class\n",
        "# -------------------------------\n",
        "# Choose a dataset (e.g., 'ag_news' for text classification)\n",
        "dataset = load_dataset(\"ag_news\", split=\"train\")\n",
        "\n",
        "# Organize texts by class\n",
        "class_data = {}\n",
        "for example in dataset.select(range(100)):\n",
        "    label = example[\"label\"]\n",
        "    if label not in class_data:\n",
        "        class_data[label] = []\n",
        "    class_data[label].append(example[\"text\"])\n",
        "\n",
        "# Run Monte Carlo Optimization for each class\n",
        "monte_carlo_optimizer = MonteCarloEncodingOptimizer(num_simulations=50)\n",
        "best_configs = monte_carlo_optimizer.run_monte_carlo(class_data)\n",
        "\n",
        "# Apply best encoding parameters per class\n",
        "hdc = HyperdimensionalEncoder(dimension=10000)\n",
        "encoded_vectors = []\n",
        "labels = []\n",
        "\n",
        "for label, class_texts in class_data.items():\n",
        "    scaling_factor = best_configs[label][\"scaling_factor\"]\n",
        "    for text in class_texts:\n",
        "        encoded_vectors.append(hdc.encode_text(text) * scaling_factor)\n",
        "        labels.append(label)\n",
        "\n",
        "# Compute similarity matrix\n",
        "num_samples = len(encoded_vectors)\n",
        "similarity_matrix = np.zeros((num_samples, num_samples))\n",
        "\n",
        "for i in range(num_samples):\n",
        "    for j in range(i, num_samples):\n",
        "        similarity = hdc.similarity(encoded_vectors[i], encoded_vectors[j])\n",
        "        similarity_matrix[i, j] = similarity\n",
        "        similarity_matrix[j, i] = similarity\n",
        "\n",
        "# Convert results to a DataFrame for visualization\n",
        "similarity_df = pd.DataFrame(similarity_matrix, columns=[f\"Sample_{i}\" for i in range(num_samples)], index=[f\"Sample_{i}\" for i in range(num_samples)])\n",
        "\n",
        "# Display best Monte Carlo results per class and similarity matrix\n",
        "print(\"Best Monte Carlo Encoding Configurations by Class:\", best_configs)\n",
        "print(similarity_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLVb-owpcx3p",
        "outputId": "2d9f13dc-b64e-4d1d-bb96-b5436da7105d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Monte Carlo Encoding Configurations by Class: {2: {'scaling_factor': 1.415925371292712, 'avg_similarity': 0.11830949756140072}, 3: {'scaling_factor': 1.4872989033196222, 'avg_similarity': 0.1589565012488857}}\n",
            "           Sample_0  Sample_1  Sample_2  Sample_3  Sample_4  Sample_5  \\\n",
            "Sample_0   1.000000  0.127933  0.188860  0.082969  0.042200  0.100987   \n",
            "Sample_1   0.127933  1.000000  0.239341  0.140729  0.052941  0.145976   \n",
            "Sample_2   0.188860  0.239341  1.000000  0.118827  0.154618  0.179992   \n",
            "Sample_3   0.082969  0.140729  0.118827  1.000000  0.085735  0.181728   \n",
            "Sample_4   0.042200  0.052941  0.154618  0.085735  1.000000  0.069536   \n",
            "...             ...       ...       ...       ...       ...       ...   \n",
            "Sample_95  0.148800  0.129457  0.057072  0.147203  0.097482  0.149536   \n",
            "Sample_96  0.202575  0.173435  0.306467  0.097455  0.108580  0.102403   \n",
            "Sample_97  0.076211  0.122522  0.060494  0.133995  0.037401  0.130469   \n",
            "Sample_98  0.089027  0.150077  0.131792  0.126643  0.139291  0.114834   \n",
            "Sample_99  0.041421  0.125760  0.125550  0.134192  0.108493  0.052409   \n",
            "\n",
            "           Sample_6  Sample_7  Sample_8  Sample_9  ...  Sample_90  Sample_91  \\\n",
            "Sample_0   0.110200  0.072800  0.056075  0.699600  ...   0.096400   0.093600   \n",
            "Sample_1   0.157546  0.146254  0.124914  0.106415  ...   0.169050   0.122820   \n",
            "Sample_2   0.276156  0.212534  0.125920  0.171739  ...   0.180617   0.192665   \n",
            "Sample_3   0.096372  0.141899  0.113963  0.051271  ...   0.113391   0.069779   \n",
            "Sample_4   0.086800  0.115800  0.097843  0.038600  ...   0.105400   0.136600   \n",
            "...             ...       ...       ...       ...  ...        ...        ...   \n",
            "Sample_95  0.043588  0.094906  0.121524  0.102636  ...   0.108862   0.107145   \n",
            "Sample_96  0.193284  0.114199  0.114623  0.176862  ...   0.130837   0.184857   \n",
            "Sample_97  0.084233  0.131065  0.137993  0.050193  ...   0.119574   0.030246   \n",
            "Sample_98  0.197648  0.099889  0.136221  0.062617  ...   0.092861   0.210001   \n",
            "Sample_99  0.159376  0.152437  0.150725  0.040159  ...   0.140663   0.142345   \n",
            "\n",
            "           Sample_92  Sample_93  Sample_94  Sample_95  Sample_96  Sample_97  \\\n",
            "Sample_0    0.070000   0.083660   0.099000   0.148800   0.202575   0.076211   \n",
            "Sample_1    0.110889   0.161458   0.147533   0.129457   0.173435   0.122522   \n",
            "Sample_2    0.190128   0.199248   0.190128   0.057072   0.306467   0.060494   \n",
            "Sample_3    0.078714   0.138099   0.114668   0.147203   0.097455   0.133995   \n",
            "Sample_4    0.105400   0.081946   0.069200   0.097482   0.108580   0.037401   \n",
            "...              ...        ...        ...        ...        ...        ...   \n",
            "Sample_95   0.079875   0.067391   0.158463   1.000000   0.119123   0.180864   \n",
            "Sample_96   0.152661   0.177185   0.144666   0.119123   1.000000   0.066409   \n",
            "Sample_97   0.064069   0.074204   0.119357   0.180864   0.066409   1.000000   \n",
            "Sample_98   0.105853   0.159473   0.126512   0.154687   0.179254   0.088316   \n",
            "Sample_99   0.180612   0.115091   0.111227   0.118961   0.115854   0.130608   \n",
            "\n",
            "           Sample_98  Sample_99  \n",
            "Sample_0    0.089027   0.041421  \n",
            "Sample_1    0.150077   0.125760  \n",
            "Sample_2    0.131792   0.125550  \n",
            "Sample_3    0.126643   0.134192  \n",
            "Sample_4    0.139291   0.108493  \n",
            "...              ...        ...  \n",
            "Sample_95   0.154687   0.118961  \n",
            "Sample_96   0.179254   0.115854  \n",
            "Sample_97   0.088316   0.130608  \n",
            "Sample_98   1.000000   0.168938  \n",
            "Sample_99   0.168938   1.000000  \n",
            "\n",
            "[100 rows x 100 columns]\n"
          ]
        }
      ]
    }
  ]
}