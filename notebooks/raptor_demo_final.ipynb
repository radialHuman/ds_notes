{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "912cd8c6-d405-4dfe-8897-46108e6a6af7",
      "metadata": {
        "id": "912cd8c6-d405-4dfe-8897-46108e6a6af7"
      },
      "source": [
        "# RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "699bb15c-f080-4e27-9446-fc5652571a87",
      "metadata": {
        "tags": [],
        "id": "699bb15c-f080-4e27-9446-fc5652571a87"
      },
      "outputs": [],
      "source": [
        "from pypdf import PdfReader\n",
        "from litellm import completion\n",
        "import os\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ebfeb1-b7b3-4a6f-a549-f1a18adf1e94",
      "metadata": {
        "tags": [],
        "id": "77ebfeb1-b7b3-4a6f-a549-f1a18adf1e94",
        "outputId": "9acdefba-997c-4a5b-ef8b-6bab665db263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of pages: 27\n"
          ]
        }
      ],
      "source": [
        "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "FILE_PATH = \"generate_wikipage.pdf\"\n",
        "reader = PdfReader(FILE_PATH)\n",
        "num_of_pages = len(reader.pages)\n",
        "print(f\"Number of pages: {num_of_pages}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a0fe98-f4d8-48d7-bee2-ec11865a42ec",
      "metadata": {
        "tags": [],
        "id": "93a0fe98-f4d8-48d7-bee2-ec11865a42ec"
      },
      "outputs": [],
      "source": [
        "# You can revise how much of the pdf file to use for this demo\n",
        "text = \"\"\n",
        "for page_num in range(num_of_pages):\n",
        "    page = reader.pages[page_num]\n",
        "    text += page.extract_text() + ' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b68a2ba5-7f29-4e70-bbde-c6b19b706ae7",
      "metadata": {
        "tags": [],
        "id": "b68a2ba5-7f29-4e70-bbde-c6b19b706ae7",
        "outputId": "c1835c8c-bd60-4e5f-ac85-13338513999b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assisting in Writing Wikipedia-like Articles From Scratch\n",
            "with Large Language Models\n",
            "Yijia Shao Yuch\n"
          ]
        }
      ],
      "source": [
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d51ebd-5597-4fdd-8c37-32636395081b",
      "metadata": {
        "id": "c7d51ebd-5597-4fdd-8c37-32636395081b"
      },
      "source": [
        "1) **Building**: RAPTOR recursively embeds, clusters, and summarizes chunks of text to construct a tree with varying levels of summarization from the bottom up. You can create a tree from the text in 'sample.txt' using `RA.add_documents(text)`.\n",
        "\n",
        "2) **Querying**: At inference time, the RAPTOR model retrieves information from this tree, integrating data across lengthy documents at different abstraction levels. You can perform queries on the tree with `RA.answer_question`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f58830-9004-48a4-b50e-61a855511d24",
      "metadata": {
        "id": "f4f58830-9004-48a4-b50e-61a855511d24"
      },
      "source": [
        "### Building the tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3753fcf9-0a8e-4ab3-bf3a-6be38ef6cd1e",
      "metadata": {
        "tags": [],
        "id": "3753fcf9-0a8e-4ab3-bf3a-6be38ef6cd1e",
        "outputId": "d2e18f5f-f058-45f6-b14c-c79feb1110e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-20 13:29:18,784 - Loading faiss.\n",
            "2024-03-20 13:29:18,807 - Successfully loaded faiss.\n"
          ]
        }
      ],
      "source": [
        "from raptor import RetrievalAugmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e843edf",
      "metadata": {
        "tags": [],
        "id": "7e843edf",
        "outputId": "f4ebada5-0cb6-4724-9a7a-a4b0d717de17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-20 13:29:22,228 - Successfully initialized TreeBuilder with Config \n",
            "        TreeBuilderConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Max Tokens: 100\n",
            "            Num Layers: 5\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Summarization Length: 100\n",
            "            Summarization Model: <raptor.SummarizationModels.GPT3TurboSummarizationModel object at 0x28911d450>\n",
            "            Embedding Models: {'OpenAI': <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x28f98b990>}\n",
            "            Cluster Embedding Model: OpenAI\n",
            "        \n",
            "        Reduction Dimension: 10\n",
            "        Clustering Algorithm: RAPTOR_Clustering\n",
            "        Clustering Parameters: {}\n",
            "        \n",
            "2024-03-20 13:29:22,228 - Successfully initialized ClusterTreeBuilder with Config \n",
            "        TreeBuilderConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Max Tokens: 100\n",
            "            Num Layers: 5\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Summarization Length: 100\n",
            "            Summarization Model: <raptor.SummarizationModels.GPT3TurboSummarizationModel object at 0x28911d450>\n",
            "            Embedding Models: {'OpenAI': <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x28f98b990>}\n",
            "            Cluster Embedding Model: OpenAI\n",
            "        \n",
            "        Reduction Dimension: 10\n",
            "        Clustering Algorithm: RAPTOR_Clustering\n",
            "        Clustering Parameters: {}\n",
            "        \n",
            "2024-03-20 13:29:22,228 - Successfully initialized RetrievalAugmentation with Config \n",
            "        RetrievalAugmentationConfig:\n",
            "            \n",
            "        TreeBuilderConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Max Tokens: 100\n",
            "            Num Layers: 5\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Summarization Length: 100\n",
            "            Summarization Model: <raptor.SummarizationModels.GPT3TurboSummarizationModel object at 0x28911d450>\n",
            "            Embedding Models: {'OpenAI': <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x28f98b990>}\n",
            "            Cluster Embedding Model: OpenAI\n",
            "        \n",
            "        Reduction Dimension: 10\n",
            "        Clustering Algorithm: RAPTOR_Clustering\n",
            "        Clustering Parameters: {}\n",
            "        \n",
            "            \n",
            "            \n",
            "        TreeRetrieverConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Context Embedding Model: OpenAI\n",
            "            Embedding Model: <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x29159e110>\n",
            "            Num Layers: None\n",
            "            Start Layer: None\n",
            "        \n",
            "            \n",
            "            QA Model: <raptor.QAModels.GPT3TurboQAModel object at 0x2915d9910>\n",
            "            Tree Builder Type: cluster\n",
            "        \n",
            "2024-03-20 13:29:22,238 - Creating Leaf Nodes\n",
            "2024-03-20 13:29:22,511 - Retrying request to /embeddings in 0.984054 seconds\n",
            "2024-03-20 13:29:24,265 - Created 109 Leaf Embeddings\n",
            "2024-03-20 13:29:24,268 - Building All Nodes\n",
            "2024-03-20 13:29:24,339 - Using Cluster TreeBuilder\n",
            "2024-03-20 13:29:24,340 - Constructing Layer 0\n",
            "2024-03-20 13:29:33,369 - Summarization Length: 100\n",
            "2024-03-20 13:29:36,247 - Node Texts Length: 284, Summarized Text Length: 100\n",
            "2024-03-20 13:29:39,129 - Node Texts Length: 387, Summarized Text Length: 100\n",
            "2024-03-20 13:29:42,031 - Node Texts Length: 291, Summarized Text Length: 100\n",
            "2024-03-20 13:29:44,731 - Node Texts Length: 478, Summarized Text Length: 100\n",
            "2024-03-20 13:29:47,686 - Node Texts Length: 285, Summarized Text Length: 100\n",
            "2024-03-20 13:29:50,026 - Node Texts Length: 390, Summarized Text Length: 100\n",
            "2024-03-20 13:29:52,414 - Node Texts Length: 481, Summarized Text Length: 100\n",
            "2024-03-20 13:29:54,738 - Node Texts Length: 195, Summarized Text Length: 100\n",
            "2024-03-20 13:29:57,676 - Node Texts Length: 565, Summarized Text Length: 100\n",
            "2024-03-20 13:30:00,365 - Node Texts Length: 395, Summarized Text Length: 100\n",
            "2024-03-20 13:30:02,925 - Node Texts Length: 563, Summarized Text Length: 100\n",
            "2024-03-20 13:30:06,036 - Node Texts Length: 301, Summarized Text Length: 100\n",
            "2024-03-20 13:30:08,652 - Node Texts Length: 707, Summarized Text Length: 100\n",
            "2024-03-20 13:30:11,278 - Node Texts Length: 289, Summarized Text Length: 100\n",
            "2024-03-20 13:30:14,060 - Node Texts Length: 667, Summarized Text Length: 100\n",
            "2024-03-20 13:30:17,038 - Node Texts Length: 492, Summarized Text Length: 100\n",
            "2024-03-20 13:30:19,662 - Node Texts Length: 92, Summarized Text Length: 100\n",
            "2024-03-20 13:30:22,286 - Node Texts Length: 390, Summarized Text Length: 100\n",
            "2024-03-20 13:30:24,633 - Node Texts Length: 475, Summarized Text Length: 100\n",
            "2024-03-20 13:30:27,366 - Node Texts Length: 278, Summarized Text Length: 100\n",
            "2024-03-20 13:30:30,215 - Node Texts Length: 1036, Summarized Text Length: 100\n",
            "2024-03-20 13:30:32,771 - Node Texts Length: 191, Summarized Text Length: 100\n",
            "2024-03-20 13:30:35,292 - Node Texts Length: 684, Summarized Text Length: 100\n",
            "2024-03-20 13:30:38,019 - Node Texts Length: 294, Summarized Text Length: 100\n",
            "2024-03-20 13:30:40,960 - Node Texts Length: 194, Summarized Text Length: 100\n",
            "2024-03-20 13:30:41,151 - Constructing Layer 1\n",
            "2024-03-20 13:30:42,451 - Summarization Length: 100\n",
            "2024-03-20 13:30:45,586 - Node Texts Length: 206, Summarized Text Length: 100\n",
            "2024-03-20 13:30:47,811 - Node Texts Length: 712, Summarized Text Length: 100\n",
            "2024-03-20 13:30:50,348 - Node Texts Length: 608, Summarized Text Length: 100\n",
            "2024-03-20 13:30:53,255 - Node Texts Length: 408, Summarized Text Length: 100\n",
            "2024-03-20 13:30:56,202 - Node Texts Length: 606, Summarized Text Length: 100\n",
            "2024-03-20 13:30:56,356 - Constructing Layer 2\n",
            "2024-03-20 13:30:56,357 - Stopping Layer construction: Cannot Create More Layers. Total Layers in tree: 2\n",
            "2024-03-20 13:30:56,358 - Successfully initialized TreeRetriever with Config \n",
            "        TreeRetrieverConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Context Embedding Model: OpenAI\n",
            "            Embedding Model: <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x29159e110>\n",
            "            Num Layers: None\n",
            "            Start Layer: None\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "RA = RetrievalAugmentation()\n",
        "\n",
        "# construct the tree\n",
        "RA.add_documents(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f219d60a-1f0b-4cee-89eb-2ae026f13e63",
      "metadata": {
        "id": "f219d60a-1f0b-4cee-89eb-2ae026f13e63"
      },
      "source": [
        "### Querying from the tree\n",
        "\n",
        "```python\n",
        "question = # any question\n",
        "RA.answer_question(question)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b4037c5-ad5a-424b-80e4-a67b8e00773b",
      "metadata": {
        "tags": [],
        "id": "1b4037c5-ad5a-424b-80e4-a67b8e00773b",
        "outputId": "12c1116a-f81a-4612-9e48-ba74ad7041ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-20 13:59:57,205 - Using collapsed_tree\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:  STORM is a writing system designed to aid in the synthesis of topic outlines through retrieval and multi-perspective question asking. It operates by discovering diverse perspectives on a given topic, simulating conversations, gathering conversations from sources, refining outlines, and adding trusted sources. The system aims to address challenges at the pre-writing stage, specifically focusing on how to research a topic and create an outline before starting to write. STORM has shown significant improvements in organization and coverage in article creation, outperforming other baseline models. It follows a multi-stage approach involving generating questions, reading and asking experts, splitting queries, and searching for information to enhance the research capabilities of Large Language Models (LLMs).\n"
          ]
        }
      ],
      "source": [
        "question = \"what is storm?\"\n",
        "\n",
        "answer = RA.answer_question(question=question)\n",
        "\n",
        "print(\"Answer: \", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5be7e57",
      "metadata": {
        "tags": [],
        "id": "f5be7e57",
        "outputId": "c4a688cb-c3e4-4810-f8ea-522e2d59d1bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-20 13:31:22,883 - Tree successfully saved to demo/paper\n"
          ]
        }
      ],
      "source": [
        "# Save the tree by calling RA.save(\"path/to/save\")\n",
        "SAVE_PATH = \"demo/paper\"\n",
        "RA.save(SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e845de9",
      "metadata": {
        "tags": [],
        "id": "2e845de9",
        "outputId": "dc854386-1a05-4312-9a20-72f639435743"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-20 14:01:22,458 - Successfully initialized TreeBuilder with Config \n",
            "        TreeBuilderConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Max Tokens: 100\n",
            "            Num Layers: 5\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Summarization Length: 100\n",
            "            Summarization Model: <raptor.SummarizationModels.GPT3TurboSummarizationModel object at 0x292e36ed0>\n",
            "            Embedding Models: {'OpenAI': <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x29292dcd0>}\n",
            "            Cluster Embedding Model: OpenAI\n",
            "        \n",
            "        Reduction Dimension: 10\n",
            "        Clustering Algorithm: RAPTOR_Clustering\n",
            "        Clustering Parameters: {}\n",
            "        \n",
            "2024-03-20 14:01:22,462 - Successfully initialized ClusterTreeBuilder with Config \n",
            "        TreeBuilderConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Max Tokens: 100\n",
            "            Num Layers: 5\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Summarization Length: 100\n",
            "            Summarization Model: <raptor.SummarizationModels.GPT3TurboSummarizationModel object at 0x292e36ed0>\n",
            "            Embedding Models: {'OpenAI': <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x29292dcd0>}\n",
            "            Cluster Embedding Model: OpenAI\n",
            "        \n",
            "        Reduction Dimension: 10\n",
            "        Clustering Algorithm: RAPTOR_Clustering\n",
            "        Clustering Parameters: {}\n",
            "        \n",
            "2024-03-20 14:01:22,469 - Successfully initialized TreeRetriever with Config \n",
            "        TreeRetrieverConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Context Embedding Model: OpenAI\n",
            "            Embedding Model: <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x29292cf50>\n",
            "            Num Layers: None\n",
            "            Start Layer: None\n",
            "        \n",
            "2024-03-20 14:01:22,479 - Successfully initialized RetrievalAugmentation with Config \n",
            "        RetrievalAugmentationConfig:\n",
            "            \n",
            "        TreeBuilderConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Max Tokens: 100\n",
            "            Num Layers: 5\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Summarization Length: 100\n",
            "            Summarization Model: <raptor.SummarizationModels.GPT3TurboSummarizationModel object at 0x292e36ed0>\n",
            "            Embedding Models: {'OpenAI': <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x29292dcd0>}\n",
            "            Cluster Embedding Model: OpenAI\n",
            "        \n",
            "        Reduction Dimension: 10\n",
            "        Clustering Algorithm: RAPTOR_Clustering\n",
            "        Clustering Parameters: {}\n",
            "        \n",
            "            \n",
            "            \n",
            "        TreeRetrieverConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Context Embedding Model: OpenAI\n",
            "            Embedding Model: <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x29292cf50>\n",
            "            Num Layers: None\n",
            "            Start Layer: None\n",
            "        \n",
            "            \n",
            "            QA Model: <raptor.QAModels.GPT3TurboQAModel object at 0x2a9300990>\n",
            "            Tree Builder Type: cluster\n",
            "        \n",
            "2024-03-20 14:01:22,490 - Using collapsed_tree\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:  STORM is a writing system designed to aid in the synthesis of topic outlines through retrieval and multi-perspective question asking. It operates by discovering diverse perspectives on a given topic, simulating conversations, gathering conversations from sources, refining outlines, and adding trusted sources. The system aims to address challenges at the pre-writing stage, specifically focusing on how to research a topic and create an outline before starting to write. STORM has shown significant improvements in organization and coverage compared to previous work, with a 25% absolute increase in organization and a 10% increase in coverage. It follows a multi-stage approach involving pre-writing and writing stages to create outlines with multi-level section headings, refined using topics, references, and conversations to generate full articles. STORM outperforms other baseline models in article generation and has been evaluated positively by experts for offering more depth in articles compared to traditional sources like Wikipedia.\n"
          ]
        }
      ],
      "source": [
        "# load back the tree by passing it into RetrievalAugmentation\n",
        "\n",
        "RA = RetrievalAugmentation(tree=SAVE_PATH)\n",
        "\n",
        "answer = RA.answer_question(question=question)\n",
        "print(\"Answer: \", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "277ab6ea-1c79-4ed1-97de-1c2e39d6db2e",
      "metadata": {
        "id": "277ab6ea-1c79-4ed1-97de-1c2e39d6db2e"
      },
      "source": [
        "## Using other Open Source Models for Summarization/QA/Embeddings (using HuggingFace)\n",
        "\n",
        "> Note: Please note that this approach is extremely slow on CPU. Instead use frameworks like Ollama for utilizing custom LLMs.\n",
        "\n",
        "If you want to use other models such as Llama or Mistral, you can very easily define your own models and use them with RAPTOR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86cbe7e",
      "metadata": {
        "tags": [],
        "id": "f86cbe7e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from raptor import BaseSummarizationModel, BaseQAModel, BaseEmbeddingModel, RetrievalAugmentationConfig\n",
        "from transformers import AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5cef43",
      "metadata": {
        "tags": [],
        "id": "fe5cef43",
        "outputId": "8c96ea39-f102-47f0-82bb-4becdb713063",
        "colab": {
          "referenced_widgets": [
            "06cb72f5db5c423bb1d20a1b92cf03b9"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06cb72f5db5c423bb1d20a1b92cf03b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# if you want to use the Gemma, you will need to authenticate with HuggingFace, Skip this step, if you have the model already downloaded\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "245b91a5",
      "metadata": {
        "tags": [],
        "id": "245b91a5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "# You can define your own Summarization model by extending the base Summarization Class.\n",
        "class GEMMASummarizationModel(BaseSummarizationModel):\n",
        "    def __init__(self, model_name=\"google/gemma-2b-it\"):\n",
        "        # Initialize the tokenizer and the pipeline for the GEMMA model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.summarization_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model_name,\n",
        "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),  # Use \"cpu\" if CUDA is not available\n",
        "        )\n",
        "\n",
        "    def summarize(self, context, max_tokens=150):\n",
        "        # Format the prompt for summarization\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": f\"Write a summary of the following, including as many key details as possible: {context}:\"}\n",
        "        ]\n",
        "\n",
        "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        # Generate the summary using the pipeline\n",
        "        outputs = self.summarization_pipeline(\n",
        "            prompt,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95\n",
        "        )\n",
        "\n",
        "        # Extracting and returning the generated summary\n",
        "        summary = outputs[0][\"generated_text\"].strip()\n",
        "        return summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a171496d",
      "metadata": {
        "tags": [],
        "id": "a171496d"
      },
      "outputs": [],
      "source": [
        "class GEMMAQAModel(BaseQAModel):\n",
        "    def __init__(self, model_name= \"google/gemma-2b-it\"):\n",
        "        # Initialize the tokenizer and the pipeline for the model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.qa_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model_name,\n",
        "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "        )\n",
        "\n",
        "    def answer_question(self, context, question):\n",
        "        # Apply the chat template for the context and question\n",
        "        messages=[\n",
        "              {\"role\": \"user\", \"content\": f\"Given Context: {context} Give the best full answer amongst the option to question {question}\"}\n",
        "        ]\n",
        "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        # Generate the answer using the pipeline\n",
        "        outputs = self.qa_pipeline(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95\n",
        "        )\n",
        "\n",
        "        # Extracting and returning the generated answer\n",
        "        answer = outputs[0][\"generated_text\"][len(prompt):]\n",
        "        return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "878f7c7b",
      "metadata": {
        "tags": [],
        "id": "878f7c7b"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
        "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def create_embedding(self, text):\n",
        "        return self.model.encode(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "255791ce",
      "metadata": {
        "tags": [],
        "id": "255791ce",
        "outputId": "2fbd1b91-27d9-414b-af02-e6a7234197a5",
        "colab": {
          "referenced_widgets": [
            "c5bcff29db9b46a4abb190159227286f",
            "be62cdc28773474f9c172f94ccc7a61f"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5bcff29db9b46a4abb190159227286f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be62cdc28773474f9c172f94ccc7a61f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "RAC = RetrievalAugmentationConfig(summarization_model=GEMMASummarizationModel(), qa_model=GEMMAQAModel(), embedding_model=SBertEmbeddingModel())\n",
        "RA = RetrievalAugmentation(config=RAC)\n",
        "\n",
        "question = \"what is storm?\"\n",
        "\n",
        "answer = RA.answer_question(question=question)\n",
        "\n",
        "print(\"Answer: \", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98076ea0-a42f-4a8d-96f8-54f8cda51600",
      "metadata": {
        "id": "98076ea0-a42f-4a8d-96f8-54f8cda51600"
      },
      "source": [
        "## Exploring Tree structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07929200-d619-4707-9000-26bc06dd553d",
      "metadata": {
        "tags": [],
        "id": "07929200-d619-4707-9000-26bc06dd553d"
      },
      "outputs": [],
      "source": [
        "tree = RA.tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd006fef-9a93-40e4-897d-26e55e9a58c1",
      "metadata": {
        "tags": [],
        "id": "bd006fef-9a93-40e4-897d-26e55e9a58c1"
      },
      "outputs": [],
      "source": [
        "nodes = tree.all_nodes\n",
        "n_layers = tree.num_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42532d20-9616-4da9-970e-3fbacfb1c965",
      "metadata": {
        "tags": [],
        "id": "42532d20-9616-4da9-970e-3fbacfb1c965",
        "outputId": "1b34f88a-f3ee-4c96-aa40-e081bde5bd46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{134: <raptor.tree_structures.Node at 0x2a5a0ee90>,\n",
              " 135: <raptor.tree_structures.Node at 0x2a5a0f510>,\n",
              " 136: <raptor.tree_structures.Node at 0x2a5a0f690>,\n",
              " 137: <raptor.tree_structures.Node at 0x2a5a0f490>,\n",
              " 138: <raptor.tree_structures.Node at 0x2a5a0e750>}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tree.root_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1bc9d59-4690-47dd-8ba0-d60159a73121",
      "metadata": {
        "tags": [],
        "id": "e1bc9d59-4690-47dd-8ba0-d60159a73121",
        "outputId": "7c768890-cad4-4549-99ad-a39c48ae1119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================= Level 0 ================= \n",
            "Index: 134, Text: The document discusses various papers and presentations related to natural language processing (NLP) and information retrieval. Some key details include:\n",
            "\n",
            "1. Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf presented a paper titled \"FLAIR: An easy-to-use framework for state-of-the-art NLP\" at the 2019 Conference.\n",
            "\n",
            "2. Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Z\n",
            "\n",
            "Index: 135, Text: The STORM research project focuses on the pre-writing stage of creating articles, particularly for the 2022 Winter Olympics opening ceremony. It introduces a method called STORM, involving two stages: pre-writing and writing. In the pre-writing stage, the system creates an outline (O) with multi-level section headings, refined using topics (t), references (R), and conversations to generate the full article (S). Evaluations of the outline are done using metrics like heading soft recall and heading entity recall\n",
            "\n",
            "Index: 136, Text: Researchers conducted a study to assess the implications of RAG baseline on article depth and organization. They identified challenges for future research, including addressing biases in internet-generated articles and preventing LLMs from forming false connections between unrelated facts. The main focus was on evaluating LLM systems' ability to create comprehensive articles from scratch, using three baselines: Direct Gen, RAG, and Outline-driven RAG (oRAG).\n",
            "\n",
            "Evaluation was performed with Prometheus evaluator LLM, considering Interest Level, Coherence,\n",
            "\n",
            "Index: 137, Text: The articles and research studies discussed focus on automatic expository writing, utilizing external documents to generate coherent text. Authors such as Jiang, Parisi, Shuster, Yao, Yang, Feng, and others contribute to understanding the connection between reading and writing. Balepur et al. (2023) introduce the Imitate-Retrieve-Paraphrase framework. Xu et al. (2023) grapple with limitations in detail and hallucinations, especially for long-tail topics highlighted by K\n",
            "\n",
            "Index: 138, Text: The document discusses a research study focused on improving the process of creating high-quality Wikipedia-like articles. The study involves utilizing a dataset of high-quality Wikipedia articles from February 2022 to September 2023, filtered to include B-class quality or above by ORES. Organizational elements such as section and subsection titles are added to the dataset. The dataset excludes list articles and articles without sub-sections. The study aims to simplify the article creation process by focusing only on plain text components.\n",
            "\n",
            "The research involves\n",
            "\n",
            "================= Level 1 ================= \n",
            "Index: 121, Text: The document mentions various papers and their authors along with details of their presentations at different conferences. These papers cover a range of topics related to natural language processing (NLP) and information retrieval. Some specific details include:\n",
            "\n",
            "1. Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf presented a paper titled \"FLAIR: An easy-to-use framework for state-of-the-art NLP\" at the 2019 Conference\n",
            "\n",
            "Index: 123, Text: The provided references cover a range of topics related to computational linguistics, artificial intelligence, and machine translation. \n",
            "\n",
            "1. Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad's work focuses on in-context examples selection for machine translation, presented at the Findings of the Association for Computational Linguistics (ACL) 2023 conference in Toronto, Canada.\n",
            "\n",
            "2. Bernd Bohnet, Vinh Q Tran,\n",
            "\n",
            "Index: 128, Text: The summary outlines the pre-writing stage for an article focused on the 2022 Winter Olympics opening ceremony. Three different question-asking prompts are explored to aid in planning the content of the article. \n",
            "\n",
            "(A) Direct Prompting involves asking 30 questions about the given topic, including details like the date and location of the opening ceremony, as well as the number of participating countries.\n",
            "\n",
            "(B) Perspective-Guided Question Asking prompts the writer to adopt the role of an event planner preparing for the opening ceremony\n",
            "\n",
            "Index: 129, Text: The text introduces a method called STORM, which involves two stages in generating a full-length article. In the pre-writing stage, the system creates an outline (O) consisting of multi-level section headings. This outline is refined in the writing stage using the topic (t), references (R), and conversations to generate the full article (S). The evaluation of the outline is done using two metrics: heading soft recall and heading entity recall. The system uses Sentence-BERT embeddings and FLAIR Named Entity\n",
            "\n",
            "Index: 130, Text: The passage discusses the process of drafting and refining an outline using the STORM method and how reported results are generated. The simulated topic expert in STORM is based on the You.com search API, and the pipeline is stated to be compatible with other search engines. The search results excludes the ground truth Wikipedia article. The focus is on using GPT-4 for final article generation, as GPT-3 has been found to produce less reliable results with citations. It emphasizes the importance of reading relevant information\n",
            "\n",
            "Index: 131, Text: The proposed STORM paradigm aims to enhance the research capabilities of Large Language Models (LLMs) by introducing a novel approach for synthesizing topic outlines through retrieval and multi-perspective question asking. The design of STORM is based on two main hypotheses: (1) diverse perspectives lead to varied questions, and (2) formulating in-depth questions requires iterative research.\n",
            "\n",
            "STORM follows a multi-stage approach where it first retrieves and analyzes Wikipedia articles from similar topics to discover diverse perspectives. It then personifies\n",
            "\n",
            "Index: 132, Text: Researchers introduced STORM, an innovative system designed to automate the pre-writing stage for generating Wikipedia-like articles from scratch. The system utilizes Language Models (LLMs) to search for information and create outlines by posing insightful questions and retrieving reliable data from the Internet. The evaluation was conducted using the FreshWiki dataset, a curated collection of recent, high-quality Wikipedia articles to prevent data leakage during pre-training.\n",
            "\n",
            "To assess the quality of the outlines generated by STORM, the researchers defined metrics for comparison with human-written\n",
            "\n",
            "Index: 133, Text: STORM is a writing system designed to aid in the synthesis of topic outlines through retrieval and multi-perspective question asking. The system aims to address challenges at the pre-writing stage, specifically focusing on how to research a topic and create an outline before starting to write. STORM operates by discovering diverse perspectives on a given topic, simulating conversations, gathering conversations from sources, refining outlines, and adding trusted sources. The process involves steps such as generating questions, reading and asking experts, splitting queries, searching\n",
            "\n",
            "Index: 126, Text: The STORM research project delves into exploring topics by engaging in perspective-guided question asking within simulated conversations as early as the pre-writing stage, a concept defined by Rohman in 1965 as a crucial stage of discovery in the writing process. Prior studies on generating Wikipedia articles have often overlooked this precursor stage, assuming reference information is readily available. Just as stakeholder theory in business recognizes the varied priorities of stakeholders, individuals approaching a topic from distinct perspectives may uncover diverse facets of information. These unique\n",
            "\n",
            "Index: 111, Text: The researchers conducted a study on RAG baseline and its implications on the depth and organization of articles. They highlighted challenges for future research, such as combating biases in internet-generated articles and preventing LLMs from creating false connections between unrelated facts. Their main contributions involved evaluating the capabilities of LLM systems in crafting comprehensive articles from scratch. They used three LLM-based baselines: Direct Gen, RAG, and Outline-driven RAG (oRAG). To push the limits of the RAG baseline\n",
            "\n",
            "Index: 115, Text: The evaluation of the article was done based on five aspects using the Prometheus (Kim et al., 2023) evaluator LLM, which includes Interest Level, Coherence and Organization, Relevance and Focus, Coverage, and Verifiability. The scoring was based on a 5-point rubric developed collaboratively. Comparing the article with human-written articles, the results were as follows:\n",
            "\n",
            "- Interest Level: RAG - 28, oRAG - 44\n",
            "- Organization: R\n",
            "\n",
            "Index: 116, Text: The study conducted compares the performance of STORM, a model generating articles without an outline stage, to one with the outline stage. Results show that removing the outline stage leads to a significant decrease in performance across all metrics. To further analyze STORM's strengths and weaknesses, human evaluation was carried out with the help of 10 experienced Wikipedia editors. The survey revealed that STORM outperforms the best baseline in pairwise comparison. Respondents noted that STORM articles offered more depth than Wikipedia articles. However\n",
            "\n",
            "Index: 117, Text: In a study comparing various models for article generation, STORM was found to have achieved significantly higher performance compared to the best baseline model, oRAG, as indicated by a paired t-test at a significance level of p<0.05. The evaluation metrics showed that STORM outperformed oRAG in aspects such as Heading Soft Recall, Heading Entity Recall, Direct Gen, and 80-23-32-39. Despite oRAG's effectiveness in using outlines for structuring full\n",
            "\n",
            "Index: 118, Text: The text presents the results of an evaluation on the article quality, outline quality, and citation quality using different methods and models. In a series of experiments, various evaluation metrics were used to compare the performance of STORM and its variations with other baselines.\n",
            "\n",
            "Results from Table 2 indicate that STORM achieved varying scores across metrics like 26, 77, 12, and 77. It is noted that STORM, when compared to other baselines, showed significant differences in performance in certain\n",
            "\n",
            "Index: 119, Text: STORM's articles show significant improvements in organization and coverage, with a 25% absolute increase in organization and a 10% increase in coverage compared to previous work. Expert feedback highlights challenges such as source bias transfer and over-association of unrelated facts in generating grounded long articles.\n",
            "\n",
            "Large language models (LLMs) have shown impressive writing capabilities, but addressing verifiability issues like source bias transfer is more nuanced than basic fact-checking. STORM outperforms the oRAG baseline but falls\n",
            "\n",
            "Index: 122, Text: The mentioned articles and research studies focus on the aspect of automatic expository writing, which involves generating coherent and informative text by leveraging external documents and understanding the relationship between reading and writing. Authors such as Jiang, Parisi, Shuster, Yao, Yang, Feng, and others have contributed to the discourse surrounding the process of retrieval, synthesis, and generation of expository texts.\n",
            "\n",
            "Balepur et al. (2023) introduce the Imitate-Retrieve-Paraphrase framework, specifically\n",
            "\n",
            "Index: 124, Text: The approach discussed in the study by Xu et al. (2023) is limited by a lack of details and hallucinations, particularly when addressing long-tail topics as noted by Kandpal et al. (2023). The importance of utilizing external sources is emphasized, with current strategies often involving retrieval-augmented generation (RAG). However, a challenge persists in researching topics during the pre-writing stage due to the inability to retrieve comprehensive information through simple topic searches.\n",
            "\n",
            "Retrieval-Augmented\n",
            "\n",
            "Index: 125, Text: Certainly! Here is a summary that incorporates key details from the references you provided:\n",
            "\n",
            "\"The Craft of Research\" by Laura Dietz and John Foley, published by the University of Chicago Press in 2019, explores the fundamental principles behind conducting effective research. The authors delve into research methodologies, strategies for data collection and analysis, as well as the process of crafting coherent and compelling arguments.\n",
            "\n",
            "In \"TREC CAR Y3: Complex Answer Retrieval Overview\" by Christina S. Doyle from the Proceedings of the\n",
            "\n",
            "Index: 127, Text: The study on human learning theories by Tawfik et al (2020) and Booth et al (2003) emphasizes the importance of asking effective questions in information acquisition. Instruction-tuned models like those discussed by Ouyang et al (2022) can generate questions directly, but they tend to produce basic \"What,\" \"When,\" and \"Where\" questions that often only cover surface-level facts about a topic.\n",
            "\n",
            "The theory of questions and question asking by Ram (1991) points\n",
            "\n",
            "Index: 109, Text: The document discusses a dataset consisting of high-quality Wikipedia articles from February 2022 to September 2023. The articles are filtered to only include those assessed as B-class quality or above by ORES. Organizational elements such as section and subsection titles are added to the dataset, although they do not require citations. The dataset excludes list articles and articles without sub-sections. While typical high-quality Wikipedia articles contain structured data and are multi-modal, only plain text components are considered for dataset construction to simplify\n",
            "\n",
            "Index: 110, Text: The text discusses a study aimed at improving the process of creating new Wikipedia-like articles. It involves writers with various perspectives posing questions to a topic expert based on reliable Internet sources. The information gathered is curated to develop an outline, with FreshWiki dataset of recent high-quality Wikipedia articles utilized for evaluation. Feedback from experienced Wikipedia editors is also collected. The study compares its approach with existing benchmarks for Wikipedia generation, highlighting the focus on longer articles and a broader scope. The process emphasizes the importance of good research skills\n",
            "\n",
            "Index: 112, Text: In the research study by Fan and Gardent (2022), they work under the assumption that an article outline is provided in advance and focus on expanding each section. However, this assumption is not always valid as crafting outlines requires high-level information literacy skills. Automating this process can assist individuals in conducting in-depth research and save time in developing expository articles. The study aims to generate Wikipedia-like articles from scratch, breaking down the process into two main tasks: creating an outline with multi-level sections and\n",
            "\n",
            "Index: 113, Text: The text discusses the generation of high-quality Wikipedia articles by human authors that typically include structured data and multi-modal information. The authors note that while they have not explored generating multi-modal grounded articles yet, they plan to do so in the future. The focus is on generating articles grounded in real-world information rather than creative content. It is emphasized that the goal is to prevent the dissemination of misinformation, and all studies and evaluations are carefully designed to ensure accuracy and avoid publishing generated content online.\n",
            "\n",
            "Ethics are a\n",
            "\n",
            "Index: 114, Text: The study focuses on utilizing large language models to create comprehensive and well-organized long-form articles from scratch, similar to full-length Wikipedia pages. The research highlights the challenges of generating grounded articles that require thorough research on a specific topic. The approach, known as FreshWiki, emphasizes the importance of the pre-writing stage, including tasks such as gathering and curating relevant information. The goal is to develop a system that can generate Wikipedia-like articles by summarizing multiple reference documents on a given topic. The research\n",
            "\n",
            "Index: 120, Text: In the op-ed article co-authored by two experienced Wikipedia editors, the authors discuss their methodology for assessing the verifiability of machine-generated text. They calculate the citation recall and citation precision following the definitions in a study by Gao et al. (2023) and utilize the Mistral 7B-Instruct tool by Jiang et al. (2023a) to determine if cited passages support the generated content. They note challenges in comparing their work to prior studies due to different methodologies and the\n",
            "\n",
            "================= Level 2 ================= \n",
            "Index: 97, Text: Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Lierni Sestorain Saralegui, Tal Schus- ter, William W  Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, and Kellie Webster 2023  Attributed question answering: Evaluation and modeling for attributed large language models Wayne C Booth, Gregory G Colomb, and Joseph M Williams  2003\n",
            "\n",
            "Index: 103, Text: Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu  2023  A survey on hallucination in large lan- guage models: Principles, taxonomy, challenges, and open questions Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\n",
            "\n",
            "Index: 104, Text: Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi- Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave  2023  Atlas: Few-shot learning with retrieval augmented language models  Journal of Machine Learning Research , 24(251):1â€“43 Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- sch, Chris Bamford, Devendra Singh Chaplot, Diego\n",
            "\n",
            "Index: 105, Text: de las Casas, Florian Bressand, Gianna Lengyel, Guil- laume Lample, Lucile Saulnier, et al  2023a  Mistral 7b arXiv preprint arXiv:2310 06825  Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig\n",
            "\n",
            "Index: 107, Text: 15696â€“15707  PMLR Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia  2022  Demonstrate-search- predict: Composing retrieval and language mod- els for knowledge-intensive NLP  arXiv preprint arXiv:2212 14024 \n",
            "\n",
            "Index: 108, Text: Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vard- hamanan, Saiful Haq, Ashutosh Sharma, Thomas T\n",
            "\n",
            "Index: 93, Text: Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland V ollgraf  2019 FLAIR: An easy-to-use framework for state-of-the- art NLP  In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) , pages 54â€“59, Minneapolis, Minnesota  Association for Computational Linguistics\n",
            "\n",
            "Index: 94, Text: Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W Bruce Croft  2019  Asking clari- fying questions in open-domain information-seeking conversations  In Proceedings of the 42nd interna- tional acm sigir conference on research and develop- ment in information retrieval , pages 475â€“484 Nishant Balepur, Jie Huang, and Kevin Chang  2023\n",
            "\n",
            "Index: 96, Text: tics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Pa- pers) , pages 867â€“877, Beijing, China  Association for Computational Linguistics Bernd Bohnet, Vinh Q  Tran, Pat Verga, Roee Aha- roni, Daniel Andor, Livio Baldini Soares, Massimil- iano Ciaramita, Jacob Eisenstein, Kuzman Ganchev,\n",
            "\n",
            "Index: 99, Text: sion at the outline stage: Introducing and encounter- ing issues of sustainable development through aca- demic writing assignments  Text & Talk , 35(2):123â€“ 153 Angela Fan and Claire Gardent  2022  Generating bi- ographies on Wikipedia: The impact of gender bias on the retrieval-based generation of women biogra- phies  In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Vol-\n",
            "\n",
            "Index: 100, Text: ume 1: Long Papers) , pages 8561â€“8576, Dublin, Ireland  Association for Computational Linguistics Xiaocheng Feng, Ming Liu, Jiahao Liu, Bing Qin, Yibo Sun, and Ting Liu  2018  Topic-to-essay generation with neural networks  In IJCAI , pages 4078â€“4084 Tira Nur Fitria  2023  Artificial intelligence (ai) tech-\n",
            "\n",
            "Index: 101, Text: nology in openai chatgpt application: A review of chatgpt in writing english essay  In ELT Forum: Jour- nal of English Language Teaching , volume 12, pages 44â€“58 Pasi FrÃ¤nti and Radu Mariescu-Istodor  2023  Soft preci- sion and recall  Pattern Recognition Letters , 167:115â€“ 121 R Edward Freeman, Jeffrey S Harrison, Andrew C\n",
            "\n",
            "Index: 102, Text: Wicks, Bidhan L Parmar, and Simone De Colle  2010 Stakeholder theory: The state of the art Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen 2023  Enabling large language models to generate text with citations  In Proceedings of the 2023 Con- ference on Empirical Methods in Natural Language Processing , pages 6465â€“6488, Singapore  Associa- tion for Computational Linguistics\n",
            "\n",
            "Index: 106, Text:  2023b  Active retrieval augmented generation  In Proceedings of the 2023 Conference on Empirical Methods in Natural Lan- guage Processing , pages 7969â€“7992, Singapore  As- sociation for Computational Linguistics Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel  2023  Large language models struggle to learn long-tail knowledge  In In- ternational Conference on Machine Learning , pages\n",
            "\n",
            "Index: 92, Text: direction for future work as more topics do not have Wikipedia pages in non-English languages References Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad  2023  In- context examples selection for machine translation InFindings of the Association for Computational Linguistics: ACL 2023 , pages 8857â€“8873, Toronto, Canada  Association for Computational Linguistics\n",
            "\n",
            "Index: 5, Text: and planning in the pre-writing stage (Rohman, (A) Direct Prompting Prompt: Ask 30 questions about the given topic 1 When was the opening ceremony held 2 Where was the opening ceremony held 3 How many countries participated in the opening ceremony (B) Perspective-Guided Question AskingPrompt: You are an event planner who focuses on the preparation of the opening ceremony\n",
            "\n",
            "Index: 6, Text:  â€¦(C) Conversational Question AskingCan you provide me with a list of the participating countries in the 2022 Winter Olympics opening ceremony The 2022 Winter Olympics featured a diverse group of countries participating in the opening ceremony  These included â€¦ Athletes from over 90 countries will enter the stadium in a specific order How is the order of participating countries in the 2022 Winter Olympics opening ceremony determined 1  Can you provide any information about the transportation arrangements for the opening ceremony 2\n",
            "\n",
            "Index: 7, Text:  Can you provide any information about the budget for the 2022 Winter Olympics opening ceremony â€¦ LLM LLM LLM-Role1 LLM-Role2 LLM-Role1TopicReferencesOutlineFull-length ArticleResearch via Question Asking2022 Winter Olympics Opening CeremonyPrewritingWriting Figure 1: We explore writing Wikipedia-like articles from scratch, which demands a pre-writing stage before producing the article  In this stage, simpler approaches like Direct Prompting have limited planning capacity  In\n",
            "\n",
            "Index: 40, Text: ai, and these sources will also be added to Rfor full article generation (Â§3 4) 3 3 Creating the Article Outline After thoroughly researching the topic through N+ 1 simulated conversations, denoted as {C0,C1,  ,CN}, STORM creates an outline before the actual writing starts  To fully leverage the inter- nal knowledge of LLMs, we first prompt the model to generate a draft outline ODgiven only the topic\n",
            "\n",
            "Index: 41, Text: t(Figure 2 7) ODtypically provides a general but organized framework  Subsequently, the LLM is prompted with the topic t, the draft outline OD, and the simulated conversations {C0,C1,  ,CN} to refine the outline (Figure 2 8)  This results in an improved outline Owhich will be used for producing the full-length article 3 4 Writing the Full-Length Article Building upon the references Rcollected and the\n",
            "\n",
            "Index: 42, Text: outline Odeveloped during the pre-writing stage, the full-length article can be composed section by section  Since it is usually impossible to fit the entireRwithin the context window of the LLM, we use the section title and headings of its all-level subsections to retrieve relevant documents from Rbased on semantic similarity calculated from Sentence-BERT embeddings  With the relevant in- formation at hand, the LLM is then prompted to generate the section with citations  Once all sec-\n",
            "\n",
            "Index: 43, Text: tions are generated, they are concatenated to form the full-length article  Since the sections are gen- erated in parallel, we prompt the LLM with the concatenated article to delete repeated information to improve coherence  Furthermore, in alignment with Wikipediaâ€™s stylistic norms, the LLM is also utilized to synthesize a summary of the entire arti- cle, forming the lead section at the beginning 4 Experiments 4 1 Article Selection STORM is capable of researching complicated top-\n",
            "\n",
            "Index: 45, Text: 2, we evaluate the outline qual- ity to assess the pre-writing stage by calculating theheading soft recall andheading entity recall   A higher recall score signifies a more comprehensive outline relative to the human-written article To assess the full-length article quality, we adopt ROUGE scores (Lin, 2004) and compute the entity recall in the article level based on FLAIR NER results  Moreover, based on Wikipedia criteria9,\n",
            "\n",
            "Index: 49, Text: tical to RAG in outline creation, but further searches additional information with section titles to generate the article section by section 4 4 STORM Implementation We build STORM with zero-shot prompting us- ing the DSPy framework (Khattab et al , 2023) Appendix B includes the pseudo code and corre- sponding prompts  The hyperparameters NandM 9https://en wikipedia org/wiki/Wikipedia:\n",
            "\n",
            "Index: 56, Text:  We set temperature as 1 0 and top_p as 0 9 for all experiments 5 Results and Analysis 5 1 Main Results We use outline coverage as a proxy to assess the pre- writing stage (see Â§2 2)  Table 3 shows the heading soft recall and entity recall  Outlines directly gen- erated by LLMs ( Direct Gen ) already demonstrate 10https://documentation you com/api-reference/\n",
            "\n",
            "Index: 27, Text:  Inspired by this, we decompose the generation of Sinto two stages  In the pre-writing stage, we require the system to create an outline O, which is defined as a list of multi-level section headings6  In the writing stage, the system uses the topic t, the references R, and an outline Oto produce the full-length article S To evaluate the outline coverage, we introduce two metrics: heading soft recall andheading en- tity recall \n",
            "\n",
            "Index: 28, Text:  These metrics compare the multi-level section headings of the human-written article, con- sidered as ground truth, and those in O  Recog- nizing that an exact match between elements in these two sets of headings is unnecessary, we cal- culate the heading soft recall (FrÃ¤nti and Mariescu- Istodor, 2023) using cosine similarity derived from Sentence-BERT (Reimers and Gurevych, 2019) em-\n",
            "\n",
            "Index: 29, Text: beddings of the headings (details in Appendix C 1) We also compute the heading entity recall which is quantified as the percentage of named entities in human-written article headings covered by O  We extract entities with FLAIR named entity recogni- tion (NER) (Akbik et al , 2019) 3 Method We present STORM to automate the pre-writing stage by researching a given topic via effective question asking (Â§3 1, Â§3\n",
            "\n",
            "Index: 30, Text: 2) and creating an out- line (Â§3 3)  The outline will be extended to a full- length article grounded on the collected references 5https://en wikipedia org/wiki/Wikipedia: Stand-alone_lists 6Since language models process and produce sequences, we can linearize Oby adding â€œ#â€ to indicate section titles, â€œ##â€ to indicate subsection titles, etc  Related ArticlesTopic ð’•â‘  Surveyâ‘¡ Identify Perspectives Draft Outline ð’ª\n",
            "\n",
            "Index: 64, Text: worse results, indicating reading relevant informa- tion is crucial to generating effective questions  We further examine how many unique sources are col- lected in Rvia different variants  As shown in Ta- ble 5, the full pipeline discovers more different sources and the trend is in accord with the auto- matic metrics for outline quality We also verify whether having an outline stage is necessary with STORM  In Table 2, â€œSTORM\n",
            "\n",
            "Index: 55, Text: drafting and refining the outline (Figure 2 7-8) For reported results, the simulated topic expert in STORM is grounded on the You com search API10, although the proposed pipeline is compatible with other search engines  The ground truth Wikipedia article is excluded from the search results For final article generation, we only report the results using gpt-4 asgpt-3 5 is not faithful to sources when generating text with citations (Gao et al , 2023)\n",
            "\n",
            "Index: 32, Text: identifies various perspectives on covering the topic by surveying related Wikipedia articles ( 1-2)  It then simulates conversations between a Wikipedia writer who asks questions guided by the given perspective and an expert grounded on trustworthy online sources ( 3-6)  The final outline is curated based on the LLMâ€™s intrinsic knowledge and the gathered conversations from different perspectives ( 7 -8 ) (Â§3 4)  Figure 2 gives an overview of STORM and\n",
            "\n",
            "Index: 35, Text: similar topics and uses these perspectives to control the question asking process  Specifically, STORM prompts an LLM to generate a list of related top- ics and subsequently extracts the tables of contents from their corresponding Wikipedia articles, if such articles can be obtained through Wikipedia API7 (Figure 2 1)  These tables of contents are con- catenated to create a context to prompt the LLM to identify Nperspectives P={p1,  , p N}that\n",
            "\n",
            "Index: 14, Text: endow LLMs with the capacity to conduct better research, we propose the STORM paradigm for theSynthesis of Topic Outlines through Retrieval andMulti-perspective Question Asking The design of STORM is based on two hypothe- ses: (1) diverse perspectives lead to varied ques- tions; (2) formulating in-depth questions requires iterative research  Building upon these hypotheses, STORM employs a novel multi-stage approach  It first discovers diverse perspectives by retrieving\n",
            "\n",
            "Index: 15, Text: and analyzing Wikipedia articles from similar top- ics and then personifies the LLM with specific per- spectives for question asking (Figure 1 (B))  Next, to elicit follow-up questions for iterative research(Figure 1 (C)), STORM simulates multi-turn con- versations where the answers to the generated ques- tions are grounded on the Internet  Finally, based on the LLMâ€™s internal knowledge and the collected information, STORM creates an outline that can\n",
            "\n",
            "Index: 57, Text: searchhigh heading soft recall, indicating LLMsâ€™ ability to grasp high-level aspects of a topic through their rich parametric knowledge  However, STORM, by asking effective questions to research the topic, can create higher recall outlines that cover more topic- specific aspects  Notably, although RAG leverages additional information, presenting unorganized in- formation in the context window makes outline generation more challenging for the weaker model, i e , GPT-3 5, leading to worse performance\n",
            "\n",
            "Index: 62, Text:  Ap- pendix C 3 investigates the unsupported sentences and reveals that the primary issues stem from draw- ing improper inferences and inaccurate paraphras- ing, rather than hallucinating non-existent contents 5 2 Ablation Studies As introduced in Â§3, STORM prompts LLMs to ask effective questions by discovering specific perspectives and simulating multi-turn conversa- tions  We conduct the ablation study on outline creation by comparing STORM with two variants:\n",
            "\n",
            "Index: 63, Text: (1) â€œSTORM w/o Perspectiveâ€, which omits per- spective in the question generation prompt; (2) â€œSTORM w/o Conversationâ€, which prompts LLMs to generate a set number of questions altogether  To ensure a fair comparison, we control an equal total number of generated questions across all variants Table 3 shows the ablation results and full STORM pipeline produces outlines with the highest recall Also, â€œSTORM w/o Conversationâ€ gives much\n",
            "\n",
            "Index: 16, Text: be expanded section by section to develop a full- length Wikipedia-like article We evaluate STORM using our FreshWiki dataset (Â§2 1) which curates recent, high-quality Wikipedia articles to avoid data leakage during pre- training 1To facilitate the study of the pre-writing stage, we define metrics for evaluating the outline quality against human-written articles We further invited a group of experienced Wikipedia editors for expert evaluation  The ed- itors found STORM outperforms an outline-driven\n",
            "\n",
            "Index: 18, Text: ticular, we curate the FreshWiki dataset and establish evaluation criteria for both outline and final article quality â€¢We propose STORM, a novel system that au- tomates the pre-writing stage  STORM re- searches the topic and creates an outline by using LLMs to ask incisive questions and re- trieving trusted information from the Internet â€¢Both automatic and human evaluation demon- strate the effectiveness of our approach  Ex- pert feedback further reveals new challenges\n",
            "\n",
            "Index: 86, Text: We propose STORM, an LLM-based writing sys- tem that automates the pre-writing stage for creat- ing Wikipedia-like articles from scratch  We cu- rate the FreshWiki dataset and establish evaluation criteria to study the generation of grounded long- form articles  Experimental results demonstrate that the question asking mechanism in STORM improves both the outline and article quality  With the improved breadth and depth, STORM helps surface new challenges for grounded writing sys- tems through expert evaluation  The experienced\n",
            "\n",
            "Index: 1, Text: and depth to Wikipedia pages  This underex- plored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writ- ing  We propose STORM , a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Ask- ing  STORM models the pre-writing stage by (1) discovering diverse perspectives in research- ing the given topic, (2) simulating conversa-\n",
            "\n",
            "Index: 31, Text: â‘¦ Direct GenerateReferences â„›Question ð’’Answer ð’‚Wikipedia Writerâ‘¢ Read & AskExpertâ‘£ Split Queriesâ‘¤ Search & Siftâ‘¥ Synthesize Add Specific PerspectiveOutline ð’ªGather Conversations {ð’ž\",â€¦,ð’ž#}â‘§ Refineð’«Add Trusted SourcesFigure 2: The overview of STORM that automates the pre-writing stage  Starting with a given topic, STORM\n",
            "\n",
            "Index: 8, Text: contrast, STORM researches the topic via perspective- guided question asking in simulated conversations 1965), even before the actual writing process can start  However, prior work on generating Wikipedia articles (Banerjee and Mitra, 2015; MinguillÃ³n et al , 2017; Liu et al , 2018; Fan and Gardent, 2022) has generally bypassed the pre-writing stage: for instance, Liu et al  (2018) presume reference\n",
            "\n",
            "Index: 33, Text: we include the pseudo code in Appendix B 3 1 Perspective-Guided Question Asking Rohman (1965) defines pre-writing as the stage of discovery in the writing process  In parallel with stakeholder theory in business (Freeman et al , 2010), where diverse stakeholders prioritize vary- ing facets of a company, individuals with distinct perspectives may concentrate on different aspects when researching the same topic and discover mul- tifaceted information  Further, the specific perspec-\n",
            "\n",
            "Index: 34, Text: tives can serve as prior knowledge, guiding individ- uals to ask more in-depth questions  For example, an event planner might ask about the â€œtransporta- tion arrangementsâ€ and â€œbudgetâ€ for â€œthe 2022 Winter Olympics opening ceremonyâ€, whereas a layperson might ask more general questions about the eventâ€™s basic information (Figure 1 (A)) Given the input topic t, STORM discovers differ- ent perspectives by surveying existing articles from\n",
            "\n",
            "Index: 84, Text:  We tackle these chal- lenges by focusing on the pre-writing stage  Question Asking in NLP Question asking capa- bilities in NLP systems have expanded across sev- eral fronts, including generating clarification ques- tions to understand user intents (Aliannejadi et al , 2019; Rahmani et al , 2023), and breaking large questions into smaller ones to improve composi- tional reasoning (Press et al , 2023)\n",
            "\n",
            "Index: 48, Text:  Instead, we use the following three LLM-based baselines 1 Direct Gen , a baseline that directly prompts the LLM to generate an outline, which is then used to generate the full-length article 2 RAG , a retrieval-augmented generation base- line that searches with the topic and uses the searched results together with the topic tto generate an outline or the entire article 3 Outline-driven RAG (oRAG) , which is iden-\n",
            "\n",
            "Index: 17, Text: RAG baseline, especially regarding the breadth and organization of the articles  They also identified challenges for future research, including address- ing cases where: (1) the bias on the Internet affects the generated articles; (2) LLMs fabricate connec- tions between unrelated facts  These challenges present new frontiers to grounded writing systems Our main contributions include: â€¢To evaluate the capacity of LLM systems at generating long-form grounded articles from scratch, and the pre-writing challenge in par-\n",
            "\n",
            "Index: 58, Text:  To test the limit of the RAG baseline, we further expand the retrieved sources by starting with the outline produced by RAG , using its section titles as search queries to collect more sources, and inputting the newly collected sources together with the initial outline to LLM to generate a polished outline  This modified approach is referred to as â€œ RAG-expand â€ in Table 3  The experiment results indicate that even though having an additional round of search and refinement can improve the outline produced\n",
            "\n",
            "Index: 66, Text: 03 70 0% 0 077 Organization 3 25 45 0% 4 00 70 0% 0 005 Relevance 3 93 62 5% 4 15 65 0% 0 347 Coverage 3 58 57 5% 4 00 67 5% 0 084 Verifiability 3 85 67 5% 3 80 67\n",
            "\n",
            "Index: 71, Text: nization ratingâ‰¥4), and 10% more are deemed to have good coverage ( Coverage ratingâ‰¥4)  Even in comparison with human-written articles, one editor praises our result as providing â€œa bit more 11For the 1-7 scale rating results on each criterion, we cal- culate the Krippendorffâ€™s Alpha to measure the inter annotator agreement (IAA), and the results are as follows: Interest Level (0 349), Organization (0\n",
            "\n",
            "Index: 72, Text: 221), Relevance (0 256), Coverage (0 346), Verifiability (0 388)  I think it can be specifically helpful for my pre-writing stage I think it will help me edit a Wikipedia article for a new topic I think it can be a potentially useful tool for the Wikipedia community Strongly AgreeSomewhat AgreeSomewhat DisagreeStrongly Disagree 30%50%20%30%70%\n",
            "\n",
            "Index: 46, Text: we evaluate the article from the aspects of (1) In- terest Level , (2) Coherence and Organization , (3) Relevance and Focus , (4)Coverage , and (5) Verifia- bility   For aspects (1)-(4), we use Prometheus (Kim et al , 2023), a 13B evaluator LLM to score the arti- cle based on a 5-point rubric collaboratively devel-\n",
            "\n",
            "Index: 50, Text: Good_article_criteria Comparsion with Human-written Articles Rubric Grading ROUGE-1 ROUGE-L Entity Recall Interest Level Organization Relevance Coverage Direct Gen 25 62 12 63 5 08 2 87 4 60 3 10 4 16 RAG 28 52 13 18 7 57 3 14 4 22 3 05 4 08 oRAG 44 26 16\n",
            "\n",
            "Index: 65, Text: w/o Outline Stageâ€ denotes the results of generat- ing the entire article given the topic and the sim- ulated conversations  Removing the outline stage significantly deteriorates the performance across all metrics 6 Human Evaluation To better understand the strengths and weaknesses of STORM, we conduct human evaluation by col- laborating with 10 experienced Wikipedia editorsoRAG STORMp-valueAvg  â‰¥4Rates Av g  â‰¥4Rates Interest Level 3 63 57 5% 4\n",
            "\n",
            "Index: 73, Text: 10%60%20%10%NeutralFigure 3: Survey results of the perceived usefulness of STORM ( n= 10 ) background informationâ€ and another notes that â€œI found that the AI articles had more depth compared to the Wikipedia articlesâ€  STORM also outper- forms the best baseline in pairwise comparison More information in |R|poses challenges be- yond factual hallucination  We examine 14 pair- wise comparison responses where editors prefer\n",
            "\n",
            "Index: 67, Text: 5% 0 843 #Preferred 14 26 Table 6: Human evaluation results on 20 pairs of articles generated by STORM and oRAG   Each pair of articles is evaluated by two Wikipedia editors  The ratings are given on a scale between 1 and 7, with values â‰¥4 indicating good quality (see Table 10)  We conduct paired t-test and report the p-value who have made at least 500 edits on Wikipedia and\n",
            "\n",
            "Index: 68, Text: have more than 1 year of experience  We randomly sample 20 topics from our dataset and evaluate the articles generated by our method and oRAG , the best baseline according to the automatic evaluation Each pair of articles is assigned to 2 editors We request editors to judge each article from the same five aspects defined in Â§4 2, but using a 1 to 7 scale for more fine-grained evaluation  While our automatic evaluation uses citation quality as\n",
            "\n",
            "Index: 70, Text: tails are included in Appendix D  Table 6 presents the rating and pairwise comparison results 11 Articles produced by STORM exhibit greater breadth and depth than oRAG outputs  In ac- cord with the finding in Â§5 1, editors judge articles produced by STORM as more interesting, orga- nized, and having broader coverage compared to oRAG outputs  Specifically, 25% more articles pro- duced by STORM are considered organized ( Orga-\n",
            "\n",
            "Index: 74, Text: oRAG outputs over STORM  Excluding 3 cases where pairwise preferences do not align with their ratings, editors assign lower Verifiability scores to articles from our approach in over 50% of the cases Through analyzing the articles and editorsâ€™ free- form feedback, we discover that low Verifiability scores stem from red herring fallacy or overspec- ulation issues   These arise when the generated articles introduce unverifiable connections between\n",
            "\n",
            "Index: 52, Text:  â€ denotes significant differences ( p <0 05) from a paired t-test between STORM and the best baseline, i e , oRAG  The rubric grading uses a 1-5 scale Heading Soft RecallHeading Entity Recall GPT-3 5Direct Gen 80 23 32 39 RAG/oRAG 73 59 33 85 RAG-expand 74 40 33 85 STORM 86\n",
            "\n",
            "Index: 59, Text: byRAG , our proposed STORM still surpasses its performance We further evaluate the full-length article quality As shown in Table 2, oRAG significantly outper- forms RAG , highlighting the effectiveness of using outlines for structuring full-length article genera- tion  Despite this methodâ€™s advantages in leverag- ing retrieval and outlining, our approach still out- performs it  The effective question asking mecha- nism enhances the articles with greater entity recall\n",
            "\n",
            "Index: 53, Text: 26 â€  40 52 â€  w/o Perspective 84 49 40 12 w/o Conversation 77 97 31 98 GPT-4Direct Gen 87 66 34 78 RAG/oRAG 89 55 42 38 RAG-expand 91 36 43 53 STORM 92 73 â€  45 91 w/o Perspective 92 39 42 70 w/o Conversation 88\n",
            "\n",
            "Index: 51, Text: 51 12 57 3 90 4 79 4 09 4 70 STORM 45 82 16 70 14 10 â€  3 99â€  4 82 4 45â€  4 88â€  w/o Outline Stage 26 77 12 77 7 39 3 33 4 87 3 35 4 37 Table 2: Results of automatic article quality evaluation\n",
            "\n",
            "Index: 61, Text:  As reported Citation Recall Citation Precision STORM 84 83 85 18 Table 4: Citation quality judged by Mistral 7B-Instruct STORM w/o Perspective w/o Conversation |R| 99 83 54 36 39 56 Table 5: Average number of unique references ( |R|) collected using different methods in Table 4, Mistral 7B-Instruct judges 84 83% of the sentences are supported by their citations\n",
            "\n",
            "Index: 54, Text: 75 39 30 Table 3: Results of outline quality evaluation (%)  â€ de- notes significant differences ( p <0 05) from a paired t-test between STORM and baselines in STORM are both set as 5  We use the chat modelgpt-3 5-turbo for question asking and usegpt-3 5-turbo-instruct for other parts of STORM  We also experiment with using gpt-4 for\n",
            "\n",
            "Index: 3, Text: STORMâ€™s articles are deemed to be organized (by a 25% absolute increase) and broad in cov- erage (by 10%)  The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts 1 Introduction Large language models (LLMs) have demonstrated impressive writing capabilities (Yang et al , 2023;\n",
            "\n",
            "Index: 75, Text: different pieces of information in |R|or between the information and the topic (examples included in Table 11)  Compared to the widely discussed factual hallucination (Shuster et al , 2021; Huang et al , 2023), addressing such verifiability issues is more nuanced, surpassing basic fact-checking (Min et al , 2023) Generated articles trail behind well-revised hu- man works  While STORM outperforms the\n",
            "\n",
            "Index: 76, Text: oRAG baseline, editors comment that the generated articles are less informative than actual Wikipedia pages   Another major issue identified is the trans- fer of bias and tone from Internet sources to the generated article , with 7 out of 10 editors men- tioning that the STORM-generated articles sound â€œemotionalâ€ or â€œunneutralâ€  More analysis is dis- cussed in Appendix E  This feedback suggests that reducing the retrieval bias in the pre-writing stage\n",
            "\n",
            "Index: 77, Text: is a worthwhile direction for future work Generated articles are a good starting point  As shown in Figure 3, editors are unanimous in agree- ing that STORM can aid them in their pre-writing stage  It is gratifying to know that the tool is help- ful to experienced editors  80% of the editors think that STORM can help them edit a Wikipedia articlefor a new topic  More reservation is expressed to the usefulness of STORM for the Wikipedia com-\n",
            "\n",
            "Index: 87, Text: Wikipedia editors in our study unanimously agree that STORM is helpful for their pre-writing stage Limitations In this work, we explore generating Wikipedia- like articles from scratch as a way to push the frontier of automatic expository writing and long- form article generation  While our approach sig- nificantly outperforms baseline methods in both automatic and human evaluations, the quality of machine-written articles still lags behind well- revised human-authored articles, specifically in aspects of neutrality and verifiability  Although\n",
            "\n",
            "Index: 88, Text: STORM discovers different perspectives in re- searching the given topic, the collected information may still be biased towards dominant sources on the Internet and may contain promotional content Moreover, the verifiability issues identified in this work go beyond factual hallucination, which high- lights new challenges to grounded writing systems Another limitation of this work is that although we focus on the task of generating Wikipedia-like articles from scratch, our task setup is still simpli-fied to only consider the generation of free-form text\n",
            "\n",
            "Index: 82, Text: time retrieval before generation, the system can be designed to self-decide when to retrieve across the course of the generation (Jiang et al , 2023b; Parisi et al , 2022; Shuster et al , 2022; Yao et al , 2023) Automatic Expository Writing Different from other types of long-form generation (Yang et al , 2022; Feng et al , 2018), automatic expository writ-\n",
            "\n",
            "Index: 83, Text: ing requires grounding on external documents and leveraging the interplay between reading and writ- ing  Balepur et al  (2023) propose the Imitate- Retrieve-Paraphrase framework for expository writ- ing at the paragraph level to address the challenges in synthesizing information from multiple sources Beyond summarizing sources, Shen et al  (2023) highlight that expository writing requires the au- thorâ€™s sensemaking process over source documents and good outline planning\n",
            "\n",
            "Index: 95, Text: Expository text generation: Imitate, retrieve, para- phrase  In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Process- ing, pages 11896â€“11919, Singapore  Association for Computational Linguistics Siddhartha Banerjee and Prasenjit Mitra  2015 WikiKreator: Improving Wikipedia stubs automat- ically  In Proceedings of the 53rd Annual Meet- ing of the Association for Computational Linguis-\n",
            "\n",
            "Index: 12, Text: ever, this approach is limited by a lack of details and hallucinations (Xu et al , 2023), particularly in addressing long-tail topics (Kandpal et al , 2023) This underscores the importance of leveraging ex- ternal sources, and current strategies often involve retrieval-augmented generation ( RAG ), which cir- cles back to the problem of researching the topic in the pre-writing stage, as much information cannot be surfaced through simple topic searches\n",
            "\n",
            "Index: 78, Text: munity at large; nonetheless, 70% of the editors think it is useful, with only 10% disagreeing 7 Related Works Retrieval-Augmented Generation (RAG) Aug- menting language models (LMs) with retrieval at inference time is a typical way to leverage exter- nal knowledge stores (Ram et al , 2023; Izacard et al , 2023)  While some works use retrieval to construct demonstrations for in-context learn-\n",
            "\n",
            "Index: 79, Text: ing (Li et al , 2023; Liu et al , 2022; Agrawal et al , 2023; Poesia et al , 2022; Shi et al , 2022; Khattab et al , 2022), another line of works uses retrieval to provide additional information for LMs to ground on  Lewis et al  (2020) study RAG on knowledge- intensive NLP tasks and find it improves diver-\n",
            "\n",
            "Index: 80, Text: sity and factuality  Semnani et al  (2023) de- signs a RAG-based chatbot grounded on English Wikipedia to stop LLM-based chatbots from hal- lucination  Besides, RAG can be used to generate text with citations (Menick et al , 2022; Gao et al , 2023) and build attributed question answering sys- tems (Bohnet et al , 2023)  While RAG is widely\n",
            "\n",
            "Index: 81, Text: studied in question answering, how to use it for long-form article generation is less investigated As a general framework, RAG is flexible in both the retrieval source and time  The retrieval sources can vary from domain databases (Zakka et al , 2023), code documentation (Zhou et al , 2023), to the whole Internet (Nakano et al , 2022; Komeili et al , 2022)  Regarding the time, besides a one-\n",
            "\n",
            "Index: 98, Text:  The craft of research   University of Chicago press Laura Dietz and John Foley  2019  Trec car y3: Com- plex answer retrieval overview  In Proceedings of Text REtrieval Conference (TREC)  Christina S Doyle  1994  Information literacy in an information society: A concept for the information age  Diane Publishing Ann-Marie Eriksson and Ã…sa MÃ¤kitalo  2015  Supervi-\n",
            "\n",
            "Index: 37, Text: 2 Simulating Conversations The theory of questions and question asking (Ram, 1991) highlights that while answers to existing questions contribute to a more comprehensive understanding of a topic, they often simultane- ously give rise to new questions  To kick off this dynamic process, STORM simulates a conversa- tion between a Wikipedia writer and a topic ex- pert  In the i-th round of the conversation, the LLM-powered Wikipedia writer generates a sin-\n",
            "\n",
            "Index: 38, Text: gle question qibased on the topic t, its assigned perspective pâˆˆ P, and the conversation history {q1, a1,  , q iâˆ’1, aiâˆ’1}where ajdenotes the sim- ulated expertâ€™s answer  The conversation history enables the LLM to update its understanding of the topic and ask follow-up questions  In practice, we limit the conversation to at most Mrounds To ensure that the conversation history provides\n",
            "\n",
            "Index: 39, Text: factual information, we use trusted sources from the Internet to ground the answer aito each query qi  Since qican be complicated, we first prompt the LLM to break down qiinto a set of search queries (Figure 2 4) and the searched results will be evaluated using a rule-based filter according to the Wikipedia guideline8to exclude untrustworthy sources (Figure 2 5)  Finally, the LLM synthe- sizes the trustworthy sources to generate the answer\n",
            "\n",
            "Index: 13, Text: Human learning theories (Tawfik et al , 2020; Booth et al , 2003) highlight asking effective questions in information acquisition  Although instruction-tuned models (Ouyang et al , 2022) can be prompted directly to generate questions, we find that they typically produce basic â€œWhatâ€, â€œWhenâ€, and â€œWhereâ€ questions (Figure 1 (A)) which often only address surface-level facts about the topic  To\n",
            "\n",
            "Index: 85, Text:  While humans usually ask questions to learn new knowledge (Taw- fik et al , 2020; Booth et al , 2003), how to opti- mize question informativeness and specificity in information-seeking conversations remains less ex- plored  The closest work is Qi et al  (2020) which defines the question informativeness using the un- igram precision function and uses reinforcement learning to increase the question informativeness 8 Conclusion\n",
            "\n",
            "Index: 24, Text: each month from February 2022 to September 20233  To ensure high-quality references, we filter these articles to keep only those having B-class quality or above assessed by ORES4  We also ex- 2In practice, Salso includes organizational elements such as section and subsection titles, which do not require citations 3Obtained from https://wikimedia org/api/rest_v1/metrics/edited-pages/ top-by-edits/en wikipedia/all-editor-types/\n",
            "\n",
            "Index: 25, Text: content/ {year}/{month}/all-days 4https://www mediawiki org/wiki/ORESclude list articles5and articles that have no sub- sections  While high-quality Wikipedia articles usually contain structured data ( e g , tables) and are multi-modal, we only consider the plain text com- ponent in constructing the dataset to simplify our task  More details of the dataset are in Appendix A 2 2 Outline Creation and Evaluation\n",
            "\n",
            "Index: 44, Text: ics and writing long articles from detailed outlines However, in this controlled experiment, we limit the final output to at most 4000 tokens (roughly 3000 words)  For a meaningful comparison, we 8https://en wikipedia org/wiki/Wikipedia: Reliable_sourcesrandomly select 100 samples from the FreshWiki dataset (see Â§2 1) that have human-written articles not exceeding 3000 words 4 2 Automatic Metrics As discussed in Â§2\n",
            "\n",
            "Index: 2, Text: tions where writers carrying different perspec- tives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the col- lected information to create an outline For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage  We further gather feedback from experienced Wikipedia editors  Com- pared to articles generated by an outline- driven retrieval-augmented baseline, more of\n",
            "\n",
            "Index: 36, Text: 7https://pypi org/project/Wikipedia-API/can collectively contribute to a comprehensive ar- ticle on t(Figure 2 2)  To ensure that the basic information about tis also covered, we add p0as â€œbasic fact writer focusing on broadly covering the basic facts about the topicâ€ into P  Each perspec- tivepâˆˆ P will be utilized to guide the LLM in the process of question asking in parallel 3\n",
            "\n",
            "Index: 21, Text: tors to view Wikipedia article writing as an educa- tional exercise for academic training (Tardy, 2010) Table 1 compares our work against prior bench- marks for Wikipedia generation  Existing work has generally focused on evaluating the generation of shorter snippets ( e g , one paragraph), within a narrower scope ( e g , a specific domain or two), or when an explicit outline or reference documents are supplied  A notable example is WikiSum (Liu et al\n",
            "\n",
            "Index: 23, Text: Creating a new Wikipedia-like article demands not only fluent writing but also good research skills  As modern LLMs are generally trained on Wikipedia text, we mitigate data leakage by explicitly seeking outrecent Wikipedia articles that were created (or very heavily edited) after the training cutoff of the LLMs we test  Our process can be repeated at future dates when new LLMs emerge To apply our date criteria, we focus on the top 100 most-edited pages, based on edit counts, for\n",
            "\n",
            "Index: 9, Text: documents are provided in advance, while Fan and Gardent (2022) assume an article outline is avail- able and focus on expanding each section  These assumptions do not hold in general, as collecting references and crafting outlines demand advanced information literacy skills (Doyle, 1994) to iden-arXiv:2402 14207v1  [cs CL]  22 Feb 2024 tify, evaluate, and organize external sources - a task\n",
            "\n",
            "Index: 10, Text: that is challenging even for experienced writers Automating this process can facilitate individuals in initiating in-depth learning about a topic and greatly reduce the expensive expert hours neces- sary for their expository writing We explore these challenges by focusing on how to generate Wikipedia-like articles from scratch  We decompose this problem into two tasks  The first is to conduct research to generate an outline, i e , a list of multi-level sections, and collect a set of reference documents  The second uses the outline\n",
            "\n",
            "Index: 11, Text: and the references to produce the full-length arti- cle  Such a task decomposition mirrors the human writing process which usually includes phases of pre-writing, drafting, and revising (Rohman, 1965; Munoz-Luna, 2015) As pre-trained language models inherently pos- sess a wealth of knowledge, a direct approach is to rely on their parametric knowledge for generating outlines or even entire articles ( Direct Gen )  How-\n",
            "\n",
            "Index: 20, Text:  (2023) All One para  / No Fan and Gardent (2022) One Full article Yes No Liu et al  (2018) All One para  / Yes Sauper and Barzilay (2009) Two Full article No No Ours All Full article No No Table 1: Comparison of different Wikipedia generation setups in existing literature  Generating one paragraph does not need an article outline writing approach which has prompted some educa-\n",
            "\n",
            "Index: 26, Text: A full-length article is hard to generate or evalu- ate (Xu et al , 2023; Krishna et al , 2023)  When human educators teach students academic writing, they sometimes supervise students at the outline stage (Eriksson and MÃ¤kitalo, 2015) because an extensive outline indicates a comprehensive under- standing of the topic and provides a solid founda- tion for writing the full-length article (Dietz and Foley, 2019)\n",
            "\n",
            "Index: 89, Text:  Human-authored high-quality Wikipedia ar- ticles usually contain structured data and multi- modal information  We leave the exploration of generating multi-modal grounded articles for fu- ture work Ethics Statement Different from the creative generation, grounded ar- ticle generation may impact how people learn about topics or consume source information  All the stud- ies and the evaluation in this work are designed to prevent the dissemination of misinformation by not publishing generated content online and im- plementing strict accuracy checks\n",
            "\n",
            "Index: 90, Text:  We avoid any disruption to Wikipedia or related communities, as our system does not interact with live pages  Also, although we try to generate grounded articles, we believe there is no privacy issue related to this work as we only use information publicly available on the Internet The primary risk of our work is that the Wikipedia articles written by our system are grounded on information on the Internet which contains some biased or discriminative content on its own  Currently, our system relies on the search\n",
            "\n",
            "Index: 91, Text: engine to retrieve information but does not include any post-processing module  We believe improv- ing the retrieval module to have good coverage of different viewpoints and adding a content sifting module to the current system will be a critical next step to achieve better neutrality and balance in the generated articles Another limitation we see from an ethical point of view is that we only consider writing English Wikipedia articles in this work  Extending the cur- rent system to a multilingual setup is a meaningful\n",
            "\n",
            "Index: 0, Text: Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models Yijia Shao Yucheng Jiang Theodore A  Kanell Peter Xu Omar Khattab Monica S  Lam Stanford University {shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford edu lam@cs stanford edu Abstract We study how to apply large language models to write grounded and organized long-form ar- ticles from scratch, with comparable breadth\n",
            "\n",
            "Index: 19, Text: in generating grounded long-form articles 2 FreshWiki We study generating Wikipedia-like articles from scratch, placing emphasis on the pre-writing stage (Rohman, 1965), which involves the demand- ing sub-tasks of gathering and curating relevant information (â€œresearchâ€)  This models the human 1Our resources and code will be publicly released upon publication  Domain ScopeGiven Outline Given Refs Balepur et al  (2023) One One para  / Yes Qian et al\n",
            "\n",
            "Index: 4, Text: Pavlik, 2023; Wenzlaff and Spaeth, 2022; Fitria, 2023), but it is unclear how we can use them to write grounded, long-form articles, like full-length Wikipedia pages  Such expository writing, which seeks to inform the reader on a topic in an or- ganized manner (Weaver III and Kintsch, 1991; Balepur et al , 2023), requires thorough research\n",
            "\n",
            "Index: 22, Text: , 2018), which treats generating Wikipedia ar- ticles as a multi-document summarization problem, with respect to the reference documents Our setup emphasizes the capability of long- form grounded writing systems to research and curate content  Specifically, given a topic t, the task is to find a set of references Rand generate a full-length article S=s1s2 sn, where each sentence sicites a list of documents in R 2 2 1 The FreshWiki Dataset\n",
            "\n",
            "Index: 60, Text: The evaluator LLM also rates these articles with sig- nificantly higher scores in the aspects of â€œInterest Levelâ€, â€œRelevance and Focusâ€, and â€œCoverageâ€ Nonetheless, we acknowledge the possibility of the evaluator LLM overrating machine-generated text  Our careful human evaluation (Â§6) reveals that STORM still has much room for improvement Although this work primarily focuses on the pre- writing stage and does not optimize generating text with citations, we still examine the citation quality of articles produced by our approach\n",
            "\n",
            "Index: 69, Text: a proxy to evaluate Verifiability , we stick to the Wikipedia standard of â€œverifiable with no original researchâ€ in human evaluation  Besides rating the articles, editors are asked to provide open-ended feedback and pairwise preference  After the evalua- tion finishes, they are further requested to compare an article produced by our method, which they have just reviewed, with its human-written counterpart, and report their perceived usefulness of STORM using a 1-5 Likert scale  More human evaluation de-\n",
            "\n",
            "Index: 47, Text: oped with two experienced Wikipedia editors (see Appendix C 2)  For verifiability, we calculate the citation recall andcitation precision based on the definition in Gao et al  (2023)  We use Mistral 7B- Instruct (Jiang et al , 2023a) to examine whether the cited passages entail the generated sentence 4 3 Baselines As prior works use different setups and do not use LLMs, they are hard to compare directly\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def print_tree_layers(root_nodes):\n",
        "    \"\"\"\n",
        "    Iterates over the tree from the root nodes and prints node index and text layer by layer.\n",
        "\n",
        "    Args:\n",
        "      root_nodes: A dictionary mapping node index to Node objects.\n",
        "    \"\"\"\n",
        "\n",
        "    all_nodes = tree.all_nodes\n",
        "    current_layer = list(root_nodes.values())  # Convert root_nodes to a list for iteration\n",
        "    level = 0\n",
        "    while current_layer:\n",
        "        print(f\"================= Level {level} ================= \")\n",
        "        next_layer = []\n",
        "        for node in current_layer:\n",
        "            print(f\"Index: {node.index}, Text: {node.text}\\n\")\n",
        "            next_layer.extend(all_nodes.get(child_index) for child_index in node.children)\n",
        "\n",
        "        current_layer = next_layer\n",
        "        level += 1\n",
        "\n",
        "print_tree_layers(tree.root_nodes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "260f657b-7de3-4c24-90c6-bfedfc4f9066",
      "metadata": {
        "id": "260f657b-7de3-4c24-90c6-bfedfc4f9066"
      },
      "source": [
        "## Generating an image from the tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b25b2626-bcd6-4cba-9da9-5bfe7b53ac7f",
      "metadata": {
        "tags": [],
        "id": "b25b2626-bcd6-4cba-9da9-5bfe7b53ac7f",
        "outputId": "134717c6-159f-4d77-972d-2c0dfeefa80b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'tree_graph.png'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make sure you have installed graphviz and it's in your system path\n",
        "\n",
        "from graphviz import Digraph\n",
        "from typing import Set\n",
        "\n",
        "def create_graph(tree):\n",
        "    # dot = Digraph(engine='neato' if layout != 'dot' else 'dot')  # Change layout algorithm if not dot\n",
        "\n",
        "    dot = Digraph()\n",
        "\n",
        "    # Add nodes\n",
        "    for index, node in tree.all_nodes.items():\n",
        "        dot.node(str(index), label=str(index))\n",
        "\n",
        "    # Add edges\n",
        "    for index, node in tree.all_nodes.items():\n",
        "        for child_index in node.children:\n",
        "            dot.edge(str(index), str(child_index))\n",
        "\n",
        "    # print(dot)\n",
        "    return dot\n",
        "\n",
        "# Create and display the graph\n",
        "graph = create_graph(tree)\n",
        "graph.attr(layout='dot')  # twopi, dot, fdp, sfdp, neato, and twopi.\n",
        "graph.render('tree_graph', format='png', cleanup=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47c2678-9284-4693-968a-9c41bb9aa763",
      "metadata": {
        "id": "d47c2678-9284-4693-968a-9c41bb9aa763",
        "outputId": "0aedb78d-03d2-4b38-c32b-ac7ca2cce1cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m13:35:26 - LiteLLM:INFO\u001b[0m: \u001b[92m\n",
            "\n",
            "POST Request Sent from LiteLLM:\n",
            "curl -X POST \\\n",
            "http://localhost:11434/api/generate \\\n",
            "-d '{'model': 'gemma:2b', 'prompt': 'who are you?', 'options': {}, 'stream': True}'\n",
            "\u001b[0m\n",
            "\n",
            "2024-03-20 13:35:26,137 - \u001b[92m\n",
            "\n",
            "POST Request Sent from LiteLLM:\n",
            "curl -X POST \\\n",
            "http://localhost:11434/api/generate \\\n",
            "-d '{'model': 'gemma:2b', 'prompt': 'who are you?', 'options': {}, 'stream': True}'\n",
            "\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am a large language model, trained by Google. I am a conversational AI that can engage in human-like conversations on a wide range of topics.\n",
            "\n",
            "**Here are some of my capabilities:**\n",
            "\n",
            "* Natural language processing (NLP)\n",
            "* Natural language generation (NLG)\n",
            "* Machine learning\n",
            "* Knowledge base access\n",
            "* Question answering\n",
            "* Summarization\n",
            "* Translation\n",
            "* Storytelling\n",
            "\n",
            "I am still under development, but I am constantly learning and improving. I am here to assist you with your queries and provide you with information and entertainment."
          ]
        }
      ],
      "source": [
        "import litellm\n",
        "\n",
        "# If you'd want to log the output\n",
        "def my_custom_logging_fn(model_call_dict):\n",
        "    print(f\"model call details: {model_call_dict}\")\n",
        "\n",
        "# Streaming response generation\n",
        "def generate_responses():\n",
        "    response = completion(\n",
        "        model=\"ollama/gemma:2b\",\n",
        "        messages=[{ \"content\": \"who are you?\", \"role\": \"user\"}],\n",
        "        api_base=\"http://localhost:11434\",\n",
        "        stream=True,\n",
        "        # logger_fn=my_custom_logging_fn\n",
        "    )\n",
        "    for chunk in response:\n",
        "        content = chunk['choices'][0]['delta'].content\n",
        "        if content is not None:  # Skip over responses with None content\n",
        "            yield content\n",
        "\n",
        "# Usage\n",
        "for response in generate_responses():\n",
        "    print(response, end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc4c09d-72fa-48d5-b09e-027724abc320",
      "metadata": {
        "tags": [],
        "id": "6dc4c09d-72fa-48d5-b09e-027724abc320"
      },
      "outputs": [],
      "source": [
        "# Non-Streaming response generation\n",
        "\n",
        "\n",
        "response = completion(\n",
        "        model=\"ollama/gemma:2b\",\n",
        "        messages=[{ \"content\": \"who are you?\", \"role\": \"user\"}],\n",
        "        api_base=\"http://localhost:11434\",\n",
        "    )\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df356a8d-2b9c-48fa-8de3-bb4017793ba3",
      "metadata": {
        "id": "df356a8d-2b9c-48fa-8de3-bb4017793ba3"
      },
      "source": [
        "## Custom Summarization model using Ollama\n",
        "> You need to install and run Ollama, and pull your LLM model e.g. Gemma-2b. See docs [here](https://ollama.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "493d2859-eb63-4f6b-9c2f-a8fd996b7cf8",
      "metadata": {
        "tags": [],
        "id": "493d2859-eb63-4f6b-9c2f-a8fd996b7cf8",
        "outputId": "989060ed-eb92-45f8-d43a-2a4abb724df8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-20 11:14:17,242 - Loading faiss.\n",
            "2024-03-20 11:14:17,267 - Successfully loaded faiss.\n"
          ]
        }
      ],
      "source": [
        "from raptor import BaseSummarizationModel\n",
        "\n",
        "\n",
        "# You can define your own Summarization model by extending the base Summarization Class.\n",
        "class GEMMASummarizationModel(BaseSummarizationModel):\n",
        "    def __init__(self, model_name=\"ollama/gemma:2b\"):\n",
        "        self.model = model_name\n",
        "\n",
        "    def summarize(self, context, max_tokens=150):\n",
        "        # Format the prompt for summarization\n",
        "        messages=[\n",
        "            { \"content\": \"You are an expert in summarizing text.\", \"role\": \"system\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Write a summary of the following, including as many key details as possible: {context}:\"}\n",
        "        ]\n",
        "\n",
        "        response = completion(\n",
        "            model=self.model,\n",
        "            messages=messages,\n",
        "            api_base=\"http://localhost:11434\"\n",
        "        )\n",
        "        return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "557ea715-b1dd-4383-8273-adb5d33abf5f",
      "metadata": {
        "tags": [],
        "id": "557ea715-b1dd-4383-8273-adb5d33abf5f"
      },
      "outputs": [],
      "source": [
        "from raptor import BaseQAModel\n",
        "\n",
        "class GEMMAQAModel(BaseQAModel):\n",
        "    def __init__(self, model_name= \"ollama/gemma:2b\"):\n",
        "        self.model = model_name\n",
        "\n",
        "    def answer_question(self, context, question):\n",
        "        # Apply the chat template for the context and question\n",
        "        messages=[\n",
        "              {\"role\": \"user\", \"content\": f\"Given Context: {context} Give the best full answer amongst the option to question {question}\"}\n",
        "        ]\n",
        "        response = completion(\n",
        "            model=self.model,\n",
        "            messages=messages,\n",
        "            api_base=\"http://localhost:11434\"\n",
        "        )\n",
        "        return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00cceb1f-1a85-4c4a-bf71-384e31e420af",
      "metadata": {
        "tags": [],
        "id": "00cceb1f-1a85-4c4a-bf71-384e31e420af"
      },
      "outputs": [],
      "source": [
        "from raptor import BaseEmbeddingModel\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
        "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def create_embedding(self, text):\n",
        "        return self.model.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "191e5aa2-c486-4e00-875c-0fe05d9aab25",
      "metadata": {
        "tags": [],
        "id": "191e5aa2-c486-4e00-875c-0fe05d9aab25",
        "outputId": "fa88bda8-3010-4d9c-f8a1-b35e304a5ecb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-20 11:14:43,364 - Load pretrained SentenceTransformer: sentence-transformers/multi-qa-mpnet-base-cos-v1\n",
            "2024-03-20 11:14:43,708 - Use pytorch device: cpu\n"
          ]
        }
      ],
      "source": [
        "from raptor import RetrievalAugmentationConfig, RetrievalAugmentation\n",
        "\n",
        "RAC = RetrievalAugmentationConfig(summarization_model=GEMMASummarizationModel(), qa_model=GEMMAQAModel(), embedding_model=SBertEmbeddingModel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c711700-1b6a-4aa4-b108-42e67e2ba24f",
      "metadata": {
        "tags": [],
        "id": "8c711700-1b6a-4aa4-b108-42e67e2ba24f",
        "outputId": "927df3a4-beee-4db0-bc25-7283038bdf63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-20 11:14:47,401 - Successfully initialized TreeBuilder with Config \n",
            "        TreeBuilderConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Max Tokens: 100\n",
            "            Num Layers: 5\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Summarization Length: 100\n",
            "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x2a07a3ad0>\n",
            "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x2a07ad090>}\n",
            "            Cluster Embedding Model: EMB\n",
            "        \n",
            "        Reduction Dimension: 10\n",
            "        Clustering Algorithm: RAPTOR_Clustering\n",
            "        Clustering Parameters: {}\n",
            "        \n",
            "2024-03-20 11:14:47,406 - Successfully initialized ClusterTreeBuilder with Config \n",
            "        TreeBuilderConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Max Tokens: 100\n",
            "            Num Layers: 5\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Summarization Length: 100\n",
            "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x2a07a3ad0>\n",
            "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x2a07ad090>}\n",
            "            Cluster Embedding Model: EMB\n",
            "        \n",
            "        Reduction Dimension: 10\n",
            "        Clustering Algorithm: RAPTOR_Clustering\n",
            "        Clustering Parameters: {}\n",
            "        \n",
            "2024-03-20 11:14:47,408 - Successfully initialized RetrievalAugmentation with Config \n",
            "        RetrievalAugmentationConfig:\n",
            "            \n",
            "        TreeBuilderConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Max Tokens: 100\n",
            "            Num Layers: 5\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Summarization Length: 100\n",
            "            Summarization Model: <__main__.GEMMASummarizationModel object at 0x2a07a3ad0>\n",
            "            Embedding Models: {'EMB': <__main__.SBertEmbeddingModel object at 0x2a07ad090>}\n",
            "            Cluster Embedding Model: EMB\n",
            "        \n",
            "        Reduction Dimension: 10\n",
            "        Clustering Algorithm: RAPTOR_Clustering\n",
            "        Clustering Parameters: {}\n",
            "        \n",
            "            \n",
            "            \n",
            "        TreeRetrieverConfig:\n",
            "            Tokenizer: <Encoding 'cl100k_base'>\n",
            "            Threshold: 0.5\n",
            "            Top K: 5\n",
            "            Selection Mode: top_k\n",
            "            Context Embedding Model: EMB\n",
            "            Embedding Model: <__main__.SBertEmbeddingModel object at 0x2a07ad090>\n",
            "            Num Layers: None\n",
            "            Start Layer: None\n",
            "        \n",
            "            \n",
            "            QA Model: <__main__.GEMMAQAModel object at 0x2a07ad9d0>\n",
            "            Tree Builder Type: cluster\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "RA = RetrievalAugmentation(config=RAC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411c106f-55b1-4b8d-9a45-3191771ead17",
      "metadata": {
        "tags": [],
        "id": "411c106f-55b1-4b8d-9a45-3191771ead17",
        "outputId": "3df1b7a1-5f1e-4328-fdff-a840f917b4fe",
        "colab": {
          "referenced_widgets": [
            "e88a64c4116a44b690424113c0a8f2ee",
            "dad32a163897479facc35aebde65f03e",
            "90549406b3604e0ab75f79665caaa223",
            "3fbb09f8760d4e388dfb17a0ba328b1a",
            "d84c6d38d82e4a4d8a621fc54c1a0b0c",
            "c2215a4f049d419a97f89ae5847ade73",
            "fea661c4c52642309a9c7c375a49a314",
            "c94c75dfac614b00b076825bf35ea955",
            "8d51a6e7e62d4007a2298a45a40a4006",
            "74254aec5b2348cf92c79860f8622053",
            "1cacb7486653481a8eb70e205e0626d0",
            "6e1d2a40e8c34111a4806f40e340910f",
            "09d4d6e896464c6da8a4ac082a69c35b",
            "e205b5dd835d4092b66ac5c83db48b7a",
            "dcbf4f9ae7054390a6f829852c99307f",
            "4030fc1b45bb46a5bb2f6af5f41e4744",
            "a7fceec48f484ff6a9a4c60872fff8d2",
            "d2b194a5385d47188436a19422d42fd0",
            "813bb182da2b4289b9a91b28f5b919a3",
            "3b8999cdd1564d52b548e2de24c24d4e",
            "cb6550eb70c94a00be25436d7f0418ee",
            "ff27531e5d1b4ae6b28e048556858db5",
            "8e816fb3cc494c9b97b429bb8ba9735a",
            "c804ee4a22864feea425b895c9284f65",
            "14ffe753771d454b902862478d5b5956"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e88a64c4116a44b690424113c0a8f2ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dad32a163897479facc35aebde65f03e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90549406b3604e0ab75f79665caaa223",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fbb09f8760d4e388dfb17a0ba328b1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d84c6d38d82e4a4d8a621fc54c1a0b0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2215a4f049d419a97f89ae5847ade73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fea661c4c52642309a9c7c375a49a314",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c94c75dfac614b00b076825bf35ea955",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d51a6e7e62d4007a2298a45a40a4006",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74254aec5b2348cf92c79860f8622053",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cacb7486653481a8eb70e205e0626d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e1d2a40e8c34111a4806f40e340910f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09d4d6e896464c6da8a4ac082a69c35b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e205b5dd835d4092b66ac5c83db48b7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcbf4f9ae7054390a6f829852c99307f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4030fc1b45bb46a5bb2f6af5f41e4744",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7fceec48f484ff6a9a4c60872fff8d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2b194a5385d47188436a19422d42fd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "813bb182da2b4289b9a91b28f5b919a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b8999cdd1564d52b548e2de24c24d4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb6550eb70c94a00be25436d7f0418ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff27531e5d1b4ae6b28e048556858db5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e816fb3cc494c9b97b429bb8ba9735a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c804ee4a22864feea425b895c9284f65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14ffe753771d454b902862478d5b5956",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# logging.disable(logging.CRITICAL)\n",
        "\n",
        "text = \"\"\n",
        "for page_num in range(2):\n",
        "    page = reader.pages[page_num]\n",
        "    text += page.extract_text() + ' '\n",
        "\n",
        "\n",
        "RA.add_documents(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51514594-6366-4572-8e85-d1b54f2bae4a",
      "metadata": {
        "id": "51514594-6366-4572-8e85-d1b54f2bae4a",
        "outputId": "a6e65267-6788-4783-c4b6-cb4ee1f76e57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-20 13:35:50,257 - Using collapsed_tree\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer:  STORM is a writing system designed to aid in the synthesis of topic outlines through retrieval and multi-perspective question asking. It operates by discovering diverse perspectives on a given topic, simulating conversations, gathering conversations from sources, refining outlines, and adding trusted sources. The system aims to address challenges at the pre-writing stage, specifically focusing on how to research a topic and create an outline before starting to write. STORM has shown significant improvements in organization and coverage in article creation, outperforming other baseline models. It follows a multi-stage approach involving generating questions, reading and asking experts, splitting queries, and searching for information to enhance the research capabilities of Large Language Models (LLMs).\n"
          ]
        }
      ],
      "source": [
        "question = \"what is storm?\"\n",
        "\n",
        "answer = RA.answer_question(question=question)\n",
        "\n",
        "print(\"Answer: \", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a656b02-e779-477f-ba63-7e276b723bad",
      "metadata": {
        "id": "4a656b02-e779-477f-ba63-7e276b723bad"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}